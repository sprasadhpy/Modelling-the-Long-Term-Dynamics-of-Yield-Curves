{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install artifacts-keyring\n",
    "%pip install --trusted-host elancapital.pkgs.visualstudio.com --extra-index-url=https://elancapital.pkgs.visualstudio.com/_packaging/ElanPyPI%40Release/pypi/simple/ cobrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --trusted-host elancapital.pkgs.visualstudio.com --extra-index-url=https://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@elancapital.pkgs.visualstudio.com/_packaging/ElanPyPI%40Release/pypi/simple/ cobrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pip._internal.main import main as pipmain\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def install_firewall_certificates():\n",
    "    \"\"\"\n",
    "    Install firewall certificates into current Python environment.\n",
    "    \"\"\"\n",
    "    pipmain(['install', 'certifi'])\n",
    "    import certifi\n",
    "    with open(certifi.where(), newline='') as f:\n",
    "        print(f'Updating certificates at {certifi.where()}')\n",
    "        current_file_contents = f.read()\n",
    "    for url in ('https://awepstorezrsinfra.blob.core.windows.net/public-certificates/elan_cas_ca.crt',\n",
    "                'https://awepstorezrsinfra.blob.core.windows.net/public-certificates/elan_cas_fw.crt'):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            cert_contents = response.read().decode()\n",
    "            # make sure certificate is only added once to the file\n",
    "            # can cause problems with az service bus amqp if duplicated\n",
    "            if cert_contents in current_file_contents:\n",
    "                print(f'Skipping {url} as it is already present')\n",
    "                continue\n",
    "            print(f'Adding {url}')\n",
    "            with open(certifi.where(), 'a+', newline='') as file:\n",
    "                file.write('\\n')\n",
    "                file.write(cert_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_firewall_certificates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cobrapy==1.1.0.485 --extra-index-url=https://pkgs.dev.azure.com/elancapital/_packaging/ElanPyPI/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install staticdataclient --extra-index-url=https://pkgs.dev.azure.com/elancapital/_packaging/ElanPyPI/pypi/simple/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobrapy import CobraApiFactory, SearchQueryBuilder, TimeseriesInterval, VirtualTimeseriesDefinition, TimeseriesInterval, TimeseriesType, FieldConfiguration\n",
    "\n",
    "cobra_api = CobraApiFactory.build(\n",
    "    \"http://prodconfig.elan-cap.com:2714/configuration/\",\n",
    "    \"Production\",\n",
    "    # \"Development\",\n",
    "    \"quantfe-jupyter-hwang\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 400)\n",
    "pd.set_option(\"display.max_rows\", 480)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"identifiers.csv\")\n",
    "data = data[['GenericSwap', 'Tenor', 'Name', 'Folder']]\n",
    "data[data['Name'].str.startswith('GBP')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/refinitiv/ohlc/00-00-10-00-00000/swaps/ICAP/bidaskprimsec/v1'\n",
    "df = cobra_api.get_reader(cobra_api.get_definition('GBP1YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df = df.set_index(df.columns[0], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "\n",
    "print(\"Pydantic version:\", pydantic.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydantic==<1.10.15>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydantic==1.10 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from staticdataclient.Instruments.RefinitivFuture import RefinitivFuture\n",
    "\n",
    "chain_futures = {}\n",
    "chains = client.GetAll(FutureChain)\n",
    "for chain in chains:\n",
    "    futures = client.GetMultiple(RefinitivFuture, IdentifiersRequest([IdentifierRequest(f.elanId, IdentifierType.Elan) for f in chain.futures]))\n",
    "    chain_futures[chain.chainRic] = (chain, futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from staticdataclient import StaticDataPythonClient\n",
    "\n",
    "client = StaticDataPythonClient(\n",
    "    \"MyTestClientId\",\n",
    "    \"https://awep-aks02-static-data-static-data-server.prod.elan-cap.com\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from staticdataclient.Instruments.QuotedVanillaSwap import QuotedVanillaSwap\n",
    "from staticdataclient import IdentifiersRequest, IdentifierRequest, IdentifierType\n",
    "elanids = ['GBPSONI01MO', 'GBPSONI01YR']\n",
    "swaps = client.GetMultiple(QuotedVanillaSwap, IdentifiersRequest([IdentifierRequest(elanid, IdentifierType.Elan) for elanid in elanids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBPSONI SONIA overnight index\n",
    "USDSOFR SOFR\n",
    "EURESTR ESTER\n",
    "JPYTONR TONAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(swaps[0].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why cant we use the First tick variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FirstTick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FirstTick'][1:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['First_Tick'] = pd.to_datetime(df['FirstTick'])\n",
    "\n",
    "\n",
    "\n",
    "# Group by day and count the number of ticks per day\n",
    "df['Date'] = df['First_Tick'].dt.date\n",
    "ticks_per_day = df.groupby('Date').size().reset_index(name='NumTicks')\n",
    "\n",
    "# Print or manipulate the result as needed\n",
    "print(ticks_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['First_Tick'][1:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we will use the index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_Daily_Primary_Asset = df['PRIMACT_1_Count'].groupby(pd.Grouper(freq='D')).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in ticks_Daily_Primary_Asset.items():\n",
    "    print(f\"Date: {index.strftime('%Y-%m-%d')}, PRIMACT_1_Count: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_Daily_Secondary_Asset =df['SEC_ACT_1_Count'].groupby(pd.Grouper(freq='D')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_Daily_Primary_Asset.shape, ticks_Daily_Secondary_Asset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_monthly_Primary_Asset = df['PRIMACT_1_Count'].groupby(pd.Grouper(freq='ME')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_monthly_Secondary_Asset = df['SEC_ACT_1_Count'].groupby(pd.Grouper(freq='ME')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in ticks_monthly_Primary_Asset.items():\n",
    "    print(f\"Date: {index.strftime('%Y-%m-%d')}, PRIMACT_1_Count: {value}\")\n",
    "    print(f\"Date: {index.strftime('%Y-%m-%d')}, SEC_ACT_1_Count: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame([(index.strftime('%Y-%m-%d'), value) for index, value in ticks_monthly_Primary_Asset.items()],\n",
    "                  columns=['Date', 'PRIMACT_1_Count'])\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame([(index.strftime('%Y-%m-%d'), value) for index, value in ticks_monthly_Secondary_Asset.items()],\n",
    "                  columns=['Date', 'SEC_ACT_1_Count'])\n",
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# Convert Date column to datetime\n",
    "df3['Date'] = pd.to_datetime(df3['Date'])\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(df3['Date'].dt.strftime('%Y-%m'), df3['PRIMACT_1_Count'], color='red')\n",
    "plt.title('Monthly PRIMACT_1_Count: Depth of the Market')\n",
    "plt.xlabel('Date (Year-Month)', fontsize=1.5) \n",
    "plt.ylabel('PRIMACT_1_Count')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels and align right\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(15))  # Adjust interval for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_monthly_Primary_Asset.shape, ticks_monthly_Secondary_Asset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Convert Date column to datetime if not already\n",
    "df3['Date'] = pd.to_datetime(df3['Date'])\n",
    "df4['Date'] = pd.to_datetime(df4['Date'])\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plotting PRIMACT_1_Count\n",
    "plt.bar(df3['Date'].dt.strftime('%Y-%m'), df3['PRIMACT_1_Count'], color='red', label='PRIMACT_1_Count')\n",
    "\n",
    "# Plotting Secondary (df4) as another bar\n",
    "plt.bar(df4['Date'].dt.strftime('%Y-%m'), df4['SEC_ACT_1_Count'], color='blue', label='SEC_ACT_1_Count')\n",
    "\n",
    "plt.title('Depth of the Market')\n",
    "plt.xlabel('Date (Year-Month)', fontsize=14) \n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels and align right\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(15))  # Adjust interval for better spacing\n",
    "plt.legend()  # Show legend\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/refinitiv/ohlc/00-00-10-00-00000/swaps/ICAP/bidaskprimsec/v1'\n",
    "df_1Y = cobra_api.get_reader(cobra_api.get_definition('GBP1YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "#df_1Y = df.set_index(df.columns[0], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1Y['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1Y = df_1Y.set_index(df_1Y.columns[0], drop=True)\n",
    "df_1Y = df_1Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_1Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1Y.columns, df_1Y.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_1Y = pd.DataFrame(df_1Y)\n",
    "\n",
    "# Convert the 'timestamp' column to datetime format\n",
    "df_1Y['Timestamp'] = pd.to_datetime(df_1Y['Timestamp'])\n",
    "\n",
    "# Set the 'timestamp' column as the index of the DataFrame\n",
    "df_1Y.set_index('Timestamp', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1Y[1:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled = df_1Y.resample('10T').last()\n",
    "\n",
    "# Select the :50 minute mark for each hour by filtering on minutes\n",
    "df_50_minutes = df_resampled[df_resampled.index.minute == 50]\n",
    "\n",
    "df_50_minutes[1:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1Y_down_sampled = df_1Y[df_1Y.index.minute == 0]\n",
    "df_1Y_down_sampled[1:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenericSwap\tTenor\tName\tFolder\n",
    "42\tGBPSONI\t1M\tGBP1MOIS=ICAP\t/refinitiv/raw/swaps\n",
    "43\tGBPSONI\t3M\tGBP3MOIS=ICAP\t/refinitiv/raw/swaps\n",
    "44\tGBPSONI\t6M\tGBP6MOIS=ICAP\t/refinitiv/raw/swaps\n",
    "45\tGBPSONI\t9M\tGBP9MOIS=ICAP\t/refinitiv/raw/swaps\n",
    "46\tGBPSONI\t1Y\tGBP1YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "47\tGBPSONI\t2Y\tGBP2YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "48\tGBPSONI\t3Y\tGBP3YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "49\tGBPSONI\t4Y\tGBP4YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "50\tGBPSONI\t5Y\tGBP5YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "51\tGBPSONI\t6Y\tGBP6YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "52\tGBPSONI\t7Y\tGBP7YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "53\tGBPSONI\t8Y\tGBP8YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "54\tGBPSONI\t9Y\tGBP9YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "55\tGBPSONI\t10Y\tGBP10YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "56\tGBPSONI\t12Y\tGBP12YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "57\tGBPSONI\t15Y\tGBP15YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "58\tGBPSONI\t20Y\tGBP20YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "59\tGBPSONI\t25Y\tGBP25YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "60\tGBPSONI\t30Y\tGBP30YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "61\tGBPSONI\t40Y\tGBP40YOIS=ICAP\t/refinitiv/raw/swaps\n",
    "62\tGBPSONI\t50Y\tGBP50YOIS=ICAP\t/refinitiv/raw/swaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/refinitiv/ohlc/00-00-10-00-00000/swaps/ICAP/bidaskprimsec/v1'\n",
    "df_3Y = cobra_api.get_reader(cobra_api.get_definition('GBP3YOIS=ICAP', folder)).download_as_frame(flatten=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3Y = df_3Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_3Y = pd.DataFrame(df_3Y)\n",
    "# Convert the 'timestamp' column to datetime format\n",
    "df_3Y['Timestamp'] = pd.to_datetime(df_3Y['Timestamp'])\n",
    "\n",
    "# Set the 'timestamp' column as the index of the DataFrame\n",
    "df_3Y.set_index('Timestamp', inplace=True)\n",
    "df_3Y_down_sampled = df_3Y[df_3Y.index.minute == 0]\n",
    "df_3Y_down_sampled[1:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/refinitiv/ohlc/00-00-10-00-00000/swaps/ICAP/bidaskprimsec/v1'\n",
    "df_5Y = cobra_api.get_reader(cobra_api.get_definition('GBP5YOIS=ICAP', folder)).download_as_frame(flatten=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5Y = df_5Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_5Y = pd.DataFrame(df_5Y)\n",
    "# Convert the 'timestamp' column to datetime format\n",
    "df_5Y['Timestamp'] = pd.to_datetime(df_5Y['Timestamp'])\n",
    "\n",
    "# Set the 'timestamp' column as the index of the DataFrame\n",
    "df_5Y.set_index('Timestamp', inplace=True)\n",
    "df_5Y_down_sampled = df_5Y[df_5Y.index.minute == 0]\n",
    "df_5Y_down_sampled[1:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1Y_down_sampled.shape,df_3Y_down_sampled.shape,df_5Y_down_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2008-05-24 17:00:00'\n",
    "end_time = '2023-07-11 16:00:00'\n",
    "\n",
    "# Filter DataFrame between two timestamps\n",
    "filtered_df_5Y = df_5Y_down_sampled.loc[start_time:end_time]\n",
    "filtered_df_3Y = df_3Y_down_sampled.loc[start_time:end_time]\n",
    "filtered_df_1Y = df_1Y_down_sampled.loc[start_time:end_time]\n",
    "filtered_df_5Y,filtered_df_3Y,filtered_df_1Y\n",
    "filtered_df_5Y.shape,filtered_df_3Y.shape,filtered_df_1Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3Y_down_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a mask that only selects times between 8:00 AM and 4:00 PM\n",
    "mask = (df_3Y_down_sampled.index.time >= pd.to_datetime('08:00:00').time()) & (df_3Y_down_sampled.index.time <= pd.to_datetime('16:00:00').time())\n",
    "\n",
    "# Apply the mask to the DataFrame\n",
    "filtered_df_3 = df_3Y_down_sampled[mask]\n",
    "\n",
    "print(filtered_df_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask that only selects times between 8:00 AM and 4:00 PM\n",
    "mask = (df_5Y_down_sampled.index.time >= pd.to_datetime('08:00:00').time()) & (df_5Y_down_sampled.index.time <= pd.to_datetime('16:00:00').time())\n",
    "\n",
    "# Apply the mask to the DataFrame\n",
    "filtered_df_5Y = df_5Y_down_sampled[mask]\n",
    "\n",
    "print(filtered_df_5Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/refinitiv/ohlc/00-00-10-00-00000/swaps/ICAP/bidaskprimsec/v1'\n",
    "df_3Y = cobra_api.get_reader(cobra_api.get_definition('GBP3YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_3Y = df_3Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_3Y = pd.DataFrame(df_3Y)\n",
    "# Convert the 'timestamp' column to datetime format\n",
    "df_3Y['Timestamp'] = pd.to_datetime(df_3Y['Timestamp'])\n",
    "\n",
    "# Set the 'timestamp' column as the index of the DataFrame\n",
    "df_3Y.set_index('Timestamp', inplace=True)\n",
    "df_3Y_down_sampled = df_3Y[df_3Y.index.minute == 0]\n",
    "df_3Y_down_sampled[1:25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final_Data_preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobrapy import CobraApiFactory, SearchQueryBuilder, TimeseriesInterval, VirtualTimeseriesDefinition, TimeseriesInterval, TimeseriesType, FieldConfiguration\n",
    "\n",
    "cobra_api = CobraApiFactory.build(\n",
    "    \"http://prodconfig.elan-cap.com:2714/configuration/\",\n",
    "    \"Production\",\n",
    "    # \"Development\",\n",
    "    \"quantfe-jupyter-hwang\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, i procure datasets for different tenor  - I filter the dataset between the trading hours 8:00 am to 4: 00 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_data(df):\n",
    "    # Convert the 'Timestamp' column to datetime format and set it as the index\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "    mask = (df.index.time >= pd.to_datetime('08:00:00').time()) & (df.index.time <= pd.to_datetime('16:00:00').time())\n",
    "    df=df[mask]\n",
    "    # Filter rows where the index minute is 0 \n",
    "    return df[df.index.minute == 0]\n",
    "\n",
    "# Define the folder path and file definitions\n",
    "folder = '/refinitiv/ohlc/00-00-10-00-00000/swaps/ICAP/bidaskprimsec/v1'\n",
    "\n",
    "df_1Y = cobra_api.get_reader(cobra_api.get_definition('GBP1YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_1Y = df_1Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_1Y['yield_1Y'] = df_1Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_1Y_down_sampled = prepare_data(df_1Y)\n",
    "\n",
    "df_2Y = cobra_api.get_reader(cobra_api.get_definition('GBP2YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_2Y = df_2Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_2Y['yield_2Y'] = df_2Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_2Y_down_sampled = prepare_data(df_2Y)\n",
    "\n",
    "\n",
    "# Download and prepare the 3-year data\n",
    "df_3Y = cobra_api.get_reader(cobra_api.get_definition('GBP3YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_3Y = df_3Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_3Y['yield_3Y'] = df_3Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_3Y_down_sampled = prepare_data(df_3Y)\n",
    "\n",
    "df_4Y = cobra_api.get_reader(cobra_api.get_definition('GBP4YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_4Y = df_4Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_4Y['yield_4Y'] = df_4Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_4Y_down_sampled = prepare_data(df_4Y)\n",
    "\n",
    "\n",
    "df_5Y = cobra_api.get_reader(cobra_api.get_definition('GBP5YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_5Y = df_5Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_5Y['yield_5Y'] = df_5Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_5Y_down_sampled = prepare_data(df_5Y)\n",
    "\n",
    "df_6Y = cobra_api.get_reader(cobra_api.get_definition('GBP6YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_6Y = df_6Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_6Y['yield_6Y'] = df_6Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_6Y_down_sampled = prepare_data(df_6Y)\n",
    "\n",
    "df_7Y = cobra_api.get_reader(cobra_api.get_definition('GBP7YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_7Y = df_7Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_7Y['yield_7Y'] = df_7Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_7Y_down_sampled = prepare_data(df_7Y)\n",
    "\n",
    "\n",
    "df_8Y = cobra_api.get_reader(cobra_api.get_definition('GBP8YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_8Y = df_8Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_8Y['yield_8Y'] = df_8Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_8Y_down_sampled = prepare_data(df_8Y)\n",
    "\n",
    "\n",
    "df_9Y = cobra_api.get_reader(cobra_api.get_definition('GBP9YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_9Y = df_9Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_9Y['yield_9Y'] = df_9Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_9Y_down_sampled = prepare_data(df_9Y)\n",
    "\n",
    "df_10Y = cobra_api.get_reader(cobra_api.get_definition('GBP10YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_10Y = df_10Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_10Y['yield_10Y'] = df_10Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_10Y_down_sampled = prepare_data(df_10Y)\n",
    "\n",
    "df_12Y = cobra_api.get_reader(cobra_api.get_definition('GBP12YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_12Y = df_12Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_12Y['yield_12Y'] = df_12Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_12Y_down_sampled = prepare_data(df_12Y)\n",
    "\n",
    "df_15Y = cobra_api.get_reader(cobra_api.get_definition('GBP15YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_15Y = df_15Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_15Y['yield_15Y'] = df_15Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_15Y_down_sampled = prepare_data(df_15Y)\n",
    "\n",
    "df_20Y = cobra_api.get_reader(cobra_api.get_definition('GBP20YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_20Y = df_20Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_20Y['yield_20Y'] = df_20Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_20Y_down_sampled = prepare_data(df_20Y)\n",
    "\n",
    "df_25Y = cobra_api.get_reader(cobra_api.get_definition('GBP25YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_25Y = df_25Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_25Y['yield_25Y'] = df_25Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_25Y_down_sampled = prepare_data(df_25Y)\n",
    "\n",
    "df_30Y = cobra_api.get_reader(cobra_api.get_definition('GBP30YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_30Y = df_30Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_30Y['yield_30Y'] = df_30Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_30Y_down_sampled = prepare_data(df_30Y)\n",
    "\n",
    "df_40Y = cobra_api.get_reader(cobra_api.get_definition('GBP40YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_40Y = df_40Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_40Y['yield_40Y'] = df_40Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_40Y_down_sampled = prepare_data(df_40Y)\n",
    "\n",
    "df_50Y = cobra_api.get_reader(cobra_api.get_definition('GBP50YOIS=ICAP', folder)).download_as_frame(flatten=2)\n",
    "df_50Y = df_50Y[['Timestamp','PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "df_50Y['yield_50Y'] = df_50Y[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "df_50Y_down_sampled = prepare_data(df_50Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimized code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, i optimize code for a better latency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_data(df):\n",
    "    # Convert the 'Timestamp' column to datetime and set it as the index\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    # Apply a time-based mask and then filter to on-the-hour entries\n",
    "    mask = (df.index.time >= pd.to_datetime('08:00:00').time()) & \\\n",
    "           (df.index.time <= pd.to_datetime('16:00:00').time())\n",
    "    df = df[mask]\n",
    "    return df[df.index.minute == 0]\n",
    "\n",
    "def fetch_and_prepare(symbol, folder):\n",
    "    # Fetch data\n",
    "    df = cobra_api.get_reader(cobra_api.get_definition(f'{symbol}=ICAP', folder)).download_as_frame(flatten=2)\n",
    "    df = df[['Timestamp', 'PRIMACT_1_Close', 'SEC_ACT_1_Close']]\n",
    "    \n",
    "    # Calculate yield\n",
    "    df[f'yield_{symbol}'] = df[['PRIMACT_1_Close', 'SEC_ACT_1_Close']].mean(axis=1)\n",
    "    \n",
    "    # Prepare data\n",
    "    return prepare_data(df)\n",
    "\n",
    "# Define the folder path\n",
    "folder = '/refinitiv/ohlc/00-00-10-00-00000/swaps/ICAP/bidaskprimsec/v1'\n",
    "\n",
    "# Process 1-year and 2-year data\n",
    "df_1Y_down_sampled = fetch_and_prepare('GBP1YOIS', folder)\n",
    "df_2Y_down_sampled = fetch_and_prepare('GBP2YOIS', folder)\n",
    "df_3Y_down_sampled = fetch_and_prepare('GBP3YOIS', folder)\n",
    "df_4Y_down_sampled = fetch_and_prepare('GBP4YOIS', folder)\n",
    "df_5Y_down_sampled = fetch_and_prepare('GBP5YOIS', folder)\n",
    "df_6Y_down_sampled = fetch_and_prepare('GBP6YOIS', folder)\n",
    "df_7Y_down_sampled = fetch_and_prepare('GBP7YOIS', folder)\n",
    "df_8Y_down_sampled = fetch_and_prepare('GBP8YOIS', folder)\n",
    "df_9Y_down_sampled = fetch_and_prepare('GBP9YOIS', folder)\n",
    "df_10Y_down_sampled = fetch_and_prepare('GBP10YOIS', folder)\n",
    "df_12Y_down_sampled = fetch_and_prepare('GBP12YOIS', folder)\n",
    "df_15Y_down_sampled = fetch_and_prepare('GBP15YOIS', folder)\n",
    "df_20Y_down_sampled = fetch_and_prepare('GBP20YOIS', folder)\n",
    "df_25Y_down_sampled = fetch_and_prepare('GBP25YOIS', folder)\n",
    "df_30Y_down_sampled = fetch_and_prepare('GBP30YOIS', folder)\n",
    "df_40Y_down_sampled = fetch_and_prepare('GBP40YOIS', folder)\n",
    "df_50Y_down_sampled = fetch_and_prepare('GBP50YOIS', folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to compare the size of individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of DataFrames _ Old one \n",
    "dfs = {\n",
    "    '1Y': df_1Y_down_sampled,\n",
    "    '2Y': df_2Y_down_sampled,\n",
    "    '3Y': df_3Y_down_sampled,\n",
    "    '4Y': df_4Y_down_sampled,\n",
    "    '5Y': df_5Y_down_sampled,\n",
    "    '6Y': df_6Y_down_sampled,\n",
    "    '7Y': df_7Y_down_sampled,\n",
    "    '8y': df_8Y_down_sampled,\n",
    "    '9Y': df_9Y_down_sampled,\n",
    "    '10Y': df_10Y_down_sampled,\n",
    "    '12Y': df_12Y_down_sampled,\n",
    "    '15Y': df_15Y_down_sampled,\n",
    "    '20Y': df_20Y_down_sampled,\n",
    "    '25Y': df_25Y_down_sampled,\n",
    "    '30Y': df_30Y_down_sampled,\n",
    "    '40Y': df_40Y_down_sampled,\n",
    "    '50Y': df_50Y_down_sampled\n",
    "}\n",
    "\n",
    "# Loop to print row counts\n",
    "for term, df in dfs.items():\n",
    "    print(f\"Row count for {term}: {df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'dfs' is your dictionary of DataFrames\n",
    "\n",
    "# Find the key corresponding to the DataFrame with the minimum number of rows\n",
    "min_df_key = min(dfs, key=lambda x: dfs[x].shape[0])\n",
    "max_df_key = max(dfs,key=lambda x:dfs[x].shape[0])\n",
    "        \n",
    "# Print the key and the number of rows of the DataFrame with the minimum rows\n",
    "print(f\"The DataFrame with the minimum rows is '{min_df_key}' with {dfs[min_df_key].shape[0]} rows.\")\n",
    "print(f\"The DataFrame with the maximum rows is '{max_df_key}' with {dfs[max_df_key].shape[0]} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dictionary of DataFrames\n",
    "dfs = {\n",
    "    '1Y': df_1Y_down_sampled,\n",
    "    '2Y': df_2Y_down_sampled,\n",
    "    '3Y': df_3Y_down_sampled,\n",
    "    '4Y': df_4Y_down_sampled,\n",
    "    '5Y': df_5Y_down_sampled,\n",
    "    '6Y': df_6Y_down_sampled,\n",
    "    '7Y': df_7Y_down_sampled,\n",
    "    '8Y': df_8Y_down_sampled,\n",
    "    '9Y': df_9Y_down_sampled,\n",
    "    '10Y': df_10Y_down_sampled,\n",
    "    '12Y': df_12Y_down_sampled,\n",
    "    '15Y': df_15Y_down_sampled,\n",
    "    '20Y': df_20Y_down_sampled,\n",
    "    '25Y': df_25Y_down_sampled,\n",
    "    '30Y': df_30Y_down_sampled,\n",
    "    '40Y': df_40Y_down_sampled,\n",
    "    '50Y': df_50Y_down_sampled\n",
    "}\n",
    "\n",
    "# Get the first index value for each DataFrame\n",
    "first_indexes = {term: df.index[0] for term, df in dfs.items()}\n",
    "\n",
    "# Printing the first index values\n",
    "for term, index in first_indexes.items():\n",
    "    print(f\"First index for {term}: {index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Made the timestamp index as a column and divided the time and date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_dataframe(df):\n",
    "    df.reset_index(inplace=True)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df['Date'] = df['Timestamp'].dt.date\n",
    "    df['Time'] = df['Timestamp'].dt.time\n",
    "    return df\n",
    "\n",
    "# Assuming df_1Y, df_2Y, df_5Y, df_10Y are already defined DataFrames\n",
    "df_1Y_down_processed = process_dataframe(df_1Y_down_sampled)\n",
    "df_2Y_down_processed = process_dataframe(df_2Y_down_sampled)\n",
    "df_3Y_down_processed = process_dataframe(df_3Y_down_sampled)\n",
    "df_4Y_down_processed = process_dataframe(df_4Y_down_sampled)\n",
    "df_5Y_down_processed = process_dataframe(df_5Y_down_sampled)\n",
    "df_6Y_down_processed = process_dataframe(df_6Y_down_sampled)\n",
    "df_7Y_down_processed = process_dataframe(df_7Y_down_sampled) \n",
    "df_8Y_down_processed = process_dataframe(df_8Y_down_sampled)\n",
    "df_9Y_down_processed = process_dataframe(df_9Y_down_sampled)\n",
    "df_10Y_down_processed = process_dataframe(df_10Y_down_sampled)\n",
    "df_12Y_down_processed = process_dataframe(df_12Y_down_sampled)\n",
    "df_15Y_down_processed = process_dataframe(df_15Y_down_sampled)\n",
    "df_20Y_down_processed = process_dataframe(df_20Y_down_sampled)\n",
    "df_25Y_down_processed  = process_dataframe(df_25Y_down_sampled)\n",
    "df_30Y_down_processed  = process_dataframe(df_30Y_down_sampled)\n",
    "df_40Y_down_processed  = process_dataframe(df_40Y_down_sampled)\n",
    "df_50Y_down_processed  = process_dataframe(df_50Y_down_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 1 - Have the same starting date 2011-09-05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1: Lets Count them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_datapoints_after_date(df, date_column, specific_date):\n",
    "    \"\"\"\n",
    "    Counts the number of datapoints in a DataFrame that occur after a specified date and returns the filtered DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        date_column (str): The name of the column containing date data.\n",
    "        specific_date (str or pd.Timestamp): The date after which to count datapoints.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame filtered to include only datapoints after the specified date.\n",
    "        int: The number of datapoints after the specified date.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is in datetime format and filter the DataFrame\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    filtered_df = df[df[date_column] >= specific_date]\n",
    "    \n",
    "    # Count the number of datapoints\n",
    "    count_of_datapoints = len(filtered_df)\n",
    "    \n",
    "    return  count_of_datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame setup for demonstration\n",
    "# Assume that the data frames df_1Y_down_processed, df_2Y_down_processed, etc., are already defined\n",
    "\n",
    "# List of all ten dataframes for different tenors\n",
    "dataframes = {\n",
    "    '1Y': df_1Y_down_processed,\n",
    "    '2Y': df_2Y_down_processed,\n",
    "    '3Y': df_3Y_down_processed,\n",
    "    '4Y': df_4Y_down_processed,\n",
    "    '5Y': df_5Y_down_processed,\n",
    "    '6Y': df_6Y_down_processed,\n",
    "    '7Y': df_7Y_down_processed,\n",
    "    '8Y': df_8Y_down_processed,\n",
    "    '9Y': df_9Y_down_processed,\n",
    "    '10Y': df_10Y_down_processed,\n",
    "    '12Y': df_12Y_down_processed,\n",
    "    '15Y': df_15Y_down_processed,\n",
    "    '20Y': df_20Y_down_processed,\n",
    "    '25Y': df_25Y_down_processed,\n",
    "    '30Y': df_30Y_down_processed,\n",
    "    '40Y': df_40Y_down_processed,\n",
    "    '50Y': df_50Y_down_processed\n",
    "}\n",
    "\n",
    "# Define the specific date for comparison\n",
    "specific_date = pd.Timestamp('2011-09-05')\n",
    "\n",
    "# Iterate through each DataFrame in the list\n",
    "for tenor, df in dataframes.items():\n",
    "    # Call the function for each DataFrame and capture the results\n",
    "    number_of_datapoints = count_datapoints_after_date(df, 'Date', specific_date)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"The number of datapoints after {specific_date.date()} for {tenor} is {number_of_datapoints}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2 Filtered dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def filter_datapoints_after_date(df, date_column, specific_date):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to include only rows where the date in the specified column is after the given date.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        date_column (str): The name of the column containing date data.\n",
    "        specific_date (str or pd.Timestamp): The date after which to include datapoints.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame filtered to include only datapoints after the specified date.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df[date_column] >= specific_date]\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specific date for filtering\n",
    "specific_date = pd.Timestamp('2011-09-05')\n",
    "\n",
    "# Apply the filtering function to each DataFrame\n",
    "df_1Y_down_processed = filter_datapoints_after_date(df_1Y_down_processed, 'Date', specific_date)\n",
    "df_2Y_down_processed = filter_datapoints_after_date(df_2Y_down_processed, 'Date', specific_date)\n",
    "df_3Y_down_processed = filter_datapoints_after_date(df_3Y_down_processed, 'Date', specific_date)\n",
    "df_4Y_down_processed = filter_datapoints_after_date(df_4Y_down_processed, 'Date', specific_date)\n",
    "df_5Y_down_processed = filter_datapoints_after_date(df_5Y_down_processed, 'Date', specific_date)\n",
    "df_6Y_down_processed = filter_datapoints_after_date(df_6Y_down_processed, 'Date', specific_date)\n",
    "df_7Y_down_processed = filter_datapoints_after_date(df_7Y_down_processed, 'Date', specific_date)\n",
    "df_8Y_down_processed = filter_datapoints_after_date(df_8Y_down_processed, 'Date', specific_date)\n",
    "df_9Y_down_processed = filter_datapoints_after_date(df_9Y_down_processed, 'Date', specific_date)\n",
    "df_10Y_down_processed = filter_datapoints_after_date(df_10Y_down_processed, 'Date', specific_date)\n",
    "df_12Y_down_processed = filter_datapoints_after_date(df_12Y_down_processed, 'Date', specific_date)\n",
    "df_15Y_down_processed = filter_datapoints_after_date(df_15Y_down_processed, 'Date', specific_date)\n",
    "df_20Y_down_processed = filter_datapoints_after_date(df_20Y_down_processed, 'Date', specific_date)\n",
    "df_25Y_down_processed = filter_datapoints_after_date(df_25Y_down_processed, 'Date', specific_date)\n",
    "df_30Y_down_processed = filter_datapoints_after_date(df_30Y_down_processed, 'Date', specific_date)\n",
    "df_40Y_down_processed = filter_datapoints_after_date(df_40Y_down_processed, 'Date', specific_date)\n",
    "df_50Y_down_processed = filter_datapoints_after_date(df_50Y_down_processed, 'Date', specific_date)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holidays removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobra_api.get_all_definitions('financial_calendars/calendars/holidays/holidays_and_weekends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays= cobra_api.get_reader(cobra_api.get_definition('LnB', 'financial_calendars/calendars/holidays/holidays_and_weekends')).download_as_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holidays['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total number of holidays if the starting date is 2011-09-05 and end  date is 2023-07-11\n",
    "\n",
    "df_holidays['Timestamp'] = pd.to_datetime(df_holidays['Timestamp'])\n",
    "\n",
    "# Define the start date and the end date\n",
    "start_date = pd.Timestamp('2011-09-05')\n",
    "end_date = pd.Timestamp('2023-07-11')\n",
    "\n",
    "# Filter the DataFrame to include only the holidays within the date range\n",
    "filtered_holidays = df_holidays[\n",
    "    (df_holidays['Timestamp'] >= start_date) &\n",
    "    (df_holidays['Timestamp'] <= end_date)\n",
    "]\n",
    "\n",
    "# Convert timestamps to date (if not already done)\n",
    "filtered_holidays['Date'] = filtered_holidays['Timestamp'].dt.date\n",
    "\n",
    "# Find unique dates\n",
    "unique_holidays_df = pd.DataFrame(filtered_holidays['Date'].unique(), columns=['UniqueHolidayDates'])\n",
    "\n",
    "\n",
    "# Count the number of unique holiday dates\n",
    "number_of_holidays = len(unique_holidays_df)\n",
    "\n",
    "print(f'The number of unique holiday dates between {start_date.date()} and {end_date.date()} is {number_of_holidays}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming all the necessary DataFrames are already defined\n",
    "dataframes = {\n",
    "    '1Y': df_1Y_down_processed,\n",
    "    '2Y': df_2Y_down_processed,\n",
    "    '3Y': df_3Y_down_processed,\n",
    "    '4Y': df_4Y_down_processed,\n",
    "    '5Y': df_5Y_down_processed,\n",
    "    '6Y': df_6Y_down_processed,\n",
    "    '7Y': df_7Y_down_processed,\n",
    "    '8Y': df_8Y_down_processed,\n",
    "    '9Y': df_9Y_down_processed,\n",
    "    '10Y': df_10Y_down_processed,\n",
    "    '12Y': df_12Y_down_processed,\n",
    "    '15Y': df_15Y_down_processed,\n",
    "    '20Y': df_20Y_down_processed,\n",
    "    '25Y': df_25Y_down_processed,\n",
    "    '30Y': df_30Y_down_processed,\n",
    "    '40Y': df_40Y_down_processed,\n",
    "    '50Y': df_50Y_down_processed\n",
    "}\n",
    "\n",
    "# Define unique_holidays_df as assumed to be previously created and filled with dates\n",
    "unique_holidays_df['UniqueHolidayDates'] = pd.to_datetime(unique_holidays_df['UniqueHolidayDates'])\n",
    "\n",
    "for tenor, df in dataframes.items():\n",
    "    # Convert the 'Date' column to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Calculate the number of rows before filtering\n",
    "    initial_row_count = len(df)\n",
    "\n",
    "    # Remove rows where the date is in the list of unique holiday dates\n",
    "    filtered_df = df[~df['Date'].isin(unique_holidays_df['UniqueHolidayDates'])]\n",
    "\n",
    "    # Calculate the number of rows after filtering\n",
    "    final_row_count = len(filtered_df)\n",
    "\n",
    "    # Calculate the number of rows removed\n",
    "    rows_removed = initial_row_count - final_row_count\n",
    "\n",
    "    # Output the result for each tenor\n",
    "    print(f\"{tenor} - Initial number of rows: {initial_row_count}\")\n",
    "    print(f\"{tenor} - Number of rows after filtering: {final_row_count}\")\n",
    "    print(f\"{tenor} - Number of rows matched and removed: {rows_removed}\")\n",
    "\n",
    "    # Update the dictionary to store the filtered dataframes if needed later\n",
    "    dataframes[tenor] = filtered_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After removing the holidays the final dataframes for individual tenor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_out_holidays(df, date_column, unique_holidays):\n",
    "\n",
    "    # Convert the date column to datetime if not already\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    # Convert unique holidays to datetime if necessary\n",
    "    unique_holidays = pd.to_datetime(unique_holidays)\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[~df[date_column].isin(unique_holidays)]\n",
    "    return filtered_df\n",
    "\n",
    "filtered_df_1Y = filter_out_holidays(df_1Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_2Y = filter_out_holidays(df_2Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_3Y = filter_out_holidays(df_3Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_4Y = filter_out_holidays(df_4Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_5Y = filter_out_holidays(df_5Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_6Y = filter_out_holidays(df_6Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_7Y = filter_out_holidays(df_7Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_8Y = filter_out_holidays(df_8Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_9Y = filter_out_holidays(df_9Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_10Y = filter_out_holidays(df_10Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_12Y = filter_out_holidays(df_12Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_15Y = filter_out_holidays(df_15Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_20Y = filter_out_holidays(df_20Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_25Y = filter_out_holidays(df_25Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_30Y = filter_out_holidays(df_30Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_40Y = filter_out_holidays(df_40Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n",
    "filtered_df_50Y = filter_out_holidays(df_50Y_down_processed, 'Date', unique_holidays_df['UniqueHolidayDates'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final rows of dataframes after removing the holidays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1Y\n",
    "\n",
    "# Dictionary of DataFrames _ Old one \n",
    "dfs = {\n",
    "    '1Y': filtered_df_1Y,\n",
    "    '2Y': filtered_df_2Y,\n",
    "    '3Y': filtered_df_3Y,\n",
    "    '4Y': filtered_df_4Y,\n",
    "    '5Y': filtered_df_5Y,\n",
    "    '6Y': filtered_df_6Y,\n",
    "    '7Y': filtered_df_7Y,\n",
    "    '8y': filtered_df_8Y,\n",
    "    '9Y': filtered_df_9Y,\n",
    "    '10Y': filtered_df_10Y,\n",
    "    '12Y': filtered_df_12Y,\n",
    "    '15Y': filtered_df_15Y,\n",
    "    '20Y': filtered_df_20Y,\n",
    "    '25Y': filtered_df_25Y,\n",
    "    '30Y': filtered_df_30Y,\n",
    "    '40Y': filtered_df_40Y,\n",
    "    '50Y': filtered_df_50Y } \n",
    "\n",
    "# Loop to print row counts\n",
    "for term, df in dfs.items():\n",
    "    print(f\"Row count for {term}: {df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df_1Y_final = filtered_df_1Y[['Date', 'Time', 'yield_1Y']]\n",
    "filtered_df_2Y_final = filtered_df_2Y[['Date', 'Time', 'yield_2Y']]\n",
    "filtered_df_3Y_final = filtered_df_3Y[['Date', 'Time', 'yield_3Y']]\n",
    "filtered_df_4Y_final = filtered_df_4Y[['Date', 'Time', 'yield_4Y']]\n",
    "filtered_df_5Y_final = filtered_df_5Y[['Date', 'Time', 'yield_5Y']]\n",
    "filtered_df_6Y_final = filtered_df_6Y[['Date', 'Time', 'yield_6Y']]\n",
    "filtered_df_7Y_final = filtered_df_7Y[['Date', 'Time', 'yield_7Y']]\n",
    "filtered_df_8Y_final = filtered_df_8Y[['Date', 'Time', 'yield_8Y']]\n",
    "filtered_df_9Y_final = filtered_df_9Y[['Date', 'Time', 'yield_9Y']]\n",
    "filtered_df_10Y_final = filtered_df_10Y[['Date', 'Time', 'yield_10Y']]\n",
    "filtered_df_12Y_final = filtered_df_12Y[['Date', 'Time', 'yield_12Y']]\n",
    "filtered_df_15Y_final = filtered_df_15Y[['Date', 'Time', 'yield_15Y']]\n",
    "filtered_df_20Y_final = filtered_df_20Y[['Date', 'Time', 'yield_20Y']]\n",
    "filtered_df_25Y_final = filtered_df_25Y[['Date', 'Time', 'yield_25Y']]\n",
    "filtered_df_30Y_final = filtered_df_30Y[['Date', 'Time', 'yield_30Y']]\n",
    "filtered_df_40Y_final = filtered_df_40Y[['Date', 'Time', 'yield_40Y']]\n",
    "filtered_df_50Y_final = filtered_df_50Y[['Date', 'Time', 'yield_50Y']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print slices of each DataFrame individually\n",
    "print(\"Filtered_df_1Y_final (Rows 1-24):\")\n",
    "print(filtered_df_1Y_final[0:25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(filtered_df_6Y_final[0:25])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df_40Y_final[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1Y_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1Y_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_30Y_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_30Y_final.index[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of DataFrames _ Old one \n",
    "dfs = {\n",
    "    'filtered_df_1Y_final': filtered_df_1Y_final,\n",
    "    'filtered_df_2Y_final': filtered_df_2Y_final,  # Assuming you have or will create filtered_df_2Y_final similarly\n",
    "    'filtered_df_3Y_final': filtered_df_3Y_final,  # Assuming processed version available, else use filtered_df_3Y directly\n",
    "    'filtered_df_4Y_final': filtered_df_4Y_final,\n",
    "    'filtered_df_5Y_final': filtered_df_5Y_final,\n",
    "    'filtered_df_6Y_final': filtered_df_6Y_final,\n",
    "    'filtered_df_7Y_final': filtered_df_7Y_final,\n",
    "    'filtered_df_8Y_final': filtered_df_8Y_final,\n",
    "    'filtered_df_9Y_final': filtered_df_9Y_final,\n",
    "    'filtered_df_10Y_final': filtered_df_10Y_final,\n",
    "    'filtered_df_12Y_final': filtered_df_12Y_final,\n",
    "    'filtered_df_15Y_final': filtered_df_15Y_final,\n",
    "    'filtered_df_20Y_final': filtered_df_20Y_final,\n",
    "    'filtered_df_25Y_final': filtered_df_25Y_final,\n",
    "    'filtered_df_30Y_final': filtered_df_30Y_final,\n",
    "    'filtered_df_40Y_final': filtered_df_40Y_final,\n",
    "    'filtered_df_50Y_final': filtered_df_50Y_final} \n",
    "\n",
    "# Loop to print row counts\n",
    "for term, df in dfs.items():\n",
    "    print(f\"Row count for {term}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to combine 'Date' and 'Time' into a single 'Datetime' and set as index\n",
    "def combine_date_time(df):\n",
    "    if 'Date' in df.columns and 'Time' in df.columns:\n",
    "        # Ensure 'Date' and 'Time' are in string format suitable for concatenation\n",
    "        df['Date'] = df['Date'].astype(str)\n",
    "        df['Time'] = df['Time'].astype(str)\n",
    "\n",
    "        # Combine into a 'Datetime' column\n",
    "        df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "\n",
    "        # Set 'Datetime' as the index\n",
    "        df.set_index('Datetime', inplace=True)\n",
    "        \n",
    "        # Drop the original 'Date' and 'Time' columns\n",
    "        df.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Assuming dataframes are previously defined and loaded\n",
    "dataframes = {\n",
    "    'filtered_df_1Y_final': filtered_df_1Y_final,\n",
    "    'filtered_df_2Y_final': filtered_df_2Y_final,  # Assuming you have or will create filtered_df_2Y_final similarly\n",
    "    'filtered_df_3Y_final': filtered_df_3Y_final,  # Assuming processed version available, else use filtered_df_3Y directly\n",
    "    'filtered_df_4Y_final': filtered_df_4Y_final,\n",
    "    'filtered_df_5Y_final': filtered_df_5Y_final,\n",
    "    'filtered_df_6Y_final': filtered_df_6Y_final,\n",
    "    'filtered_df_7Y_final': filtered_df_7Y_final,\n",
    "    'filtered_df_8Y_final': filtered_df_8Y_final,\n",
    "    'filtered_df_9Y_final': filtered_df_9Y_final,\n",
    "    'filtered_df_10Y_final': filtered_df_10Y_final,\n",
    "    'filtered_df_12Y_final': filtered_df_12Y_final,\n",
    "    'filtered_df_15Y_final': filtered_df_15Y_final,\n",
    "    'filtered_df_20Y_final': filtered_df_20Y_final,\n",
    "    'filtered_df_25Y_final': filtered_df_25Y_final,\n",
    "    'filtered_df_30Y_final': filtered_df_30Y_final,\n",
    "    'filtered_df_40Y_final': filtered_df_40Y_final,\n",
    "    'filtered_df_50Y_final': filtered_df_50Y_final\n",
    "}\n",
    "\n",
    "# Apply the function to each DataFrame\n",
    "for name, df in dataframes.items():\n",
    "    dataframes[name] = combine_date_time(df)\n",
    "\n",
    "# Example to access the modified DataFrame\n",
    "print(dataframes['filtered_df_1Y_final'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_1Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    'filtered_df_1Y_final': filtered_df_1Y_final,\n",
    "    'filtered_df_2Y_final': filtered_df_2Y_final,  # Assuming you have or will create filtered_df_2Y_final similarly\n",
    "    'filtered_df_3Y_final': filtered_df_3Y_final,  # Assuming processed version available, else use filtered_df_3Y directly\n",
    "    'filtered_df_4Y_final': filtered_df_4Y_final,\n",
    "    'filtered_df_5Y_final': filtered_df_5Y_final,\n",
    "    'filtered_df_6Y_final': filtered_df_6Y_final,\n",
    "    'filtered_df_7Y_final': filtered_df_7Y_final,\n",
    "    'filtered_df_8Y_final': filtered_df_8Y_final,\n",
    "    'filtered_df_9Y_final': filtered_df_9Y_final,\n",
    "    'filtered_df_10Y_final': filtered_df_10Y_final,\n",
    "    'filtered_df_12Y_final': filtered_df_12Y_final,\n",
    "    'filtered_df_15Y_final': filtered_df_15Y_final,\n",
    "    'filtered_df_20Y_final': filtered_df_20Y_final,\n",
    "    'filtered_df_25Y_final': filtered_df_25Y_final,\n",
    "    'filtered_df_30Y_final': filtered_df_30Y_final,\n",
    "    'filtered_df_40Y_final': filtered_df_40Y_final,\n",
    "    'filtered_df_50Y_final': filtered_df_50Y_final} \n",
    "\n",
    "# Loop to print row counts\n",
    "for term, df in dfs.items():\n",
    "    print(f\"Row count for {term}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrames are indexed by datetime and stored in a dictionary\n",
    "dataframes = {\n",
    "    'filtered_df_1Y_final': filtered_df_1Y_final,\n",
    "    'filtered_df_2Y_final': filtered_df_2Y_final,\n",
    "    'filtered_df_3Y_final': filtered_df_3Y_final,\n",
    "    'filtered_df_4Y_final': filtered_df_4Y_final,\n",
    "    'filtered_df_5Y_final': filtered_df_5Y_final,\n",
    "    'filtered_df_6Y_final': filtered_df_6Y_final,\n",
    "    'filtered_df_7Y_final': filtered_df_7Y_final,\n",
    "    'filtered_df_8Y_final': filtered_df_8Y_final,\n",
    "    'filtered_df_9Y_final': filtered_df_9Y_final,\n",
    "    'filtered_df_10Y_final': filtered_df_10Y_final,\n",
    "    'filtered_df_12Y_final': filtered_df_12Y_final,\n",
    "    'filtered_df_15Y_final': filtered_df_15Y_final,\n",
    "    'filtered_df_20Y_final': filtered_df_20Y_final,\n",
    "    'filtered_df_25Y_final': filtered_df_25Y_final,\n",
    "    'filtered_df_30Y_final': filtered_df_30Y_final,\n",
    "    'filtered_df_40Y_final': filtered_df_40Y_final,\n",
    "    'filtered_df_50Y_final': filtered_df_50Y_final\n",
    "}\n",
    "\n",
    "# Concatenate all dataframes horizontally, using an outer join\n",
    "combined_df = pd.concat(dataframes.values(), axis=1, join='outer', keys=dataframes.keys())\n",
    "\n",
    "# Optionally handle NaN values, e.g., fill with zeros or forward fill\n",
    "# combined_df.fillna(0, inplace=True)\n",
    "# combined_df.ffill(inplace=True)\n",
    "\n",
    "# Print the combined DataFrame to check results\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = combined_df.isna().sum()\n",
    "\n",
    "# Print the count of NaNs for each column\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Flatten the MultiIndex by joining the levels with a separator and renaming:\n",
    "combined_df.columns = ['_'.join(col).split('_final_')[-1] for col in combined_df.columns]\n",
    "\n",
    "# Now df has columns renamed simply to reflect the yield and its tenor, e.g., 'yield_1Y'\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_nonan= combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df.to_csv('H:/Excel/combined_df_1.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined_df = pd.read_csv('H:/Excel/combined_df_1.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 7))  # Set the figure size for better readability\n",
    "for column in combined_df.columns:\n",
    "    plt.plot(combined_df.index, combined_df[column], label=column)\n",
    "\n",
    "plt.title('Time Series Plot for All DataFrames')  # Add a title\n",
    "plt.xlabel('Date')  # X-axis label\n",
    "plt.ylabel('Yield')  # Y-axis label\n",
    "plt.legend()  # Add a legend to distinguish the lines\n",
    "plt.grid(True)  # Add a grid for easier readability\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FD-yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming combined_df is already defined with datetime index and yield columns\n",
    "\n",
    "# Compute first differences for each column\n",
    "first_diff_df = combined_df.diff()\n",
    "\n",
    "# Drop the first row since it will have NaN values due to differencing\n",
    "#first_diff_df = first_diff_df.dropna()\n",
    "\n",
    "\n",
    "# Optionally, you can reset the index if you prefer a flat structure\n",
    "#first_diff_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(first_diff_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 7))  # Set the figure size for better readability\n",
    "for column in first_diff_df.columns:\n",
    "    plt.plot(first_diff_df.index, first_diff_df[column], label=column)\n",
    "\n",
    "plt.title('Return Series Plot for All DataFrames')  # Add a title\n",
    "plt.xlabel('Date')  # X-axis label\n",
    "plt.ylabel('Yield')  # Y-axis label\n",
    "plt.legend()  # Add a legend to distinguish the lines\n",
    "plt.grid(True)  # Add a grid for easier readability\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Assuming 'combined_df' is your DataFrame with multiple columns representing different maturities\n",
    "\n",
    "n = len(first_diff_df.columns)  # Number of columns\n",
    "col = 3  # Number of columns per row in the subplot\n",
    "row = int(np.ceil(n / col))  # Calculate the number of rows needed\n",
    "fig_height = row * 6  # Height of the figure\n",
    "fig_width = col * 8  # Width of the figure\n",
    "\n",
    "# Plotting the DataFrame with subplots\n",
    "ax = first_diff_df.plot(subplots=True, layout=(row, col), figsize=(fig_width, fig_height),\n",
    "             sharey=True, sharex=True, title='Return Series vs. Year', grid=True)\n",
    "\n",
    "# Customizing the x-axis tick labels for each subplot in the first row\n",
    "for i in range(col):\n",
    "    ax[0, i].xaxis.set_tick_params(which='both', top=True, labeltop=True, labelrotation=40)\n",
    "\n",
    "# Adjust the layout and margins\n",
    "fig = ax[0, 0].get_figure()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.93)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you're analyzing 'filtered_df_1Y_yield'\n",
    "column_to_check = 'yield_1Y'\n",
    "\n",
    "# Find the indices where NaN values occur\n",
    "nan_indices = combined_df[combined_df[column_to_check].isna()].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['yield_1Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Assuming 'combined_df' is your DataFrame\n",
    "# Make sure to replace 'combined_df' with your actual DataFrame variable name\n",
    "\n",
    "# Get the first column name\n",
    "first_column = combined_df.columns[1]\n",
    "\n",
    "# Plotting the first column and marking NaN values\n",
    "plt.figure(figsize=(14, 7))  # Set the figure size for better readability\n",
    "\n",
    "# Plot the first column\n",
    "plt.plot(combined_df.index, combined_df[first_column], label=first_column, linestyle='-', color='b')\n",
    "print( combined_df[first_column])\n",
    "# Identify where NaN values occur and plot them\n",
    "nan_indices = combined_df[first_column].isna()\n",
    "plt.scatter(combined_df.index[nan_indices], np.ones(len(combined_df[first_column][nan_indices]),), color='red', label='NaN', marker='x')  # Mark NaNs with red x\n",
    "\n",
    "plt.title(f'Time Series Plot of {first_column} with NaN Values Marked')  # Add a title with the first column name\n",
    "plt.xlabel('Date')  # X-axis label\n",
    "plt.ylabel('Yield')  # Y-axis label based on the data type\n",
    "plt.legend()  # Add a legend to distinguish the line and NaN markers\n",
    "plt.grid(True)  # Add a grid for easier readability\n",
    "plt.show()  # Display the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eomonth = combined_df.groupby(combined_df.index.to_period(\"M\")).apply(lambda x: x.index.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoyear = combined_df.groupby(combined_df.index.to_period(\"Y\")).apply(lambda x: x.index.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eomonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,7))\n",
    "\n",
    "for i,j in enumerate(eoyear):\n",
    "    if i%2==0: \n",
    "        ax.plot(combined_df.loc[j], marker=\"o\", markersize=5, label=j.date())\n",
    "    \n",
    "\n",
    "ax.set_title (\"Yield Curves (End-Of-Year)\")\n",
    "ax.set_xlabel (\"Maturity\")\n",
    "ax.set_ylabel (\"in pp.\")\n",
    "ax.legend(title=\"Date\", loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_df is your DataFrame containing yield data\n",
    "\n",
    "# Get all unique years present in the index of combined_df\n",
    "unique_years = combined_df.index.year.unique()\n",
    "\n",
    "# Initialize an empty list to store end-of-year dates for all years\n",
    "eoyear_all = []\n",
    "\n",
    "# Iterate through each unique year and find the maximum date for that year\n",
    "for year in unique_years:\n",
    "    max_date = combined_df.loc[combined_df.index.year == year].index.max()\n",
    "    eoyear_all.append(max_date)\n",
    "\n",
    "# Convert the list of end-of-year dates to a Pandas DatetimeIndex\n",
    "eoyear_all = pd.DatetimeIndex(eoyear_all)\n",
    "\n",
    "# Print or use eoyear_all as needed\n",
    "print(eoyear_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,7))\n",
    "\n",
    "for i, j in enumerate(eoyear_all):\n",
    "    ax.plot(combined_df.loc[j], marker=\"o\", markersize=5, label=j.date())\n",
    "    \n",
    "ax.set_title(\"Yield Curves (End-Of-Year)\")\n",
    "ax.set_xlabel(\"Maturity\")\n",
    "ax.set_ylabel(\"in pp.\")\n",
    "ax.legend(title=\"Date\", loc=\"upper left\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(25,7))\n",
    "\n",
    "for i,j in enumerate(eomonth):\n",
    "    if i%2==0: \n",
    "        ax.plot(combined_df.loc[j], marker=\"o\", markersize=5, label=j.date())\n",
    "    \n",
    "\n",
    "ax.set_title (\"Yield Curves (End-Of-Month)\")\n",
    "ax.set_xlabel (\"Maturity\")\n",
    "ax.set_ylabel (\"in pp.\")\n",
    "ax.legend(title=\"Date\", loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming combined_df is your DataFrame with columns 'yield_1Y', 'yield_2Y', ..., 'yield_50Y'\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(14, 12))\n",
    "\n",
    "# Plotting all columns in the first subplot (ax1)\n",
    "for column in combined_df.columns:\n",
    "    ax1.plot(combined_df.index, combined_df[column], label=column)\n",
    "\n",
    "ax1.set_title('Time Series Plot for All DataFrames')  # Set title for the first subplot\n",
    "ax1.set_xlabel('Date')  # X-axis label\n",
    "ax1.set_ylabel('Yield')  # Y-axis label\n",
    "ax1.legend()  # Add legend to the first subplot\n",
    "ax1.grid(True)  # Add grid for easier readability\n",
    "\n",
    "\n",
    "# Iterate through columns and plot the differences\n",
    "for column in combined_df.columns:\n",
    "    if column != 'yield_15Y':  # Exclude yield_50Y column\n",
    "        ax2.plot(combined_df.index, combined_df[column] - combined_df['yield_15Y'], label=f\"{column} - yield_15Y\", linewidth=1)\n",
    "\n",
    "# Plot horizontal line at y=0\n",
    "ax2.hlines(y=0, xmin=min(combined_df.index), xmax=max(combined_df.index), colors=\"black\", linestyles=\"dashed\")\n",
    "\n",
    "# Formatting\n",
    "ax2.set_title(\"Yield Spreads to yield_15Y\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "ax2.set_ylabel(\"Difference in Yield\")\n",
    "ax2.legend(title=\"Yield Spreads\", loc=\"upper right\")\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume combined_df_nonan is your DataFrame without NaNs and contains yield data\n",
    "tenors = combined_df.columns\n",
    "\n",
    "# Calculate and display the quartiles and minimum value for each column\n",
    "summary = combined_df.describe(percentiles=[0.25, 0.5, 0.75]).T[['min', '25%', '50%', '75%']]\n",
    "\n",
    "# Display the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df = combined_df.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume combined_df_nonan is your DataFrame without NaNs and contains yield data\n",
    "tenors = first_diff_df.columns\n",
    "\n",
    "# Calculate and display the quartiles and minimum value for each column\n",
    "summary = first_diff_df.describe(percentiles=[0.25, 0.5, 0.75]).T[['min', '25%', '50%', '75%']]\n",
    "\n",
    "# Display the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming combined_df_nonan is your DataFrame without NaNs and it has columns like 'yield_2Y', 'yield_3Y', etc.\n",
    "tenors = combined_df_nonan.columns\n",
    "num_plots = len(tenors)  # Number of plots needed\n",
    "\n",
    "# Set up the grid with 3 columns per row\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(num_plots / ncols))\n",
    "\n",
    "# Set up the matplotlib figure and subplots with reduced size\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows * 3))  # Smaller figsize to reduce plot size\n",
    "axes = axes.flatten()  # Flatten in case of a grid layout\n",
    "\n",
    "# Iterate over each tenor and create the corresponding plot\n",
    "for i, tenor in enumerate(tenors):\n",
    "    # Extract the yield data for the given tenor\n",
    "    data = combined_df_nonan[tenor].dropna()\n",
    "\n",
    "    # Plot the histogram of the yield data\n",
    "    sns.histplot(data, bins=30, kde=False, color='tan', stat=\"density\", ax=axes[i])\n",
    "\n",
    "    # Fit a normal distribution and plot it\n",
    "    mu, std = norm.fit(data)\n",
    "    xmin, xmax = axes[i].get_xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    axes[i].plot(x, p, 'g-', lw=2, label='Normal')\n",
    "\n",
    "    # Add titles and labels\n",
    "    axes[i].set_title(f\"Distribution of {tenor}\")\n",
    "    axes[i].set_xlabel(\"Interest Rate (%)\")\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming combined_df is your DataFrame with columns 'yield_1Y', 'yield_2Y', ..., 'yield_50Y'\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(14, 12))\n",
    "\n",
    "# Plotting all columns in the first subplot (ax1)\n",
    "for column in combined_df.columns:\n",
    "    ax1.plot(combined_df.index, combined_df[column], label=column)\n",
    "\n",
    "ax1.set_title('Time Series Plot for All DataFrames')  # Set title for the first subplot\n",
    "ax1.set_xlabel('Date')  # X-axis label\n",
    "ax1.set_ylabel('Yield')  # Y-axis label\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(-0.15, 0.5))  # Move legend to the left\n",
    "ax1.grid(True)  # Add grid for easier readability\n",
    "\n",
    "# Iterate through columns and plot the differences\n",
    "for column in combined_df.columns:\n",
    "    if column != 'yield_15Y':  # Exclude yield_50Y column\n",
    "        ax2.plot(combined_df.index, combined_df[column] - combined_df['yield_15Y'], label=f\"{column} - yield_15Y\", linewidth=1)\n",
    "\n",
    "# Plot horizontal line at y=0\n",
    "ax2.hlines(y=0, xmin=min(combined_df.index), xmax=max(combined_df.index), colors=\"black\", linestyles=\"dashed\")\n",
    "\n",
    "# Formatting\n",
    "ax2.set_title(\"Yield Spreads to yield_15Y\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "ax2.set_ylabel(\"Difference in Yield\")\n",
    "ax2.legend(title=\"Yield Spreads\", loc='center left', bbox_to_anchor=(-0.15, 0.5))  # Move legend to the left\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'combined_df' is your DataFrame containing the yield data for different tenors\n",
    "first_diff_df = combined_df.diff().dropna()  # Calculate first differences and drop NaN values\n",
    "\n",
    "tenors = first_diff_df.columns\n",
    "\n",
    "# Set up the matplotlib figure and subplots dynamically\n",
    "ncols = 3\n",
    "nrows = (len(tenors) + ncols - 1) // ncols  # Calculate the number of rows needed\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, nrows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over each tenor and create the corresponding plot\n",
    "for i, tenor in enumerate(tenors):\n",
    "    # Get the first differences for the current tenor\n",
    "    diff_returns = first_diff_df[tenor]\n",
    "\n",
    "    # Plot the histogram of first differences\n",
    "    sns.histplot(diff_returns, bins=30, kde=False, color='skyblue', stat=\"density\", ax=axes[i])\n",
    "\n",
    "    # Fit a normal distribution and plot it\n",
    "    mu, std = norm.fit(diff_returns)\n",
    "    xmin, xmax = axes[i].get_xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    axes[i].plot(x, p, 'g-', lw=2, label='Normal')\n",
    "\n",
    "    # Add titles and labels\n",
    "    axes[i].set_title(f\"Distribution of {tenor} abs returns\")\n",
    "    axes[i].set_xlabel(\"Abs Returns (differences)\")\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"absolute_returns_distribution.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'first_diff_df' is your DataFrame containing the yield returns (first differences)\n",
    "# Replace 'first_diff_df' with the actual DataFrame containing your data.\n",
    "\n",
    "# Generate the pair plot\n",
    "sns.pairplot(first_diff_df, diag_kind='kde', plot_kws={'alpha': 0.5, 's': 10, 'edgecolor': 'k'}, diag_kws={'shade': True})\n",
    "\n",
    "# Adjust the layout for better visualization\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle('Figure 3.4: Exhibits high positive correlation between neighbor tenors for yield returns', fontsize=14)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('pairplot_yield_returns.png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'first_diff_df' is your DataFrame containing the yield returns\n",
    "corr = first_diff_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation  of Yield Returns across Tenors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert time window for rolling volatility (in units of previously defined frequency)--- be carfeful \n",
    "roll_window = 10\n",
    "roll_vola = combined_df.rolling(roll_window).std().iloc[roll_window-1:]\n",
    "\n",
    "# Depicting rolling volatility for various maturities over time\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,7))\n",
    "\n",
    "for column in roll_vola.columns:\n",
    "    ax.plot(roll_vola.index, roll_vola[column], label=column)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(f'Rolling Volatility over {roll_window} Periods')\n",
    "ax.set_ylabel(\"Standard Deviation\")\n",
    "ax.legend(title=\"Maturities\", loc=\"upper right\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_nonan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'combined_df' is your DataFrame with multiple columns representing different maturities\n",
    "roll_window = 10\n",
    "roll_vola = combined_df.rolling(roll_window).std().iloc[roll_window-1:]\n",
    "n = len(roll_vola.columns)  # Number of columns\n",
    "col = 3  # Number of columns per row in the subplot\n",
    "row = int(np.ceil(n / col))  # Calculate the number of rows needed\n",
    "fig_height = row * 6  # Height of the figure\n",
    "fig_width = col * 8  # Width of the figure\n",
    "\n",
    "# Plotting the DataFrame with subplots\n",
    "ax = roll_vola.plot(subplots=True, layout=(row, col), figsize=(fig_width, fig_height),\n",
    "             sharey=True, sharex=True, title='rolling volatility vs. Year', grid=True)\n",
    "\n",
    "# Customizing the x-axis tick labels for each subplot in the first row\n",
    "for i in range(col):\n",
    "    ax[0, i].xaxis.set_tick_params(which='both', top=True, labeltop=True, labelrotation=40)\n",
    "\n",
    "# Adjust the layout and margins\n",
    "fig = ax[0, 0].get_figure()\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.93)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components - Yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import principalcomponents as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_all = combined_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_cleaned = combined_df.dropna()\n",
    "pc_model =pc.PCA(spot = combined_df_cleaned,maturities=mat_all,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "pc_scores   = pc_model.eig_scores_k\n",
    "pc_vect     = pc_model.eig_vect_k\n",
    "pc_vect_inv = pc_model.eig_vect_inv_k\n",
    "pc_yields   = pc_model.yields\n",
    "pc_idx      = pc_model.idx[:k]\n",
    "pc_idx2     = pc_model.idx[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "def rainbow(categories):\n",
    "    \"\"\"\n",
    "    This function generates a dictionary of color codes for each category.\n",
    "    \"\"\"\n",
    "    c_scale = cm.rainbow(np.linspace(0,1,len(categories)))\n",
    "    c_dict = {}\n",
    "\n",
    "    for i,c in zip(categories,c_scale):\n",
    "        c_dict[i] = c\n",
    "        \n",
    "    return c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\n",
    "color = cm.rainbow(np.linspace(0,1,3))\n",
    "\n",
    "for i,c in zip(pc_idx2, color):\n",
    "    ax.plot(pc_vect[i], c=c, label=i, linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title (\"Eigenvector Loadings\")\n",
    "ax.set_xlabel (\"Maturities\")\n",
    "ax.legend(title=\"Components\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (15,7))\n",
    "\n",
    "# Plotting\n",
    "ax.bar(pc_model.eig_vals.index[:5], pc_model.eig_vals[\"eig_val_rel\"][:5])\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_title (\"Eigenvalues Explainable Variance\")\n",
    "\n",
    "# Annotations\n",
    "for p in ax.patches:\n",
    "    x_val = p.get_x()\n",
    "    y_val = p.get_height()\n",
    "    bar   = p.get_width()/2\n",
    "\n",
    "    ax.annotate(text = str(round(y_val,4)), \n",
    "                xy = (x_val+bar, y_val+0.025), \n",
    "                ha = \"center\", \n",
    "                va = \"center\",\n",
    "                fontsize = 11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "color = rainbow(set(pc_scores.index.year))\n",
    "\n",
    "# Prepare data for visualization\n",
    "temp         = pc_scores.copy()\n",
    "temp[\"year\"] = pc_scores.index.year\n",
    "temp         = temp.groupby(\"year\")\n",
    "\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\n",
    "\n",
    "for label, data in temp:\n",
    "    ax.plot(data[\"PC_2\"], data[\"PC_1\"], c=color[label], marker=\"o\", linestyle=\"\", alpha=0.3, label=label)\n",
    "\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Eigen-Scores\")\n",
    "ax.set_ylabel(\"PC1\")\n",
    "ax.set_xlabel(\"PC2\")\n",
    "plt.legend(title=\"Years\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield Curve Factors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Scaled principal components\n",
    "pc_1 = scaler.fit_transform(pc_scores[\"PC_1\"].values.reshape(-1, 1))*-1\n",
    "pc_2 = scaler.fit_transform(pc_scores[\"PC_2\"].values.reshape(-1, 1))*-1\n",
    "pc_3 = scaler.fit_transform(pc_scores[\"PC_3\"].values.reshape(-1, 1))\n",
    "\n",
    "# Classicl yield curve factors\n",
    "lvl   = scaler.fit_transform(combined_df_cleaned[\"yield_10Y\"].values.reshape(-1, 1))\n",
    "slope = scaler.fit_transform((combined_df_cleaned[\"yield_30Y\"] - combined_df_cleaned[\"yield_5Y\"]).values.reshape(-1, 1))\n",
    "curve = scaler.fit_transform((combined_df_cleaned[\"yield_30Y\"] + combined_df_cleaned[\"yield_2Y\"] - 2*combined_df_cleaned[\"yield_5Y\"]).values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Date formats\n",
    "days_loc   = mdates.DayLocator(interval = 7)\n",
    "months_loc = mdates.MonthLocator()\n",
    "years_loc  = mdates.YearLocator()\n",
    "\n",
    "days_fmt   = mdates.DateFormatter(\"%Y-%m-%d\")\n",
    "months_fmt = mdates.DateFormatter(\"%Y-%m\")\n",
    "years_fmt  = mdates.DateFormatter('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize = (15,20))\n",
    "colors = rainbow([\"PC\",\"YC\"])\n",
    "\n",
    "# Create plot\n",
    "ax[0].plot(pc_scores.index, pc_1, label=\"PC1\", c=colors[\"PC\"])\n",
    "ax[0].plot(pc_scores.index, lvl,  label=\"10Y\", c=colors[\"YC\"])\n",
    "\n",
    "ax[1].plot(pc_scores.index, pc_2, label=\"PC2\", c=colors[\"PC\"])\n",
    "ax[1].plot(pc_scores.index, slope,label=\"30Y - 5Y\", c=colors[\"YC\"])\n",
    "\n",
    "ax[2].plot(pc_scores.index, pc_3*-1, label=\"PC3\", c=colors[\"PC\"])\n",
    "ax[2].plot(pc_scores.index, curve,label=\"30Y + 10Y - 2*5Y\", c=colors[\"YC\"])\n",
    "\n",
    "\n",
    "\n",
    "# Formatting\n",
    "ax[0].set_title (\"Level Factor\")\n",
    "ax[1].set_title (\"Slope Factor\")\n",
    "ax[2].set_title (\"Curvature Factor\")\n",
    "\n",
    "for i in range (3):\n",
    "    ax[i].xaxis.set_major_formatter(years_fmt)\n",
    "    ax[i].xaxis.set_major_locator(years_loc)\n",
    "    ax[i].xaxis.set_minor_locator(months_loc)\n",
    "    \n",
    "    ax[i].set_ylabel (\"Scaled Score/Yield\")\n",
    "    ax[i].legend(loc = \"upper left\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_diff_df_cleaned = first_diff_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Yields -FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned = first_diff_df.dropna()\n",
    "\n",
    "pc_model_diff=pc.PCA(spot = first_diff_df_cleaned,maturities=mat_all,k=3)\n",
    "k=3\n",
    "pc_scores_diff   = pc_model_diff.eig_scores_k\n",
    "pc_vect_diff     = pc_model_diff.eig_vect_k\n",
    "pc_vect_inv_diff = pc_model_diff.eig_vect_inv_k\n",
    "pc_yields_diff   = pc_model_diff.yields\n",
    "pc_idx_diff    = pc_model_diff.idx[:k]\n",
    "pc_idx2_diff    = pc_model_diff.idx[:3]\n",
    "\n",
    "pc_vect_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "def rainbow(categories):\n",
    "    \"\"\"\n",
    "    This function generates a dictionary of color codes for each category.\n",
    "    \"\"\"\n",
    "    c_scale = cm.rainbow(np.linspace(0,1,len(categories)))\n",
    "    c_dict = {}\n",
    "\n",
    "    for i,c in zip(categories,c_scale):\n",
    "        c_dict[i] = c\n",
    "        \n",
    "    return c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\n",
    "color = cm.rainbow(np.linspace(0,1,3))\n",
    "\n",
    "for i,c in zip(pc_idx_diff, color):\n",
    "    ax.plot(pc_vect_diff[i], c=c, label=i, linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title (\"Eigenvector Loadings\")\n",
    "ax.set_xlabel (\"Maturities\")\n",
    "ax.legend(title=\"Components\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (15,7))\n",
    "\n",
    "# Plotting\n",
    "ax.bar(pc_model_diff.eig_vals.index[:5], pc_model_diff.eig_vals[\"eig_val_rel\"][:5])\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_title (\"Eigenvalues Explainable Variance\")\n",
    "\n",
    "# Annotations\n",
    "for p in ax.patches:\n",
    "    x_val = p.get_x()\n",
    "    y_val = p.get_height()\n",
    "    bar   = p.get_width()/2\n",
    "\n",
    "    ax.annotate(text = str(round(y_val,4)), \n",
    "                xy = (x_val+bar, y_val+0.025), \n",
    "                ha = \"center\", \n",
    "                va = \"center\",\n",
    "                fontsize = 11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "color = rainbow(set(pc_scores_diff.index.year))\n",
    "\n",
    "# Prepare data for visualization\n",
    "temp         = pc_scores_diff.copy()\n",
    "temp[\"year\"] = pc_scores_diff.index.year\n",
    "temp         = temp.groupby(\"year\")\n",
    "\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\n",
    "\n",
    "for label, data in temp:\n",
    "    ax.plot(data[\"PC_2\"], data[\"PC_1\"], c=color[label], marker=\"o\", linestyle=\"\", alpha=0.3, label=label)\n",
    "\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(\"Eigen-Scores for Yield - FD\")\n",
    "ax.set_ylabel(\"PC1\")\n",
    "ax.set_xlabel(\"PC2\")\n",
    "plt.legend(title=\"Years\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield Curve factors  - FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Scaled principal components\n",
    "pc_1 = scaler.fit_transform(pc_scores_diff[\"PC_1\"].values.reshape(-1, 1))*-1\n",
    "pc_2 = scaler.fit_transform(pc_scores_diff[\"PC_2\"].values.reshape(-1, 1))*-1\n",
    "pc_3 = scaler.fit_transform(pc_scores_diff[\"PC_3\"].values.reshape(-1, 1))\n",
    "\n",
    "# Classicl yield curve factors\n",
    "lvl   = scaler.fit_transform(first_diff_df_cleaned[\"yield_10Y\"].values.reshape(-1, 1))\n",
    "slope = scaler.fit_transform((first_diff_df_cleaned[\"yield_30Y\"] - first_diff_df_cleaned[\"yield_5Y\"]).values.reshape(-1, 1))\n",
    "curve = scaler.fit_transform((first_diff_df_cleaned[\"yield_30Y\"] + first_diff_df_cleaned[\"yield_2Y\"] - 2*first_diff_df_cleaned[\"yield_5Y\"]).values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize = (15,20))\n",
    "colors = rainbow([\"PC\",\"YC\"])\n",
    "\n",
    "# Create plot\n",
    "ax[0].plot(pc_scores_diff.index, pc_1, label=\"PC1\", c=colors[\"PC\"])\n",
    "ax[0].plot(pc_scores_diff.index, lvl,  label=\"10Y\", c=colors[\"YC\"])\n",
    "\n",
    "ax[1].plot(pc_scores_diff.index, pc_2, label=\"PC2\", c=colors[\"PC\"])\n",
    "ax[1].plot(pc_scores_diff.index, slope,label=\"30Y - 5Y\", c=colors[\"YC\"])\n",
    "\n",
    "ax[2].plot(pc_scores_diff.index, pc_3*-1, label=\"PC3\", c=colors[\"PC\"])\n",
    "ax[2].plot(pc_scores_diff.index, curve,label=\"30Y + 10Y - 2*5Y\", c=colors[\"YC\"])\n",
    "\n",
    "\n",
    "\n",
    "# Formatting\n",
    "ax[0].set_title (\"Level Factor\")\n",
    "ax[1].set_title (\"Slope Factor\")\n",
    "ax[2].set_title (\"Curvature Factor\")\n",
    "\n",
    "for i in range (3):\n",
    "    ax[i].xaxis.set_major_formatter(years_fmt)\n",
    "    ax[i].xaxis.set_major_locator(years_loc)\n",
    "    ax[i].xaxis.set_minor_locator(months_loc)\n",
    "    \n",
    "    ax[i].set_ylabel (\"Scaled Score/Yield\")\n",
    "    ax[i].legend(loc = \"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARY - Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.pca import PCA\n",
    "\n",
    "# Initialize PCA\n",
    "combined_df_cleaned = combined_df.dropna()\n",
    "pca = PCA(combined_df_cleaned, ncomp=3)\n",
    "\n",
    "#pca.fit\n",
    "\n",
    "# Access components and explained variance\n",
    "loadings = pca.loadings\n",
    "eigenvalues = pca.eigenvals\n",
    "\n",
    "# Print or use the components and explained variance\n",
    "print(loadings)\n",
    "print(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(loadings.shape[1]):  # Iterate over each component\n",
    "    plt.plot(combined_df_cleaned.columns, loadings.iloc[:, i], marker='o', linestyle='-', label=f'PC{i+1}')\n",
    "\n",
    "plt.title('Principal Component Loadings')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Loading')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pd.DataFrame(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pd.DataFrame(eigenvalues)\n",
    "eigenvalues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library YIELD - FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.pca import PCA\n",
    "\n",
    "\n",
    "pca_diff = PCA(first_diff_df_cleaned, ncomp=3)\n",
    "\n",
    "#pca.fit\n",
    "\n",
    "# Access components and explained variance\n",
    "loadings_diff = pca_diff.loadings\n",
    "eigenvalues_diff = pca_diff.eigenvals\n",
    "\n",
    "# Print or use the components and explained variance\n",
    "print(loadings_diff)\n",
    "print(eigenvalues_diff)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(loadings_diff.shape[1]):  # Iterate over each component\n",
    "    plt.plot(first_diff_df_cleaned.columns, loadings_diff.iloc[:, i], marker='o', linestyle='-', label=f'PC{i+1}')\n",
    "\n",
    "plt.title('Principal Component Loadings')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Loading')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Sample Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_yields   = pc_model.yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_yields[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RMSE \n",
    "e = (pc_yields - combined_df_cleaned)**2\n",
    "rmse = pd.Series(data = e.T.mean())**0.5\n",
    "rmse_year = e.groupby(e.index.year).mean()**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_date = eoyear[-1].strftime(\"%Y-%m-%d\")\n",
    "x = pc_yields.loc[chart_date,:]\n",
    "y = combined_df_cleaned.loc[chart_date,:]\n",
    "e = np.sqrt((x-y)**2)\n",
    "k=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'yield_1Y': 'blue',\n",
    "    'yield_2Y': 'green',\n",
    "    'yield_3Y': 'red',\n",
    "    'yield_4Y': 'purple',\n",
    "    'yield_5Y': 'orange',\n",
    "    'yield_6Y': 'brown',\n",
    "    'yield_7Y': 'pink',\n",
    "    'yield_8Y': 'cyan',\n",
    "    'yield_9Y': 'magenta',\n",
    "    'yield_10Y': 'yellow',\n",
    "    'yield_12Y': 'teal',\n",
    "    'yield_15Y': 'gold',\n",
    "    'yield_20Y': 'navy',\n",
    "    'yield_25Y': 'sienna',\n",
    "    'yield_30Y': 'lime',\n",
    "    'yield_40Y': 'maroon',\n",
    "    'yield_50Y': 'Olive'\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y, x, colors, chart_date, and k are defined\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plotting on the first subplot (ax[0])\n",
    "for col in y.columns:\n",
    "    try:\n",
    "        actual_color = colors[f'{col}']\n",
    "        model_color = colors[f'{col}']\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Color not defined for column '{col}' in 'colors' dictionary.\") from e\n",
    "\n",
    "    ax[0].plot(y[col], label=f\"Actual ({col})\", c=actual_color, linewidth=2, marker=\"o\")\n",
    "    ax[0].plot(x[col], label=f\"Model ({col})\", c=model_color, linestyle='dashed', linewidth=2, marker=\"o\")\n",
    "\n",
    "# Plotting on the second subplot (ax[1])\n",
    "for col in y.columns:\n",
    "    ax[1].plot((y[col] - x[col])**2, label=f\"Squared Errors ({col})\", c=colors[col], linewidth=2, marker=\"o\")\n",
    "\n",
    "# Formatting for the first subplot (ax[0])\n",
    "ax[0].set_xlabel(\"Maturity\")\n",
    "ax[0].set_ylabel(\"Yield\")\n",
    "ax[0].set_title(f\"Yield Curve Comparison ({chart_date})\")\n",
    "\n",
    "# Formatting for the second subplot (ax[1])\n",
    "ax[1].set_xlabel(\"Maturity\")\n",
    "ax[1].set_ylabel(\"Squared Errors\")\n",
    "ax[1].set_title(f\"Goodness-of-fit ({chart_date})\")\n",
    "\n",
    "# Moving legends outside the plot\n",
    "ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Actual and Model\")\n",
    "ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Squared Errors\")\n",
    "\n",
    "# Adjusting subplot spacing\n",
    "plt.subplots_adjust(hspace=0.25)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (15,10))\n",
    "\n",
    "sns.heatmap(rmse_year, \n",
    "            fmt=\".2f\", \n",
    "            cmap = \"OrRd\", \n",
    "            linewidth = 0.3, \n",
    "            annot = True,\n",
    "            cbar = False,\n",
    "            center = 0.1)\n",
    "\n",
    "\n",
    "ax.title.set_text(\"RMSE (k = \"+str(k)+\")\")\n",
    "ax.set(xlabel=\"Maturity\")\n",
    "\n",
    "for label in ax.yaxis.get_ticklabels():\n",
    "    label.set_verticalalignment(\"center\")\n",
    "    label.set_rotation(0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of sample fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_cleaned.index[15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df_cleaned_train = combined_df_cleaned[combined_df_cleaned.index < '2022-11-24 11:00:00']\n",
    "combined_df_cleaned_test = combined_df_cleaned[combined_df_cleaned.index >= '2022-11-24 11:00:00']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA object class\n",
    "temp = pc.PCA(spot=combined_df_cleaned_train, maturities=mat_all, k=k)\n",
    "\n",
    "# Use out-of-sample function to derive model test yields fitting on eigen-vectors from train-set\n",
    "pc_yields_oos = temp.pca_oos(eig_vect_train = temp.eig_vect, spot_test = combined_df_cleaned_test)\n",
    "pc_yields_oos.iloc[:5,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_oos = (pc_yields_oos - combined_df_cleaned_test)**2\n",
    "rmse_oos = pd.Series(data = e_oos.T.mean())**0.5\n",
    "rmse_oos[0] = rmse[rmse_oos.index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,6))\n",
    "color = rainbow([\"in-sample\",\"out-of-sample\"])\n",
    "\n",
    "# Plots\n",
    "ax.plot (rmse, c=color[\"in-sample\"], label=\"in-sample\")\n",
    "ax.plot (rmse_oos, c=color[\"out-of-sample\"], label=\"out-of-sample\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_title (\"Goodness-of-fit (k = \"+ str(k) + \")\")\n",
    "ax.set_ylabel (\"RMSE\")\n",
    "ax.xaxis.set_major_formatter(years_fmt)\n",
    "ax.xaxis.set_major_locator(years_loc)\n",
    "ax.xaxis.set_minor_locator(months_loc)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'H:\\\\Excel\\\\combined_df_1.csv' with the correct path to your CSV file\n",
    "\n",
    "# Load the 'combined_df_1' CSV file into a DataFrame and use the first column as the index\n",
    "combined_df_1 = pd.read_csv('H:\\\\Excel\\\\combined_df_1.csv', index_col=0)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(combined_df_1.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_all = combined_df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import principalcomponents as pc\n",
    "\n",
    "# Calculate the first difference for each column in the DataFrame\n",
    "first_diff_df = combined_df_1.diff()\n",
    "\n",
    "# Drop NaN values after taking the first difference\n",
    "first_diff_df_cleaned = first_diff_df.dropna()\n",
    "\n",
    "# Define the training and out-of-sample periods\n",
    "train_data = first_diff_df_cleaned.loc['2010-01-01':'2020-12-31']\n",
    "oos_data = first_diff_df_cleaned.loc['2021-01-01':]\n",
    "\n",
    "# Fit the PCA model on the training data\n",
    "pc_model_diff = pc.PCA(spot=train_data, maturities=mat_all, k=3)\n",
    "k = 3\n",
    "pc_scores_diff = pc_model_diff.eig_scores_k\n",
    "pc_vect_diff = pc_model_diff.eig_vect_k\n",
    "pc_vect_inv_diff = pc_model_diff.eig_vect_inv_k\n",
    "pc_yields_diff = pc_model_diff.yields\n",
    "pc_idx_diff = pc_model_diff.idx[:k]\n",
    "pc_idx2_diff = pc_model_diff.idx[:3]\n",
    "\n",
    "# Display the principal components\n",
    "print(pc_vect_diff)\n",
    "\n",
    "# You can now apply the trained PCA model on the out-of-sample data if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_yields_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.index = pd.to_datetime(train_data.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "\n",
    "\n",
    "# Calculate the squared errors between the reconstructed yields and the actual first differences\n",
    "e = (pc_yields_diff - train_data) ** 2\n",
    "\n",
    "#\n",
    "# Calculate the RMSE for the entire training period\n",
    "rmse = e.mean(axis=1) ** 0.5\n",
    "\n",
    "# Calculate the RMSE for each year in the training data\n",
    "rmse_year = e.groupby(e.index.year).mean() ** 0.5\n",
    "\n",
    "# Display the RMSE values\n",
    "print(\"Overall RMSE for the training period:\")\n",
    "print(rmse)\n",
    "\n",
    "print(\"\\nYearly RMSE for the training period:\")\n",
    "print(rmse_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import principalcomponents as pc\n",
    "\n",
    "# Calculate the first difference for each column in the DataFrame\n",
    "first_diff_df = combined_df_1\n",
    "\n",
    "# Drop NaN values after taking the first difference\n",
    "first_diff_df_cleaned = first_diff_df.dropna()\n",
    "\n",
    "# Define the training and out-of-sample periods\n",
    "train_data = first_diff_df_cleaned.loc['2010-01-01':'2020-12-31']\n",
    "oos_data = first_diff_df_cleaned.loc['2021-01-01':]\n",
    "\n",
    "# Fit the PCA model on the training data\n",
    "pc_model_diff = pc.PCA(spot=train_data, maturities=mat_all, k=3)\n",
    "k = 3\n",
    "pc_scores_diff = pc_model_diff.eig_scores_k\n",
    "pc_vect_diff = pc_model_diff.eig_vect_k\n",
    "pc_vect_inv_diff = pc_model_diff.eig_vect_inv_k\n",
    "pc_yields_diff = pc_model_diff.yields\n",
    "pc_idx_diff = pc_model_diff.idx[:k]\n",
    "pc_idx2_diff = pc_model_diff.idx[:3]\n",
    "\n",
    "# Display the principal components\n",
    "print(pc_vect_diff)\n",
    "\n",
    "# Calculate the squared errors between the reconstructed yields and the actual first differences\n",
    "e = (pc_yields_diff - train_data) ** 2\n",
    "\n",
    "# Calculate the RMSE for the entire training period\n",
    "rmse = e.mean(axis=1) ** 0.5\n",
    "\n",
    "# Calculate the RMSE for each year in the training data\n",
    "rmse_year = e.groupby(e.index.year).mean() ** 0.5\n",
    "\n",
    "# Display the RMSE values\n",
    "print(\"Overall RMSE for the training period:\")\n",
    "print(rmse)\n",
    "\n",
    "print(\"\\nYearly RMSE for the training period:\")\n",
    "print(rmse_year)\n",
    "\n",
    "# Plot the squared errors for each tenor over time\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plotting squared errors for each maturity/tenor\n",
    "for col in e.columns:\n",
    "    plt.plot(e.index, e[col], label=f'Squared Errors ({col})', linewidth=2)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Squared Errors\")\n",
    "plt.title(\"Squared Errors Over Time for Each Maturity\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (15,10))\n",
    "\n",
    "sns.heatmap(rmse_year, \n",
    "            fmt=\".2f\", \n",
    "            cmap = \"OrRd\", \n",
    "            linewidth = 0.3, \n",
    "            annot = True,\n",
    "            cbar = False,\n",
    "            center = 0.1)\n",
    "\n",
    "\n",
    "ax.title.set_text(\"RMSE (k = \"+str(k)+\")\")\n",
    "ax.set(xlabel=\"Maturity\")\n",
    "\n",
    "for label in ax.yaxis.get_ticklabels():\n",
    "    label.set_verticalalignment(\"center\")\n",
    "    label.set_rotation(0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import ListedColormap\n",
    "import principalcomponents as pc\n",
    "\n",
    "# Assuming that `train_data`, `oos_data`, and `combined_df_cleaned_test` are defined\n",
    "\n",
    "# Use out-of-sample function to derive model test yields fitting on eigen-vectors from the train-set\n",
    "pc_yields_oos = pc_model_diff.pca_oos(eig_vect_train=pc_model_diff.eig_vect, spot_test=oos_data)\n",
    "\n",
    "# Calculate the squared errors for the OOS data\n",
    "e_oos = (pc_yields_oos - oos_data) ** 2\n",
    "\n",
    "# Calculate the RMSE for the out-of-sample period\n",
    "rmse_oos = e_oos.mean(axis=1) ** 0.5\n",
    "\n",
    "# Calculate the RMSE for the in-sample period\n",
    "e_is = (pc_yields_diff - train_data) ** 2\n",
    "rmse_is = e_is.mean(axis=1) ** 0.5\n",
    "\n",
    "# Create a color map for plotting\n",
    "colors = {\"in-sample\": \"blue\", \"out-of-sample\": \"red\"}\n",
    "\n",
    "# Plot the RMSE for in-sample and out-of-sample data\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot in-sample RMSE\n",
    "ax.plot(rmse_is.index, rmse_is, color=colors[\"in-sample\"], label=\"In-Sample\")\n",
    "\n",
    "# Plot out-of-sample RMSE\n",
    "ax.plot(rmse_oos.index, rmse_oos, color=colors[\"out-of-sample\"], label=\"Out-of-Sample\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(f\"Goodness-of-fit (k = {k})\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import ListedColormap\n",
    "import principalcomponents as pc\n",
    "\n",
    "# Assuming that `train_data`, `oos_data`, and `combined_df_cleaned_test` are defined\n",
    "\n",
    "# Use out-of-sample function to derive model test yields fitting on eigen-vectors from the train-set\n",
    "pc_yields_oos = pc_model_diff.pca_oos(eig_vect_train=pc_model_diff.eig_vect, spot_test=oos_data)\n",
    "\n",
    "# Calculate the squared errors for the OOS data\n",
    "e_oos = (pc_yields_oos - oos_data) ** 2\n",
    "\n",
    "# Calculate the RMSE for the out-of-sample period\n",
    "rmse_oos = e_oos.mean(axis=1) ** 0.5\n",
    "\n",
    "# Calculate the RMSE for the in-sample period\n",
    "e_is = (pc_yields_diff - train_data) ** 2\n",
    "rmse_is = e_is.mean(axis=1) ** 0.5\n",
    "\n",
    "# Create a color map for plotting\n",
    "colors = {\"in-sample\": \"blue\", \"out-of-sample\": \"red\"}\n",
    "\n",
    "# Plot the RMSE for in-sample and out-of-sample data\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot in-sample RMSE\n",
    "ax.plot(rmse_is.index, rmse_is, color=colors[\"in-sample\"], label=\"In-Sample\")\n",
    "\n",
    "# Plot out-of-sample RMSE\n",
    "ax.plot(rmse_oos.index, rmse_oos, color=colors[\"out-of-sample\"], label=\"Out-of-Sample\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_title(f\"Comparison of In-Sample and Out-of-Sample RMSE (k = {k})\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'pc_yields_oos' contains the predicted yields for the out-of-sample period\n",
    "# And 'oos_data' contains the actual yields for the same period\n",
    "\n",
    "# Select a specific date or range for comparison\n",
    "oos_date_range = oos_data.index  # Use the full out-of-sample period or a specific range\n",
    "\n",
    "# Retrieve the actual and predicted yields for this period\n",
    "actual_yields_oos = oos_data.loc[oos_date_range, :]\n",
    "predicted_yields_oos = pc_yields_oos.loc[oos_date_range, :]\n",
    "\n",
    "# Plotting the comparison\n",
    "plt.figure(figsize=(15, 8))\n",
    "for col in actual_yields_oos.columns:\n",
    "    plt.plot(actual_yields_oos.index, actual_yields_oos[col], label=f\"Actual {col}\", linestyle='-', marker=\"o\")\n",
    "    plt.plot(predicted_yields_oos.index, predicted_yields_oos[col], label=f\"Predicted {col}\", linestyle='--', marker=\"x\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Yield\")\n",
    "plt.title(\"Comparison of Actual and Predicted Yields (Out-of-Sample)\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify the tenor you want to compare (e.g., 'yield_10Y')\n",
    "tenor = 'yield_10Y'\n",
    "\n",
    "# Retrieve the actual and predicted yields for this specific tenor\n",
    "actual_yields_oos = oos_data.loc[oos_data.index, tenor]\n",
    "predicted_yields_oos = pc_yields_oos.loc[pc_yields_oos.index, tenor]\n",
    "\n",
    "# Plotting the comparison for the specified tenor\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(actual_yields_oos.index, actual_yields_oos, label=f\"Actual {tenor}\", linestyle='-', marker=\"o\")\n",
    "plt.plot(predicted_yields_oos.index, predicted_yields_oos, label=f\"Predicted {tenor}\", linestyle='--', marker=\"x\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Yield\")\n",
    "plt.title(f\"Comparison of Actual and Predicted Yields (Out-of-Sample) for {tenor}\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://davidstutz.de/projects/probabilistic-pca/\n",
    "\n",
    "https://medium.com/practical-coding/the-simplest-generative-model-you-probably-missed-c840d68b704\n",
    "\n",
    "https://github.com/smrfeld/python_prob_pca_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ppca-rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jaquesgrobler.github.io/online-sklearn-build/modules/generated/sklearn.decomposition.ProbabilisticPCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "combined_df_cleaned = combined_df.dropna()\n",
    "pca = PCA(n_components=3)\n",
    "transformed_data = pca.fit_transform(combined_df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'loadings' is defined as follows:\n",
    "loadings = pca.components_\n",
    "print(\"PCA Loadings (Components):\\n\", loadings)\n",
    "\n",
    "# Get the transpose of the loadings\n",
    "loadings_transposed = loadings.T\n",
    "print(\"Transposed Loadings:\\n\", loadings_transposed)\n",
    "\n",
    "# To check the shape of the transposed loadings\n",
    "print(\"Shape of Transposed Loadings:\", loadings_transposed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Assuming loadings_transposed is a numpy array and combined_df_cleaned.columns contains the column names\n",
    "for i in range(loadings_transposed.shape[1]):  # Iterate over each component\n",
    "    plt.plot(combined_df_cleaned.columns, loadings_transposed[:, i], marker='o', linestyle='-', label=f'PC{i+1}')\n",
    "\n",
    "plt.title('Probabilistic Principal Component Loadings')\n",
    "plt.xlabel('Columns')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability if needed\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "D = 1\n",
    "N = combined_df_cleaned['yield_10Y'].shape[0]  \n",
    "K = 3  # Number of latent dimensions\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "w = tfd.Normal(loc=tf.zeros([D, K]), scale=2.0 * tf.ones([D, K]))\n",
    "z = tfd.Normal(loc=tf.zeros([N, K]), scale=tf.ones([N, K]))\n",
    "x= combined_df_cleaned['yield_10Y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw = tfd.Normal(loc=tf.Variable(\"qw/loc\", [D, K]),\n",
    "            scale=tf.nn.softplus(tf.Variable(\"qw/scale\", [D, K])))\n",
    "qz = tfd.Normal(loc=tf.Variable(\"qz/loc\", [N, K]),\n",
    "            scale=tf.nn.softplus(tf.Variable(\"qz/scale\", [N, K])))\n",
    "\n",
    "inference = tfd.kldivergence({w: qw, z: qz}, data={x: x_train})\n",
    "inference.run(n_iter=500, n_print=100, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Assuming combined_df_cleaned['yield_10Y'] is a pandas Series or numpy array\n",
    "x_observed = combined_df_cleaned['yield_10Y'].values.astype(np.float32)\n",
    "N = x_observed.shape[0]  # Number of observations\n",
    "D = 1  # Dimensionality of observed data\n",
    "K = 3  # Number of latent dimensions\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Prior distributions for latent variables\n",
    "prior_w = tfd.Normal(loc=tf.zeros([D, K]), scale=2.0 * tf.ones([D, K]))\n",
    "prior_z = tfd.Normal(loc=tf.zeros([N, K]), scale=tf.ones([N, K]))\n",
    "\n",
    "# Define trainable variables for variational distributions\n",
    "qw_loc = tf.Variable(tf.random.normal([D, K]), name='qw_loc')\n",
    "qw_scale = tf.nn.softplus(tf.Variable(tf.random.normal([D, K]), name='qw_scale'))\n",
    "qz_loc = tf.Variable(tf.random.normal([N, K]), name='qz_loc')\n",
    "qz_scale = tf.nn.softplus(tf.Variable(tf.random.normal([N, K]), name='qz_scale'))\n",
    "\n",
    "# Variational distributions for latent variables\n",
    "qw = tfd.Normal(loc=qw_loc, scale=qw_scale)\n",
    "qz = tfd.Normal(loc=qz_loc, scale=qz_scale)\n",
    "\n",
    "# Sample from variational distributions\n",
    "w_sample = qw.sample()\n",
    "z_sample = qz.sample()\n",
    "\n",
    "# Likelihood of observed data given latent variables\n",
    "x_dist = tfd.Normal(loc=tf.matmul(w_sample, z_sample, transpose_b=True), scale=tf.ones([D, N]))\n",
    "\n",
    "# Log-likelihood\n",
    "log_likelihood = tf.reduce_sum(x_dist.log_prob(x_observed))\n",
    "\n",
    "# KL divergence between variational distributions and priors\n",
    "kl_divergence = tf.reduce_sum(tfd.kl_divergence(qw, prior_w)) + tf.reduce_sum(tfd.kl_divergence(qz, prior_z))\n",
    "\n",
    "# ELBO (Evidence Lower Bound) as the objective function\n",
    "elbo = log_likelihood - kl_divergence\n",
    "\n",
    "\n",
    "# After training, you can extract the learned parameters w_sample and z_sample\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_steps = 1000\n",
    "log_interval = 100\n",
    "\n",
    "for step in range(num_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = -elbo\n",
    "\n",
    "    gradients = tape.gradient(loss, [qw_loc, qw_scale, qz_loc, qz_scale])\n",
    "    optimizer.apply_gradients(zip(gradients, [qw_loc, qw_scale, qz_loc, qz_scale]))\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f'Step {step}, Loss: {loss.numpy()}')\n",
    "\n",
    "# After training, you can extract the learned parameters w_sample and z_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYMC3 library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pytensor.tensor as tt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D, K, sigma=1):\n",
    "    x_train = np.zeros((D, N))\n",
    "    w = np.random.normal(0.0, 2.0, size=(D, K))\n",
    "    z = np.random.normal(0.0, 1.0, size=(K, N))\n",
    "    mean = np.dot(w, z)\n",
    "    for d in range(D):\n",
    "        for n in range(N):\n",
    "            x_train[d, n] = np.random.normal(mean[d, n], sigma)\n",
    "\n",
    "    print(\"True principal axes:\")\n",
    "    print(w)\n",
    "    return x_train\n",
    "\n",
    "N = 5000  # number of data points\n",
    "D = 2  # data dimensionality\n",
    "K = 1  # latent dimensionality\n",
    "\n",
    "x_train = build_toy_dataset(N, D, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train[0, :], x_train[1, :], color='blue', alpha=0.1)\n",
    "plt.axis([-10, 10, -10, 10])\n",
    "plt.title(\"Simulated data set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as PPCA:\n",
    "    w = pm.Normal('w', mu=tt.zeros([D, K]), sigma=2.0 * tt.ones([D, K]), shape=[D, K])\n",
    "    z = pm.Normal('z', mu=tt.zeros([N, K]), sigma=tt.ones([N, K]), shape=[N, K])\n",
    "    x = pm.Normal('x', mu=w.dot(z.T), sigma=tt.ones([D, N]), shape=[D, N], observed=x_train)  \n",
    "\n",
    "    trace = pm.sample(draws=10, tune=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_slim as slim\n",
    "from edward.models import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Check for NaN values in the entire DataFrame\n",
    "if first_diff_df_cleaned.isnull().any().any():\n",
    "    print(\"DataFrame has NaN values\")\n",
    "else:\n",
    "    print(\"DataFrame does not have NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x= combined_df_cleaned['yield_10Y']\n",
    "\n",
    "# Assuming 'x' is a pandas Series\n",
    "# Convert the Series to a numpy array and reshape it\n",
    "X = x.values.reshape(-1, 1)  # Convert to 2D array (16391 samples, 1 feature)\n",
    "\n",
    "# Initialize a dictionary to store the number of components and their corresponding log likelihoods\n",
    "log_likelihoods = {}\n",
    "\n",
    "# Loop over a range of n_components, for example from 1 to 10\n",
    "for n in range(1, 11):  # From 1 to 10 components\n",
    "    # Create an instance of GaussianHMM with varying n_components\n",
    "    model = hmm.GaussianHMM(n_components=n, covariance_type=\"diag\", n_iter=100)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Evaluate its likelihood on the same data\n",
    "    log_likelihood = model.score(X)\n",
    "    \n",
    "    # Print the log likelihood\n",
    "    print(f\"Log Likelihood for {n} components: {log_likelihood}\")\n",
    "    \n",
    "    # Store the result\n",
    "    log_likelihoods[n] = log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of yield tenors to analyze\n",
    "tenors = mat_all  # Add or modify as per your actual columns\n",
    "\n",
    "# Dictionary to store results for each tenor\n",
    "tenor_results = {}\n",
    "\n",
    "# Process each tenor\n",
    "for tenor in tenors:\n",
    "    X = combined_df_cleaned[tenor].values.reshape(-1, 1)  # Reshape data for HMM\n",
    "    \n",
    "    # Dictionary to store log likelihoods for this tenor across different numbers of components\n",
    "    log_likelihoods = {}\n",
    "    \n",
    "    # Loop over a range of n_components, from 1 to 10\n",
    "    for n in range(1, 11):  # From 1 to 10 components\n",
    "        # Create an instance of GaussianHMM with varying n_components\n",
    "        model = hmm.GaussianHMM(n_components=n, covariance_type=\"diag\", n_iter=1000, random_state=42)\n",
    "        \n",
    "        # Fit the model to the data\n",
    "        model.fit(X)\n",
    "        \n",
    "        # Evaluate its likelihood on the same data\n",
    "        log_likelihood = model.score(X)\n",
    "        \n",
    "        # Print the log likelihood for this number of components and tenor\n",
    "        print(f\"Log Likelihood for {tenor} with {n} components: {log_likelihood}\")\n",
    "        \n",
    "        # Store the result\n",
    "        log_likelihoods[n] = log_likelihood\n",
    "    \n",
    "    # Store results for this tenor\n",
    "    tenor_results[tenor] = log_likelihoods\n",
    "\n",
    "# Convert results to a DataFrame for better visualization or analysis\n",
    "results_df = pd.DataFrame(tenor_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "tenors = results_df.columns.tolist()  # List of tenors from columns of results_df\n",
    "\n",
    "n = len(tenors)\n",
    "col = 4  # Number of columns per row in the subplot\n",
    "row = int(np.ceil(n / col))  # Calculate the number of rows needed\n",
    "fig_height = row * 6  # Height of the figure\n",
    "fig_width = col * 8  # Width of the figure\n",
    "\n",
    "# Plotting the DataFrame with subplots\n",
    "fig, axes = plt.subplots(nrows=row, ncols=col, figsize=(fig_width, fig_height), sharey=True, sharex=True)\n",
    "fig.suptitle('Likelihood vs. Components for Different Tenors', fontsize=16)\n",
    "\n",
    "# Flatten axes if there is only one row or one column to simplify indexing\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Iterate over tenors and plot on each subplot\n",
    "for i, tenor in enumerate(tenors):\n",
    "    ax = axes_flat[i]\n",
    "    ax.plot(results_df.index, results_df[tenor], marker='o')\n",
    "    ax.set_title(f'{tenor}')\n",
    "    ax.set_xlabel('Number of Components')\n",
    "    ax.set_ylabel('Log Likelihood')\n",
    "\n",
    "# Hide empty subplots if there are any\n",
    "for j in range(n, len(axes_flat)):\n",
    "    axes_flat[j].axis('off')\n",
    "\n",
    "# Adjust the layout and margins\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.92)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'combined_df_cleaned' is your DataFrame and it has been preprocessed\n",
    "# List of yield tenors to analyze\n",
    "tenors = mat_all  # Add or modify as per your actual columns\n",
    "\n",
    "# Dictionary to store results for each tenor\n",
    "tenor_results = {}\n",
    "\n",
    "# Process each tenor\n",
    "for tenor in tenors:\n",
    "    X = combined_df_cleaned[tenor].values.reshape(-1, 1)  # Reshape data for HMM\n",
    "    \n",
    "    # Create an instance of GaussianHMM with a chosen number of components\n",
    "    n_components = 10  # Adjust as needed\n",
    "    model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=1000, random_state=42)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Obtain the hidden states for the data points\n",
    "    hmm_states = model.predict(X)\n",
    "    \n",
    "    # Get unique states\n",
    "    unique_states = np.unique(hmm_states)\n",
    "    \n",
    "    # Store the hidden states and unique states for this tenor\n",
    "    tenor_results[tenor] = {\n",
    "        'hmm_states': hmm_states,\n",
    "        'unique_states': unique_states\n",
    "    }\n",
    "\n",
    "# Print or use the results as needed\n",
    "for tenor, results in tenor_results.items():\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Hidden States:\")\n",
    "    print(results['hmm_states'])\n",
    "    print(\"Unique States:\")\n",
    "    print(results['unique_states'])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter states:\n",
    "hmm_state_0 = hmm_states == 0\n",
    "hmm_state_1 = hmm_states == 1\n",
    "hmm_state_2 = hmm_states == 2\n",
    "hmm_state_3 = hmm_states == 3\n",
    "hmm_state_4 = hmm_states == 4\n",
    "hmm_state_5 = hmm_states == 5\n",
    "hmm_state_6 = hmm_states == 6\n",
    "hmm_state_7 = hmm_states == 7\n",
    "hmm_state_8 = hmm_states == 8\n",
    "hmm_state_9 = hmm_states == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))  # Set the figure size for better readability\n",
    " \n",
    "plt.plot(combined_df_cleaned.index, combined_df_cleaned['yield_50Y'], label=column)\n",
    "\n",
    "plt.title('Time Series Plot for All DataFrames')  # Add a title\n",
    "plt.xlabel('Date')  # X-axis label\n",
    "plt.ylabel('Yield')  # Y-axis label\n",
    "plt.legend()  # Add a legend to distinguish the lines\n",
    "plt.grid(True)  # Add a grid for easier readability\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "close_price = combined_df_cleaned['yield_50Y']\n",
    "print(close_price.shape)\n",
    "hmm_states = model.predict(X)\n",
    "print(close_price.shape)\n",
    "var_hidden_state_0 = np.cov(close_price[hmm_state_0])\n",
    "print(close_price[hmm_state_0].shape)\n",
    "print(var_hidden_state_0)\n",
    "var_hidden_state_1 = np.cov(close_price[hmm_state_1])\n",
    "print(close_price[hmm_state_1].shape)\n",
    "print(var_hidden_state_1)\n",
    "print(close_price[hmm_state_2].shape)\n",
    "var_hidden_state_2 = np.cov(close_price[hmm_state_2])\n",
    "print(var_hidden_state_2)\n",
    "var_hidden_state_3 = np.cov(close_price[hmm_state_3])\n",
    "print(close_price[hmm_state_3].shape)\n",
    "print(var_hidden_state_3)\n",
    "var_hidden_state_4 = np.cov(close_price[hmm_state_4])\n",
    "print(close_price[hmm_state_4].shape)\n",
    "print(var_hidden_state_4)\n",
    "var_hidden_state_5 = np.cov(close_price[hmm_state_5])\n",
    "print(close_price[hmm_state_5].shape)\n",
    "print(var_hidden_state_5)\n",
    "var_hidden_state_6 = np.cov(close_price[hmm_state_6])\n",
    "print(close_price[hmm_state_6].shape)\n",
    "print(var_hidden_state_6)\n",
    "var_hidden_state_7 = np.cov(close_price[hmm_state_7])\n",
    "print(close_price[hmm_state_7].shape)\n",
    "print(var_hidden_state_7)\n",
    "var_hidden_state_8 = np.cov(close_price[hmm_state_8])\n",
    "print(close_price[hmm_state_8].shape)\n",
    "print(var_hidden_state_8)\n",
    "var_hidden_state_9 = np.cov(close_price[hmm_state_9])\n",
    "print(close_price[hmm_state_9].shape)\n",
    "print(var_hidden_state_9)\n",
    "# Plot states:\n",
    "default = 'white'\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "# Data:\n",
    "markov_states = fig.add_subplot(111)\n",
    "plt.plot_date(close_price.index[hmm_state_0],close_price[hmm_state_0], marker='*', ms=5, alpha=0.4, label='Hidden State 0')\n",
    "plt.plot_date(close_price.index[hmm_state_1],close_price[hmm_state_1], marker='*', ms=5, alpha=0.4, label='Hidden State 1')\n",
    "plt.plot_date(close_price.index[hmm_state_2],close_price[hmm_state_2], marker='*', ms=5, alpha=0.4, label='Hidden State 2')\n",
    "plt.plot_date(close_price.index[hmm_state_3],close_price[hmm_state_3], marker='*', ms=5, alpha=0.4, label='Hidden State 3')\n",
    "plt.plot_date(close_price.index[hmm_state_4],close_price[hmm_state_4], marker='*', ms=5, alpha=0.4, label='Hidden State 4')\n",
    "plt.plot_date(close_price.index[hmm_state_5],close_price[hmm_state_5], marker='*', ms=5, alpha=0.4, label='Hidden State 5')\n",
    "plt.plot_date(close_price.index[hmm_state_6],close_price[hmm_state_6], marker='*', ms=5, alpha=0.4, label='Hidden State 6')\n",
    "plt.plot_date(close_price.index[hmm_state_7],close_price[hmm_state_7], marker='*', ms=5, alpha=0.4, label='Hidden State 7')\n",
    "plt.plot_date(close_price.index[hmm_state_8],close_price[hmm_state_8], marker='*', ms=5, alpha=0.4, label='Hidden State 8')\n",
    "plt.plot_date(close_price.index[hmm_state_9],close_price[hmm_state_9], marker='*', ms=5, alpha=0.4, label='Hidden State 9')\n",
    "# Aesthetic adjustments:\n",
    "markov_states.set_title('Hidden Markov States', fontsize=30, color=default)\n",
    "markov_states.set_xlabel(\"Date\", fontsize=20, color=default)\n",
    "markov_states.set_ylabel(\"Closing Price\", fontsize=20, color=default)\n",
    "markov_states.tick_params(axis='both', labelsize=15, colors=default)\n",
    "markov_states.legend(loc='upper left', fontsize=15)\n",
    "# Generate and save graph:\n",
    "plt.savefig('hmm_graph.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "\n",
    "close_price = combined_df_cleaned['yield_50Y']\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "markov_states = fig.add_subplot(111)\n",
    "plt.plot(combined_df_cleaned.index, combined_df_cleaned['yield_50Y'], label='yield_50Y')\n",
    "plt.plot_date(close_price.index[hmm_state_0],close_price[hmm_state_0], marker='*', ms=5, alpha=0.4, label='Hidden State 0')\n",
    "plt.plot_date(close_price.index[hmm_state_1],close_price[hmm_state_1], marker='*', ms=5, alpha=0.4, label='Hidden State 1')\n",
    "plt.plot_date(close_price.index[hmm_state_2],close_price[hmm_state_2], marker='*', ms=5, alpha=0.4, label='Hidden State 2')\n",
    "plt.plot_date(close_price.index[hmm_state_3],close_price[hmm_state_3], marker='*', ms=5, alpha=0.4, label='Hidden State 3')\n",
    "plt.plot_date(close_price.index[hmm_state_4],close_price[hmm_state_4], marker='*', ms=5, alpha=0.4, label='Hidden State 4')\n",
    "plt.plot_date(close_price.index[hmm_state_5],close_price[hmm_state_5], marker='*', ms=5, alpha=0.4, label='Hidden State 5')\n",
    "plt.plot_date(close_price.index[hmm_state_6],close_price[hmm_state_6], marker='*', ms=5, alpha=0.4, label='Hidden State 6')\n",
    "plt.plot_date(close_price.index[hmm_state_7],close_price[hmm_state_7], marker='*', ms=5, alpha=0.4, label='Hidden State 7')\n",
    "plt.plot_date(close_price.index[hmm_state_8],close_price[hmm_state_8], marker='*', ms=5, alpha=0.4, label='Hidden State 8')\n",
    "plt.plot_date(close_price.index[hmm_state_9],close_price[hmm_state_9], marker='*', ms=5, alpha=0.4, label='Hidden State 9')\n",
    "# Aesthetic adjustments:\n",
    "# Aesthetic adjustments:\n",
    "markov_states.set_title('Hidden Markov States', fontsize=30, color=default)\n",
    "markov_states.set_xlabel(\"Date\", fontsize=20, color=default)\n",
    "markov_states.set_ylabel(\"Closing Price\", fontsize=20, color=default)\n",
    "markov_states.tick_params(axis='both', labelsize=15, colors=default)\n",
    "markov_states.legend(loc='upper left', fontsize=15)\n",
    "ax.set_title('Time Series Plot with Hidden Markov States')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Yield')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Assuming model is defined appropriately\n",
    "# Example: Assuming combined_df_cleaned is a DataFrame with yield tenors as columns\n",
    "\n",
    "# Get list of all yield tenors\n",
    "yield_tenors = combined_df_cleaned.columns\n",
    "\n",
    "# Calculate number of tenors and define subplot layout\n",
    "n = len(yield_tenors)\n",
    "col = 2  # Number of columns per row in the subplot\n",
    "row = int(np.ceil(n / col))  # Calculate the number of rows needed\n",
    "fig_height = row * 6  # Height of the figure\n",
    "fig_width = col * 8  # Width of the figure\n",
    "\n",
    "# Create figure and axes\n",
    "fig, axes = plt.subplots(nrows=row, ncols=col, figsize=(fig_width, fig_height), sharey=True, sharex=True)\n",
    "fig.suptitle('Hidden Markov States for Different Tenors', fontsize=16)\n",
    "\n",
    "# Flatten axes if there is only one row or one column to simplify indexing\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Iterate through each yield tenor and plot its hidden states\n",
    "for i, tenor in enumerate(yield_tenors):\n",
    "    close_price = combined_df_cleaned[tenor]\n",
    "    X = close_price.values.reshape(-1, 1)  # Reshape data for HMM input\n",
    "    \n",
    "    # Predict hidden states\n",
    "    hmm_states = model.predict(X)\n",
    "    \n",
    "    # Plot each state separately\n",
    "    for state in np.unique(hmm_states):\n",
    "        state_indices = hmm_states == state\n",
    "        axes_flat[i].plot(close_price.index[state_indices], close_price[state_indices], marker='*', linestyle='', ms=5, alpha=0.4, label=f'Hidden State {state}')\n",
    "    \n",
    "    # Set title and labels for each subplot\n",
    "    axes_flat[i].set_title(f'{tenor}', fontsize=14)\n",
    "    axes_flat[i].set_xlabel(\"Date\", fontsize=12)\n",
    "    axes_flat[i].set_ylabel(\"Closing Price\", fontsize=12)\n",
    "    axes_flat[i].tick_params(axis='both', labelsize=10)\n",
    "    axes_flat[i].legend(loc='upper left', fontsize=10)\n",
    "\n",
    "# Hide empty subplots if there are any\n",
    "for j in range(n, len(axes_flat)):\n",
    "    axes_flat[j].axis('off')\n",
    "\n",
    "# Adjust the layout and margins\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.92)\n",
    "\n",
    "# Save and display the plot\n",
    "plt.savefig('hmm_graphs.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "first_diff_df_cleaned = first_diff_df.dropna()\n",
    "# Assuming your DataFrame is named df\n",
    "# Check for NaN values in the entire DataFrame\n",
    "if first_diff_df_cleaned.isnull().any().any():\n",
    "    print(\"DataFrame has NaN values\")\n",
    "else:\n",
    "    print(\"DataFrame does not have NaN values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of yield tenors to analyze\n",
    "tenors = mat_all  # Add or modify as per your actual columns\n",
    "\n",
    "# Dictionary to store results for each tenor\n",
    "tenor_results = {}\n",
    "\n",
    "# Process each tenor\n",
    "for tenor in tenors:\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)  # Reshape data for HMM\n",
    "    \n",
    "    # Dictionary to store log likelihoods for this tenor across different numbers of components\n",
    "    log_likelihoods = {}\n",
    "    \n",
    "    # Loop over a range of n_components, from 1 to 10\n",
    "    for n in range(1, 11):  # From 1 to 10 components\n",
    "        # Create an instance of GaussianHMM with varying n_components\n",
    "        model = hmm.GaussianHMM(n_components=n, covariance_type=\"diag\", n_iter=1000, random_state=42)\n",
    "        \n",
    "        # Fit the model to the data\n",
    "        model.fit(X)\n",
    "        \n",
    "        # Evaluate its likelihood on the same data\n",
    "        log_likelihood = model.score(X)\n",
    "        \n",
    "        # Print the log likelihood for this number of components and tenor\n",
    "        print(f\"Log Likelihood for {tenor} with {n} components: {log_likelihood}\")\n",
    "        \n",
    "        # Store the result\n",
    "        log_likelihoods[n] = log_likelihood\n",
    "    \n",
    "    # Store results for this tenor\n",
    "    tenor_results[tenor] = log_likelihoods\n",
    "\n",
    "# Convert results to a DataFrame for better visualization or analysis\n",
    "results_df = pd.DataFrame(tenor_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(tenors)\n",
    "col = 4  # Number of columns per row in the subplot\n",
    "row = int(np.ceil(n / col))  # Calculate the number of rows needed\n",
    "fig_height = row * 6  # Height of the figure\n",
    "fig_width = col * 8  # Width of the figure\n",
    "\n",
    "# Plotting the DataFrame with subplots\n",
    "fig, axes = plt.subplots(nrows=row, ncols=col, figsize=(fig_width, fig_height), sharey=True, sharex=True)\n",
    "fig.suptitle('Likelihood vs. Components for Different Tenors', fontsize=16)\n",
    "\n",
    "# Flatten axes if there is only one row or one column to simplify indexing\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, tenor in enumerate(tenors):\n",
    "    ax = axes_flat[i]\n",
    "    ax.plot(results_df.index, results_df[tenor], marker='o')\n",
    "    ax.set_title(f'{tenor}')\n",
    "    ax.set_xlabel('Number of Components')\n",
    "    ax.set_ylabel('Log Likelihood')\n",
    "\n",
    "# Hide empty subplots if there are any\n",
    "for j in range(i + 1, len(axes_flat)):\n",
    "    axes_flat[j].axis('off')\n",
    "\n",
    "# Adjust the layout and margins\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.92)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'combined_df_cleaned' is your DataFrame and it has been preprocessed\n",
    "# List of yield tenors to analyze\n",
    "tenors = mat_all  # Add or modify as per your actual columns\n",
    "\n",
    "# Dictionary to store results for each tenor\n",
    "tenor_results = {}\n",
    "\n",
    "# Process each tenor\n",
    "for tenor in tenors:\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)  # Reshape data for HMM\n",
    "    \n",
    "    # Create an instance of GaussianHMM with a chosen number of components\n",
    "    n_components = 10  # Adjust as needed\n",
    "    model = hmm.GaussianHMM(n_components=n, covariance_type=\"diag\", n_iter=1000, random_state=42)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Obtain the hidden states for the data points\n",
    "    hmm_states = model.predict(X)\n",
    "    \n",
    "    # Get unique states\n",
    "    unique_states = np.unique(hmm_states)\n",
    "    \n",
    "    # Store the hidden states and unique states for this tenor\n",
    "    tenor_results[tenor] = {\n",
    "        'hmm_states': hmm_states,\n",
    "        'unique_states': unique_states\n",
    "    }\n",
    "\n",
    "# Print or use the results as needed\n",
    "for tenor, results in tenor_results.items():\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Hidden States:\")\n",
    "    print(results['hmm_states'])\n",
    "    print(\"Unique States:\")\n",
    "    print(results['unique_states'])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'combined_df_cleaned' is your DataFrame and it has been preprocessed\n",
    "# List of yield tenors to analyze\n",
    "tenors = mat_all  # Add or modify as per your actual columns\n",
    "\n",
    "# Dictionary to store results for each tenor\n",
    "tenor_results = {}\n",
    "\n",
    "# Process each tenor\n",
    "for tenor in tenors:\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)  # Reshape data for HMM\n",
    "    \n",
    "    # Create an instance of GaussianHMM with a chosen number of components\n",
    "    n_components = 10  # Adjust as needed\n",
    "    model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Obtain the hidden states for the data points\n",
    "    hmm_states = model.predict(X)\n",
    "    \n",
    "    # Get unique states\n",
    "    unique_states = np.unique(hmm_states)\n",
    "    \n",
    "    # Store the hidden states and unique states for this tenor\n",
    "    tenor_results[tenor] = {\n",
    "        'hmm_states': hmm_states,\n",
    "        'unique_states': unique_states\n",
    "    }\n",
    "\n",
    "# Print or use the results as needed\n",
    "for tenor, results in tenor_results.items():\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Hidden States:\")\n",
    "    print(results['hmm_states'])\n",
    "    print(\"Unique States:\")\n",
    "    print(results['unique_states'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'combined_df_cleaned' is your DataFrame and it has been preprocessed\n",
    "# List of yield tenors to analyze\n",
    "tenors = mat_all  # Add or modify as per your actual columns\n",
    "\n",
    "# Dictionary to store results for each tenor\n",
    "tenor_results = {}\n",
    "\n",
    "# Process each tenor\n",
    "for tenor in tenors:\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)  # Reshape data for HMM\n",
    "    \n",
    "    # Create an instance of GaussianHMM with a chosen number of components\n",
    "    n_components = 10  # Adjust as needed\n",
    "    model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Obtain the hidden states for the data points\n",
    "    hmm_states = model.predict(X)\n",
    "    \n",
    "    # Get unique states\n",
    "    unique_states = np.unique(hmm_states)\n",
    "    \n",
    "    # Get the transition matrix\n",
    "    transmat = model.transmat_\n",
    "    \n",
    "    # Store the hidden states, unique states, and transition matrix for this tenor\n",
    "    tenor_results[tenor] = {\n",
    "        'hmm_states': hmm_states,\n",
    "        'unique_states': unique_states,\n",
    "        'transmat': transmat\n",
    "    }\n",
    "\n",
    "    # Plot the transition matrix heat map\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(transmat, cmap='viridis')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_title(f'Transition Matrix Heat Map - Tenor: {tenor}')\n",
    "    ax.set_xlabel('To State')\n",
    "    ax.set_ylabel('From State')\n",
    "    \n",
    "    # Ensure correct tick labels and locations\n",
    "    ax.set_xticks(np.arange(transmat.shape[1]))\n",
    "    ax.set_yticks(np.arange(transmat.shape[0]))\n",
    "    ax.set_xticklabels(np.arange(10))\n",
    "    ax.set_yticklabels(np.arange(10))\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned.shape, close_price.shape,X.shape,hmm_states.shape,set(hmm_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter states_YD_FD:\n",
    "hmm_state_0 = hmm_states == 0\n",
    "hmm_state_1 = hmm_states == 1\n",
    "hmm_state_2 = hmm_states == 2\n",
    "hmm_state_3 = hmm_states == 3\n",
    "hmm_state_4 = hmm_states == 4\n",
    "hmm_state_5 = hmm_states == 5\n",
    "hmm_state_6 = hmm_states == 6\n",
    "hmm_state_7 = hmm_states == 7\n",
    "hmm_state_8 = hmm_states == 8\n",
    "hmm_state_9 = hmm_states == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "X = first_diff_df_cleaned['yield_50Y'].values.reshape(-1, 1)\n",
    "model = hmm.GaussianHMM(n_components=10, covariance_type=\"diag\", n_iter=100,random_state=42)\n",
    "model.fit(X)\n",
    "close_price = first_diff_df_cleaned['yield_50Y']\n",
    "hmm_states = model.predict(X)\n",
    "# Plot states:\n",
    "default = 'white'\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "# Data:\n",
    "markov_states = fig.add_subplot(111)\n",
    "plt.plot_date(close_price.index[hmm_state_0],close_price[hmm_state_0], marker='*', ms=5, alpha=0.4, label='Hidden State 0')\n",
    "plt.plot_date(close_price.index[hmm_state_1],close_price[hmm_state_1], marker='*', ms=5, alpha=0.4, label='Hidden State 1')\n",
    "plt.plot_date(close_price.index[hmm_state_2],close_price[hmm_state_2], marker='*', ms=5, alpha=0.4, label='Hidden State 2')\n",
    "plt.plot_date(close_price.index[hmm_state_3],close_price[hmm_state_3], marker='*', ms=5, alpha=0.4, label='Hidden State 3')\n",
    "plt.plot_date(close_price.index[hmm_state_4],close_price[hmm_state_4], marker='*', ms=5, alpha=0.4, label='Hidden State 4')\n",
    "plt.plot_date(close_price.index[hmm_state_5],close_price[hmm_state_5], marker='*', ms=5, alpha=0.4, label='Hidden State 5')\n",
    "plt.plot_date(close_price.index[hmm_state_6],close_price[hmm_state_6], marker='*', ms=5, alpha=0.4, label='Hidden State 6')\n",
    "plt.plot_date(close_price.index[hmm_state_7],close_price[hmm_state_7], marker='*', ms=5, alpha=0.4, label='Hidden State 7')\n",
    "plt.plot_date(close_price.index[hmm_state_8],close_price[hmm_state_8], marker='*', ms=5, alpha=0.4, label='Hidden State 8')\n",
    "plt.plot_date(close_price.index[hmm_state_9],close_price[hmm_state_9], marker='*', ms=5, alpha=0.4, label='Hidden State 9')\n",
    "# Aesthetic adjustments:\n",
    "markov_states.set_title('Hidden Markov States', fontsize=30, color=default)\n",
    "markov_states.set_xlabel(\"Date\", fontsize=20, color=default)\n",
    "markov_states.set_ylabel(\"Closing Price\", fontsize=20, color=default)\n",
    "markov_states.tick_params(axis='both', labelsize=15, colors=default)\n",
    "markov_states.legend(loc='upper left', fontsize=15)\n",
    "# Generate and save graph:\n",
    "plt.savefig('hmm_graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X = first_diff_df_cleaned['yield_50Y'].values.reshape(-1, 1)\n",
    "model = hmm.GaussianHMM(n_components=10, covariance_type=\"diag\", n_iter=100,random_state=42)\n",
    "model.fit(X)\n",
    "close_price = first_diff_df_cleaned['yield_50Y']\n",
    "hmm_states = model.predict(X)\n",
    "\n",
    "# Determine the number of unique states to plot\n",
    "num_states_to_plot = 3  # Plot Hidden State 0 and Hidden State 1\n",
    "\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(25, 15))\n",
    "\n",
    "# Data:\n",
    "markov_states = fig.add_subplot(111)\n",
    "\n",
    "# Plot data for each selected hidden state\n",
    "for state in range(num_states_to_plot):\n",
    "    state_indices = hmm_states == state\n",
    "    plt.plot_date(close_price.index[state_indices], close_price[state_indices],\n",
    "                  marker='*', ms=5, alpha=0.4, label=f'Hidden State {state}')\n",
    "\n",
    "# Aesthetic adjustments\n",
    "markov_states.set_title('Hidden Markov States (First Three States)', fontsize=30, color='black')\n",
    "markov_states.set_xlabel(\"Date\", fontsize=20, color='black')\n",
    "markov_states.set_ylabel(\"Closing Price\", fontsize=20, color='black')\n",
    "markov_states.tick_params(axis='both', labelsize=15, colors='black')\n",
    "markov_states.legend(loc='upper left', fontsize=15)\n",
    "\n",
    "# Generate and save graph\n",
    "plt.savefig('hmm_graph_first_two_states.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and 'yield_50Y' column are defined correctly\n",
    "\n",
    "# Prepare the data for HMM\n",
    "X = first_diff_df_cleaned['yield_50Y'].values.reshape(-1, 1)\n",
    "model = hmm.GaussianHMM(n_components=10, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "model.fit(X)\n",
    "regimes = model.predict(X)\n",
    "print(np.unique(regimes))\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Plot yield data\n",
    "ax.plot(first_diff_df_cleaned.index, first_diff_df_cleaned['yield_50Y'], label='Yield', color='black')\n",
    "\n",
    "# Visualize the regimes\n",
    "colors = ['green', 'blue', 'red', 'purple', 'orange', 'brown', 'pink', 'maroon', 'cyan']\n",
    " # Define more colors if you have more regimes\n",
    "for i, color in enumerate(colors):\n",
    "    mask = regimes == i\n",
    "    ax.fill_between(first_diff_df_cleaned.index, first_diff_df_cleaned['yield_50Y'].min(), first_diff_df_cleaned['yield_50Y'].max(),\n",
    "                    where=mask, facecolor=color, alpha=0.9, label=f'Regime {i}',interpolate='True')\n",
    "\n",
    "ax.set_title('Time Series of Yields with Different Regimes (HMM)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Yield')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming first_diff_df_cleaned and combined_df_cleaned are defined correctly\n",
    "\n",
    "# Prepare the data for HMM\n",
    "X = first_diff_df_cleaned['yield_50Y'].values.reshape(-1, 1)\n",
    "\n",
    "# Create and fit the HMM model\n",
    "model = hmm.GaussianHMM(n_components=3, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "model.fit(X)\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Plot yield data\n",
    "# Visualize the regimes\n",
    "colors = ['green', 'blue', 'red', 'purple', 'orange', 'brown', 'pink', 'maroon', 'cyan']\n",
    " # Define more colors if you have more regimes\n",
    "for i, color in enumerate(colors):\n",
    "    mask = regimes == i\n",
    "    ax.fill_between(first_diff_df_cleaned.index, combined_df_cleaned['yield_50Y'].min(), combined_df_cleaned['yield_50Y'].max(),\n",
    "                    where=mask, facecolor=color, alpha=0.9, label=f'Regime {i}',interpolate='True')\n",
    "\n",
    "ax.set_title('Time Series of Yields with Different Regimes (HMM)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Yield')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Two states  100 ITERATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'combined_df_cleaned' is your DataFrame and it has been preprocessed\n",
    "# List of yield tenors to analyze\n",
    "tenors = mat_all  # Add or modify as per your actual columns\n",
    "\n",
    "# Dictionary to store results for each tenor\n",
    "tenor_results = {}\n",
    "\n",
    "# Process each tenor\n",
    "for tenor in tenors:\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)  # Reshape data for HMM\n",
    "    \n",
    "    # Create an instance of GaussianHMM with a chosen number of components\n",
    "    n_components = 10  # Adjust as needed\n",
    "    model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Obtain the hidden states for the data points\n",
    "    hmm_states = model.predict(X)\n",
    "    \n",
    "    # Calculate the counts of each state\n",
    "    state_counts = np.bincount(hmm_states)\n",
    "    \n",
    "    # Find the indices of the two most frequent states\n",
    "    max_two_states = np.argsort(state_counts)[-2:]  # This returns indices of the first two max elements\n",
    "    \n",
    "    # Store results\n",
    "    tenor_results[tenor] = {\n",
    "        'hmm_states': hmm_states,\n",
    "        'max_two_states': max_two_states,\n",
    "        'state_counts': state_counts\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for tenor, results in tenor_results.items():\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Top Two States:\")\n",
    "    print(results['max_two_states'])\n",
    "    print(\"State Counts:\")\n",
    "    print(results['state_counts'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list to hold the data for DataFrame construction\n",
    "data_for_df = []\n",
    "\n",
    "# Iterate through results to prepare data for DataFrame\n",
    "for tenor, results in tenor_results.items():\n",
    "    # Retrieve the states sorted by frequency\n",
    "    states_sorted_by_frequency = np.argsort(results['state_counts'])[::-1]\n",
    "    top_two_states = states_sorted_by_frequency[:2]\n",
    "\n",
    "    # Append each tenor's top two states and their counts to the list\n",
    "    data_for_df.append({\n",
    "        'Tenor': tenor,\n",
    "        '1st Most Frequent State': top_two_states[0],\n",
    "        '1st State Count': results['state_counts'][top_two_states[0]],\n",
    "        '2nd Most Frequent State': top_two_states[1],\n",
    "        '2nd State Count': results['state_counts'][top_two_states[1]]\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(data_for_df)\n",
    "\n",
    "# Optional: sort the DataFrame by Tenor or any other column if desired\n",
    "results_df = results_df.sort_values(by='Tenor')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Assuming hmm_states is defined correctly from your model.predict() call\n",
    "X = first_diff_df_cleaned['yield_50Y'].values.reshape(-1, 1)\n",
    "close_price = first_diff_df_cleaned['yield_50Y']\n",
    "hmm_states = model.predict(X)\n",
    "\n",
    "# Count occurrences of each state\n",
    "state_counts = np.bincount(hmm_states)\n",
    "# Use argsort to find indices of states sorted by their counts (most frequent last)\n",
    "sorted_states = np.argsort(state_counts)\n",
    "# Select indices of the two most frequent states\n",
    "top_two_states = sorted_states[-2:]\n",
    "print(top_two_states)\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(25, 15))\n",
    "\n",
    "# Data:\n",
    "markov_states = fig.add_subplot(111)\n",
    "\n",
    "# Plot data for each of the top two states\n",
    "for state in top_two_states:\n",
    "    state_indices = hmm_states == state\n",
    "    plt.plot_date(close_price.index[state_indices], close_price[state_indices],\n",
    "                  marker='*', ms=5, alpha=0.4, label=f'Hidden State {state}')\n",
    "\n",
    "# Aesthetic adjustments\n",
    "markov_states.set_title('Hidden Markov States (Two Most Frequent States)', fontsize=30, color='black')\n",
    "markov_states.set_xlabel(\"Date\", fontsize=20, color='black')\n",
    "markov_states.set_ylabel(\"Closing Price\", fontsize=20, color='black')\n",
    "markov_states.tick_params(axis='both', labelsize=15, colors='black')\n",
    "markov_states.legend(loc='upper left', fontsize=15)\n",
    "\n",
    "# Generate and save graph\n",
    "plt.savefig('hmm_graph_most_frequent_states.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_markov_states(index, data, states, top_states):\n",
    "    \"\"\"\n",
    "    Plots specified data and highlights the top Markov states.\n",
    "\n",
    "    Parameters:\n",
    "        index (Index): The index (typically datetime) for plotting.\n",
    "        data (array-like): The data points to plot.\n",
    "        states (array-like): The hidden states as predicted by HMM.\n",
    "        top_states (list): List of indices for the most frequent or relevant states to highlight.\n",
    "    \"\"\"\n",
    "    # Initialize figure\n",
    "    fig, ax = plt.subplots(figsize=(25, 15))\n",
    "\n",
    "    # Plot all data points with a basic style\n",
    "    ax.plot(index, data, label='Yield Data', color='grey', alpha=0.5)\n",
    "\n",
    "    # Highlight top Markov states\n",
    "    for state in top_states:\n",
    "        state_indices = states == state\n",
    "        ax.plot_date(index[state_indices], data[state_indices],\n",
    "                     marker='o', markersize=5, alpha=0.7, label=f'Hidden State {state}')\n",
    "\n",
    "    # Aesthetic adjustments\n",
    "    ax.set_title('Hidden Markov States Highlighted', fontsize=30)\n",
    "    ax.set_xlabel(\"Date\", fontsize=20)\n",
    "    ax.set_ylabel(\"Yield\", fontsize=20)\n",
    "    ax.legend(loc='upper left', fontsize=15)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have first_diff_df_cleaned, hmm_states, and top_two_states defined as before\n",
    "plot_markov_states(first_diff_df_cleaned.index, first_diff_df_cleaned['yield_50Y'], hmm_states, top_two_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and 'yield_50Y' column are defined correctly\n",
    "\n",
    "# Prepare the data for HMM\n",
    "X = first_diff_df_cleaned['yield_50Y'].values.reshape(-1, 1)\n",
    "\n",
    "# Initialize and fit the Gaussian HMM\n",
    "model = hmm.GaussianHMM(n_components=10, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict the hidden states\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Determine the frequency of each state\n",
    "state_counts = np.bincount(regimes)\n",
    "# Identify the indices of the two most frequent states\n",
    "most_frequent_states = np.argsort(state_counts)[-2:]\n",
    "\n",
    "# Outputting identified states and their frequencies for clarity\n",
    "print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "# Set up plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Plot the yield data\n",
    "ax.plot(first_diff_df_cleaned.index, first_diff_df_cleaned['yield_50Y'], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "# Define colors for the two most frequent states\n",
    "colors = ['green', 'blue']\n",
    "\n",
    "# Visualize only the top two regimes using distinct colors\n",
    "for color, state in zip(colors, most_frequent_states):\n",
    "    mask = regimes == state\n",
    "    #mask1 = np.roll(mask, 1) \n",
    "    #mask = mask1 | mask\n",
    "    ax.fill_between(first_diff_df_cleaned.index, \n",
    "                    first_diff_df_cleaned['yield_50Y'].min(), \n",
    "                    first_diff_df_cleaned['yield_50Y'].max(),\n",
    "                    where=mask, facecolor=color, alpha=0.9, label=f'Regime {state}',interpolate=True)\n",
    "    #print(sum(mask[:1000]))\n",
    "    #print(np.where(mask[:1000]))\n",
    "\n",
    "# Customize plot aesthetics\n",
    "ax.set_title('Time Series Analysis of Yields with Top Two HMM Regimes', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Yield', fontsize=12)\n",
    "ax.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "plt.xticks([])\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.arange(0, 4 * np.pi, 0.01)\n",
    "y = np.sin(x)\n",
    "ax.plot(x, y, color='black')\n",
    "\n",
    "threshold = 0.75\n",
    "ax.axhline(threshold, color='green', lw=2, alpha=0.7)\n",
    "ax.fill_between(x, 0, 1, where=y > threshold,\n",
    "                color='green', alpha=0.5, transform=ax.get_xaxis_transform())\n",
    "ax.fill_between(x, 0, 1, where=y < threshold,\n",
    "                color='red', alpha=0.5, transform=ax.get_xaxis_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and 'yield_50Y' column are defined correctly\n",
    "\n",
    "# Prepare the data for HMM\n",
    "X = first_diff_df_cleaned['yield_50Y'].values.reshape(-1, 1)\n",
    "\n",
    "# Initialize and fit the Gaussian HMM\n",
    "model = hmm.GaussianHMM(n_components=10, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict the hidden states\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Determine the frequency of each state\n",
    "state_counts = np.bincount(regimes)\n",
    "# Identify the indices of the two most frequent states\n",
    "most_frequent_states = np.argsort(state_counts)[-3:]\n",
    "\n",
    "# Outputting identified states and their frequencies for clarity\n",
    "print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "# Set up plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Plot the yield data\n",
    "ax.plot(combined_df_cleaned.index, combined_df_cleaned['yield_50Y'], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "# Define colors for the two most frequent states\n",
    "colors = ['green', 'blue','red']\n",
    "\n",
    "# Visualize only the top two regimes using distinct colors\n",
    "for color, state in zip(colors, most_frequent_states):\n",
    "    mask = regimes == state\n",
    "    #mask1 = np.roll(mask, 1) \n",
    "    #mask = mask1 | mask\n",
    "    ax.fill_between(first_diff_df_cleaned.index, \n",
    "                    combined_df_cleaned['yield_50Y'].min(), \n",
    "                    combined_df_cleaned['yield_50Y'].max(),\n",
    "                    where=mask, facecolor=color, alpha=0.9, label=f'Regime {state}',interpolate=True)\n",
    "    #print(sum(mask[:1000]))\n",
    "    #print(np.where(mask[:1000]))\n",
    "\n",
    "# Customize plot aesthetics\n",
    "ax.set_title('Time Series Analysis of Yields with Top Two HMM Regimes', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Yield', fontsize=12)\n",
    "ax.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "plt.xticks([])\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Set up plotting\n",
    "fig, axes = plt.subplots(len(tenors), 1, figsize=(15, 7 * len(tenors)), sharex=True)\n",
    "\n",
    "# Loop through each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=10, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Determine the frequency of each state\n",
    "    state_counts = np.bincount(regimes)\n",
    "    # Identify the indices of the two most frequent states\n",
    "    most_frequent_states = np.argsort(state_counts)[-2:]  # Choose the top two most frequent states\n",
    "\n",
    "    # Outputting identified states and their frequencies for clarity\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "    print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "    # Plot the yield data\n",
    "    axes[idx].plot(combined_df_cleaned.index, combined_df_cleaned[tenor], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "    # Define colors for the two most frequent states\n",
    "    colors = ['green', 'blue']\n",
    "\n",
    "    # Visualize the top two regimes using distinct colors\n",
    "    for color, state in zip(colors, most_frequent_states):\n",
    "        mask = regimes == state\n",
    "        axes[idx].fill_between(first_diff_df_cleaned.index,\n",
    "                               combined_df_cleaned[tenor].min(),\n",
    "                               combined_df_cleaned[tenor].max(),\n",
    "                               where=mask, facecolor=color, alpha=0.3, label=f'Regime {state}',interpolate= 'True')\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    axes[idx].set_title(f'Time Series Analysis of {tenor} with Top Two HMM Regimes', fontsize=16, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Yield', fontsize=12)\n",
    "    axes[idx].legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "# Set common x-axis label and adjust layout\n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Set up plotting\n",
    "fig, axes = plt.subplots(len(tenors), 1, figsize=(15, 7 * len(tenors)), sharex=True)\n",
    "\n",
    "# Loop through each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Determine the frequency of each state\n",
    "    state_counts = np.bincount(regimes)\n",
    "    # Identify the indices of the two most frequent states\n",
    "    most_frequent_states = np.argsort(state_counts)[-2:]  # Choose the top two most frequent states\n",
    "\n",
    "    # Outputting identified states and their frequencies for clarity\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "    print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "    # Plot the yield data\n",
    "    axes[idx].plot(combined_df_cleaned.index, combined_df_cleaned[tenor], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "    # Define colors for the two most frequent states\n",
    "    colors = ['green', 'blue']\n",
    "\n",
    "    # Visualize the top two regimes using distinct colors\n",
    "    for color, state in zip(colors, most_frequent_states):\n",
    "        mask = regimes == state\n",
    "        axes[idx].fill_between(first_diff_df_cleaned.index,\n",
    "                               combined_df_cleaned[tenor].min(),\n",
    "                               combined_df_cleaned[tenor].max(),\n",
    "                               where=mask, facecolor=color, alpha=0.3, label=f'Regime {state}',interpolate= 'True')\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    axes[idx].set_title(f'Time Series Analysis of {tenor} with Top Two HMM Regimes', fontsize=16, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Yield', fontsize=12)\n",
    "    axes[idx].legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "# Set common x-axis label and adjust layout\n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Three regimes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Set up plotting\n",
    "fig, axes = plt.subplots(len(tenors), 1, figsize=(15, 7 * len(tenors)), sharex=True)\n",
    "\n",
    "# Loop through each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=3, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Determine the frequency of each state\n",
    "    state_counts = np.bincount(regimes)\n",
    "    # Identify the indices of the two most frequent states\n",
    "    most_frequent_states = np.argsort(state_counts)[-3:]  # Choose the top two most frequent states\n",
    "\n",
    "    # Outputting identified states and their frequencies for clarity\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "    print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "    # Plot the yield data\n",
    "    axes[idx].plot(combined_df_cleaned.index, combined_df_cleaned[tenor], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "    # Define colors for the two most frequent states\n",
    "    colors = ['green', 'blue','red']\n",
    "\n",
    "    # Visualize the top two regimes using distinct colors\n",
    "    for color, state in zip(colors, most_frequent_states):\n",
    "        mask = regimes == state\n",
    "        axes[idx].fill_between(first_diff_df_cleaned.index,\n",
    "                               combined_df_cleaned[tenor].min(),\n",
    "                               combined_df_cleaned[tenor].max(),\n",
    "                               where=mask, facecolor=color, alpha=0.3, label=f'Regime {state}',interpolate= 'True')\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    axes[idx].set_title(f'Time Series Analysis of {tenor} with Top Two HMM Regimes', fontsize=16, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Yield', fontsize=12)\n",
    "    axes[idx].legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "# Set common x-axis label and adjust layout\n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Daily 4 regimes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily 4 regimes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Set up plotting\n",
    "fig, axes = plt.subplots(len(tenors), 1, figsize=(15, 7 * len(tenors)), sharex=True)\n",
    "\n",
    "# Loop through each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Determine the frequency of each state\n",
    "    state_counts = np.bincount(regimes)\n",
    "    # Identify the indices of the two most frequent states\n",
    "    most_frequent_states = np.argsort(state_counts)[-4:]  # Choose the top two most frequent states\n",
    "\n",
    "    # Outputting identified states and their frequencies for clarity\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "    print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "    # Plot the yield data\n",
    "    axes[idx].plot(combined_df_cleaned.index, combined_df_cleaned[tenor], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "    # Define colors for the two most frequent states\n",
    "    colors = ['green', 'blue','red','magenta']\n",
    "\n",
    "    # Visualize the top two regimes using distinct colors\n",
    "    for color, state in zip(colors, most_frequent_states):\n",
    "        mask = regimes == state\n",
    "        axes[idx].fill_between(first_diff_df_cleaned.index,\n",
    "                               combined_df_cleaned[tenor].min(),\n",
    "                               combined_df_cleaned[tenor].max(),\n",
    "                               where=mask, facecolor=color, alpha=0.3, label=f'Regime {state}',interpolate= 'True')\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    axes[idx].set_title(f'Time Series Analysis of {tenor} with Top four HMM Regimes', fontsize=16, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Yield', fontsize=12)\n",
    "    axes[idx].legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "# Set common x-axis label and adjust layout\n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame with hourly returns\n",
    "# 'time' is assumed to be a DateTime index\n",
    "\n",
    "# Step 1: Convert hourly returns to daily returns\n",
    "daily_returns = first_diff_df_cleaned.resample('D').sum()  # Resample to daily and sum up hourly returns\n",
    "\n",
    "# Step 2: Convert daily returns to weekly returns\n",
    "weekly_returns = daily_returns.resample('W').sum()  # Resample to weekly and sum up daily returns\n",
    "\n",
    "# Print or inspect the resulting DataFrames\n",
    "print(\"Daily Returns:\")\n",
    "print(daily_returns.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nWeekly Returns:\")\n",
    "print(weekly_returns.shape)\n",
    "print(weekly_returns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = weekly_returns\n",
    "\n",
    "# Set the style of seaborn plot\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Create a figure to hold all subplots\n",
    "plt.figure(figsize=(20, 20))  # Adjust the size based on your display\n",
    "\n",
    "# Loop through each column in the DataFrame\n",
    "for index, column in enumerate(df.columns):\n",
    "    plt.subplot(5, 4, index + 1)  # Adjust the grid dimensions if necessary\n",
    "    sns.histplot(df[column], kde=True, color='blue')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel('Weekly Returns')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 7))  # Set the figure size for better readability\n",
    "for column in weekly_returns.columns:\n",
    "    plt.plot(weekly_returns.index, weekly_returns[column], label=column)\n",
    "\n",
    "plt.title('Weekly return Plot for All tenors')  # Add a title\n",
    "plt.xlabel('Date')  # X-axis label\n",
    "plt.ylabel('Yield')  # Y-axis label\n",
    "#plt.legend()  # Add a legend to distinguish the lines\n",
    "plt.grid(True)  # Add a grid for easier readability\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_returns.to_csv('H:/Excel/weekly_returns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWO states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Set up plotting\n",
    "fig, axes = plt.subplots(len(tenors), 1, figsize=(15, 7 * len(tenors)), sharex=True)\n",
    "\n",
    "# Loop through each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = weekly_returns[tenor].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Determine the frequency of each state\n",
    "    state_counts = np.bincount(regimes)\n",
    "    # Identify the indices of the two most frequent states\n",
    "    most_frequent_states = np.argsort(state_counts)[-2:]  # Choose the top two most frequent states\n",
    "\n",
    "    # Outputting identified states and their frequencies for clarity\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "    print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "    # Plot the yield data\n",
    "    axes[idx].plot(combined_df_cleaned.index, combined_df_cleaned[tenor], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "    # Define colors for the two most frequent states\n",
    "    colors = ['green', 'blue','red']\n",
    "\n",
    "    # Visualize the top two regimes using distinct colors\n",
    "    for color, state in zip(colors, most_frequent_states):\n",
    "        mask = regimes == state\n",
    "        axes[idx].fill_between(weekly_returns.index,\n",
    "                               combined_df_cleaned[tenor].min(),\n",
    "                               combined_df_cleaned[tenor].max(),\n",
    "                               where=mask, facecolor=color, alpha=0.3, label=f'Regime {state}',interpolate= 'True')\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    axes[idx].set_title(f'Time Series Analysis of weekly abs returns of {tenor} with Top two  HMM Regimes', fontsize=16, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Yield', fontsize=12)\n",
    "    axes[idx].legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "# Set common x-axis label and adjust layout\n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three States "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Set up plotting\n",
    "fig, axes = plt.subplots(len(tenors), 1, figsize=(15, 7 * len(tenors)), sharex=True)\n",
    "\n",
    "# Loop through each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = weekly_returns[tenor].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=3, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Determine the frequency of each state\n",
    "    state_counts = np.bincount(regimes)\n",
    "    # Identify the indices of the two most frequent states\n",
    "    most_frequent_states = np.argsort(state_counts)[-3:]  # Choose the top two most frequent states\n",
    "\n",
    "    # Outputting identified states and their frequencies for clarity\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "    print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "    # Plot the yield data\n",
    "    axes[idx].plot(combined_df_cleaned.index, combined_df_cleaned[tenor], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "    # Define colors for the two most frequent states\n",
    "    colors = ['green', 'blue','red']\n",
    "\n",
    "    # Visualize the top two regimes using distinct colors\n",
    "    for color, state in zip(colors, most_frequent_states):\n",
    "        mask = regimes == state\n",
    "        axes[idx].fill_between(weekly_returns.index,\n",
    "                               combined_df_cleaned[tenor].min(),\n",
    "                               combined_df_cleaned[tenor].max(),\n",
    "                               where=mask, facecolor=color, alpha=0.3, label=f'Regime {state}',interpolate= 'True')\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    axes[idx].set_title(f'Time Series Analysis of weekly abs returns of {tenor} with Top three  HMM Regimes', fontsize=16, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Yield', fontsize=12)\n",
    "    axes[idx].legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "# Set common x-axis label and adjust layout\n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_returns =weekly_returns.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# Assume 'weekly_returns' is already prepared and suitable for input to HMM\n",
    "data = weekly_returns\n",
    "\n",
    "# Train the HMM model\n",
    "model = GaussianHMM(n_components=2, covariance_type=\"full\", n_iter=100).fit(data)\n",
    "\n",
    "# Predict the states (regimes)\n",
    "states = model.predict(data)\n",
    "\n",
    "# Add states to the data\n",
    "data['Regime'] = states\n",
    "print(data['Regime'])\n",
    "print(data.columns)\n",
    "\n",
    "# Define a common bin size\n",
    "common_bin_width = 0.05  # Adjust this value based on your data's range and variability\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(16, 40))\n",
    "for i, duration in enumerate(data.columns[:-1]):  # Assuming last column is 'Regime'\n",
    "    plt.subplot(10, 2, i+1)\n",
    "    # Determine min and max values for consistent binning across regimes\n",
    "    min_value = data[duration].min()\n",
    "    max_value = data[duration].max()\n",
    "    bins = np.arange(min_value, max_value + common_bin_width, common_bin_width)\n",
    "    \n",
    "    for regime in data['Regime'].unique():\n",
    "        subset = data[data['Regime'] == regime][duration]\n",
    "        sns.histplot(subset, kde=False, element='step', stat='density', bins=bins, label=f'Regime {regime}')\n",
    "    plt.title(f'SONIA {duration} Weekly Abs Returns')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "data = weekly_returns\n",
    "\n",
    "\n",
    "model = GaussianHMM(n_components=2, covariance_type=\"full\", n_iter=100).fit(data)\n",
    "\n",
    "\n",
    "states = model.predict(data)\n",
    "\n",
    "data['Regime'] = states\n",
    "print(data['Regime'])\n",
    "print(data.columns)\n",
    "\n",
    "\n",
    "common_bin_width = 0.05 \n",
    "\n",
    "plt.figure(figsize=(16, 40))\n",
    "for i, duration in enumerate(data.columns[:-1]):  \n",
    "    plt.subplot(10, 2, i+1)\n",
    "    min_value = data[duration].min()\n",
    "    max_value = data[duration].max()\n",
    "    bins = np.arange(min_value, max_value + common_bin_width, common_bin_width)\n",
    "    \n",
    "    for regime in data['Regime'].unique():\n",
    "        subset = data[data['Regime'] == regime][duration]\n",
    "        sns.histplot(subset, kde=False, element='step', stat='density', bins=bins, label=f'Regime {regime}')\n",
    "\n",
    "   \n",
    "        mu, std = subset.mean(), subset.std()\n",
    "      \n",
    "        x = np.linspace(mu - 3*std, mu + 3*std, 100)\n",
    "       \n",
    "        p = norm.pdf(x, mu, std)\n",
    "        plt.plot(x, p, color='black') \n",
    "\n",
    "    plt.title(f'SONIA {duration} Weekly Abs Returns')\n",
    "    plt.xlabel('Returns')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'weekly_returns' contains your data and 'mat_all' is a list of columns to analyze\n",
    "tenors = mat_all  # Example: ['US_2Y', 'US_5Y', 'US_10Y']\n",
    "\n",
    "# Dictionary to store results for each tenor\n",
    "tenor_results = {}\n",
    "\n",
    "# Process each tenor\n",
    "for tenor in tenors:\n",
    "    X = weekly_returns[tenor].values.reshape(-1, 1)  # Reshape data for HMM\n",
    "    \n",
    "    # Create an instance of GaussianHMM\n",
    "    n_components = 2  # Number of hidden states\n",
    "    model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Obtain the hidden states and the transition matrix\n",
    "    hmm_states = model.predict(X)\n",
    "    transmat = model.transmat_\n",
    "    \n",
    "    # Store the results\n",
    "    tenor_results[tenor] = {\n",
    "        'hmm_states': hmm_states,\n",
    "        'transmat': transmat\n",
    "    }\n",
    "\n",
    "    # Plot the transition matrix as a heat map\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(transmat, cmap='viridis')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_title(f'Transition Matrix Heat Map - Tenor: {tenor}')\n",
    "    ax.set_xlabel('To State')\n",
    "    ax.set_ylabel('From State')\n",
    "    \n",
    "    # Set tick labels\n",
    "    ax.set_xticks(np.arange(n_components))\n",
    "    ax.set_yticks(np.arange(n_components))\n",
    "    ax.set_xticklabels(np.arange(n_components))\n",
    "    ax.set_yticklabels(np.arange(n_components))\n",
    "\n",
    "    # Annotate each cell with the numeric value of the transition probabilities\n",
    "    for (i, j), val in np.ndenumerate(transmat):\n",
    "        ax.text(j, i, f\"{val:.2f}\", ha='center', va='center', color='white')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A= weekly_returns['yield_10Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Radial Basis Function kernel with default nu value\n",
    "svm_1 = OneClassSVM(kernel='rbf', nu=0.5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.values.reshape(-1, 1)\n",
    "svm_1.fit(PolynomialFeatures(degree=3).fit_transform(A))\n",
    "print(svm_1)\n",
    "hmm_states = svm_1.predict(PolynomialFeatures(degree=3).fit_transform(A))\n",
    "hmm_states_rw = pd.Series(hmm_states).rolling(window=10).mean().fillna(1)\n",
    "print(hmm_states_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter HSVM states:\n",
    "hmm_states_rw[hmm_states_rw >= 0] = 1\n",
    "hmm_states_rw[hmm_states_rw < 0] = 0\n",
    "hmm_states = hmm_states_rw\n",
    "hmm_states.index = weekly_returns.index\n",
    "hmm_state_0_cluster = hmm_states == 0\n",
    "hmm_state_1_cluster = hmm_states == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_returns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Hidden SVM states:\n",
    "default = 'white'\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "# Data:\n",
    "hidden_svm_states = fig.add_subplot(111)\n",
    "plt.plot_date(weekly_returns.index[hmm_state_0_cluster],weekly_returns['yield_10Y'][hmm_state_0_cluster], marker='*', ms=5, alpha=0.4, color='C1', label='Cluster 0')\n",
    "plt.plot_date(weekly_returns.index[hmm_state_1_cluster],weekly_returns['yield_10Y'][hmm_state_1_cluster], marker='*', ms=5, alpha=0.4, color='C2',label='Cluster 1')\n",
    "# Aesthetic adjustments:\n",
    "hidden_svm_states.set_title('SVM Clusters', fontsize=30, color=default)\n",
    "hidden_svm_states.set_xlabel(\"Date\", fontsize=20, color=default)\n",
    "hidden_svm_states.set_ylabel(\"Closing Price\", fontsize=20, color=default)\n",
    "hidden_svm_states.tick_params(axis='both', labelsize=15, colors=default)\n",
    "hidden_svm_states.legend(loc='upper left', fontsize=15)\n",
    "# Generate and save graph:\n",
    "plt.savefig('hmm_svm_graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Set up plotting\n",
    "fig, axes = plt.subplots(len(tenors), 1, figsize=(15, 7 * len(tenors)), sharex=True)\n",
    "\n",
    "# Loop through each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Determine the frequency of each state\n",
    "    state_counts = np.bincount(regimes)\n",
    "    # Identify the indices of the two most frequent states\n",
    "    most_frequent_states = np.argsort(state_counts)[-4:]  # Choose the top two most frequent states\n",
    "\n",
    "    # Outputting identified states and their frequencies for clarity\n",
    "    print(f\"Tenor: {tenor}\")\n",
    "    print(\"Unique Regimes Identified:\", np.unique(regimes))\n",
    "    print(\"Most Frequent States:\", most_frequent_states, \"with counts:\", state_counts[most_frequent_states])\n",
    "\n",
    "    # Plot the yield data\n",
    "    axes[idx].plot(combined_df_cleaned.index, combined_df_cleaned[tenor], label='Yield', color='black', linewidth=1.5)\n",
    "\n",
    "    # Define colors for the two most frequent states\n",
    "    colors = ['green', 'blue','red','magenta']\n",
    "\n",
    "    # Visualize the top two regimes using distinct colors\n",
    "    for color, state in zip(colors, most_frequent_states):\n",
    "        mask = regimes == state\n",
    "        axes[idx].fill_between(first_diff_df_cleaned.index,\n",
    "                               combined_df_cleaned[tenor].min(),\n",
    "                               combined_df_cleaned[tenor].max(),\n",
    "                               where=mask, facecolor=color, alpha=0.3, label=f'Regime {state}',interpolate= 'True')\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    axes[idx].set_title(f'Time Series Analysis of {tenor} with Top four HMM Regimes', fontsize=16, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Yield', fontsize=12)\n",
    "    axes[idx].legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "# Set common x-axis label and adjust layout\n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to calculate intraday volatility\n",
    "def calculate_intraday_volatility(df):\n",
    "    # Extract the date part of the index to group by date\n",
    "    df['date'] = df.index.date\n",
    "    # Group by date and calculate the standard deviation for each tenor\n",
    "    intraday_volatility = df.groupby('date').std()\n",
    "    return intraday_volatility\n",
    "\n",
    "# Calculate intraday volatility\n",
    "intraday_volatility = calculate_intraday_volatility(first_diff_df_cleaned)\n",
    "\n",
    "# Print intraday volatility\n",
    "print(intraday_volatility)\n",
    "\n",
    "# Plot intraday volatility for each tenor\n",
    "tenors = first_diff_df_cleaned.columns[:-1]  # Exclude the 'date' column\n",
    "fig, axes = plt.subplots(len(tenors), 1, figsize=(15, 5 * len(tenors)), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    ax = axes[idx]\n",
    "    ax.plot(intraday_volatility.index, intraday_volatility[tenor], label=f'Intraday Volatility {tenor}')\n",
    "    ax.set_title(f'Intraday Volatility for {tenor}', fontsize=14)\n",
    "    ax.set_ylabel('Volatility', fontsize=12)\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming first_diff_df_cleaned DataFrame is already defined and contains the Datetime index\n",
    "first_diff_df_cleaned['date'] = first_diff_df_cleaned.index.date\n",
    "\n",
    "# Define the specific date you want to filter by\n",
    "specific_date = pd.to_datetime('2011-09-14').date()\n",
    "\n",
    "# Filter the DataFrame by the specific date\n",
    "filtered_rows = first_diff_df_cleaned[first_diff_df_cleaned['date'] == specific_date]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(filtered_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last three years Mid week Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Every Tuesday Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# Filter the last three years of data\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all # Add more tenors as needed\n",
    "\n",
    "# Determine the number of rows and columns for the subplot grid\n",
    "num_tenors = len(tenors)\n",
    "num_cols = 2  # Number of columns in the grid\n",
    "num_rows = (num_tenors + num_cols - 1) // num_cols  # Calculate the number of rows needed\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 7 * num_rows), sharex=True)\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states for the entire dataset\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Add the predicted regimes to the original DataFrame\n",
    "    first_diff_df_cleaned['regime'] = np.nan\n",
    "    first_diff_df_cleaned.loc[first_diff_df_cleaned[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter the data for the last three years to include only Thursdays\n",
    "    thursday_data = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "    thursday_data = thursday_data[thursday_data.index.dayofweek == 1]\n",
    "    print(thursday_data.shape)\n",
    "\n",
    "    # Count the occurrences of each state for each Thursday\n",
    "    thursday_state_counts = thursday_data.groupby(thursday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Plot the state counts as a bar chart\n",
    "    ax = axes[idx]\n",
    "    thursday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax)\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    ax.set_title(f'Hidden State Counts on Tuesday for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=5)\n",
    "    ax.set_ylabel('Number of States', fontsize=10)\n",
    "    ax.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    \n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(num_tenors, num_rows * num_cols):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = mat_all  # Replace with your actual tenors\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))\n",
    "\n",
    "# Set up figures and axes for state counts and volatility separately\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Compute HMM and regimes\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter Tuesday data\n",
    "    tuesday_data = df_copy.loc[start_date:end_date]\n",
    "    tuesday_data = tuesday_data[tuesday_data.index.dayofweek == 1]\n",
    "\n",
    "    # Calculate state counts for each Tuesday\n",
    "    tuesday_state_counts = tuesday_data.groupby(tuesday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    tuesday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "    # Convert the index to datetime.date and then format\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(tuesday_state_counts.index)]\n",
    "\n",
    "    # Set the x-axis labels to the formatted dates\n",
    "    ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "    # Calculate Volatility (Line plot)\n",
    "    volatility = filtered_df[tenor].rolling(window=10).std()\n",
    "\n",
    "    # Plot Volatility (Line plot)\n",
    "    ax_volatility = axes[idx * 2 + 1]\n",
    "    ax_volatility.plot(volatility.index, volatility, color=colors[idx], linestyle='-', linewidth=2, label='Volatility')\n",
    "    ax_volatility.set_title(f'Volatility for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_volatility.set_ylabel('Volatility', fontsize=10)\n",
    "    ax_volatility.set_xlabel('Date', fontsize=10)\n",
    "    ax_volatility.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Every Wednesday Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# Filter the last three years of data\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all # Add more tenors as needed\n",
    "\n",
    "# Determine the number of rows and columns for the subplot grid\n",
    "num_tenors = len(tenors)\n",
    "num_cols = 2  # Number of columns in the grid\n",
    "num_rows = (num_tenors + num_cols - 1) // num_cols  # Calculate the number of rows needed\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 7 * num_rows), sharex=True)\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states for the entire dataset\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Add the predicted regimes to the original DataFrame\n",
    "    first_diff_df_cleaned['regime'] = np.nan\n",
    "    first_diff_df_cleaned.loc[first_diff_df_cleaned[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter the data for the last three years to include only Thursdays\n",
    "    thursday_data = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "    thursday_data = thursday_data[thursday_data.index.dayofweek == 2]\n",
    "\n",
    "    # Count the occurrences of each state for each Thursday\n",
    "    thursday_state_counts = thursday_data.groupby(thursday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    print(thursday_data.shape)\n",
    "\n",
    "    # Plot the state counts as a bar chart\n",
    "    ax = axes[idx]\n",
    "    thursday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax)\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    ax.set_title(f'Hidden State Counts on Wednesday for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=5)\n",
    "    ax.set_ylabel('Number of States', fontsize=10)\n",
    "    ax.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    \n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(num_tenors, num_rows * num_cols):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = mat_all  # Replace with your actual tenors\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))\n",
    "\n",
    "# Set up figures and axes for state counts and volatility separately\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Compute HMM and regimes\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter Tuesday data\n",
    "    tuesday_data = df_copy.loc[start_date:end_date]\n",
    "    tuesday_data = tuesday_data[tuesday_data.index.dayofweek == 2]\n",
    "\n",
    "    # Calculate state counts for each Tuesday\n",
    "    tuesday_state_counts = tuesday_data.groupby(tuesday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    tuesday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "    # Convert the index to datetime.date and then format\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(tuesday_state_counts.index)]\n",
    "\n",
    "    # Set the x-axis labels to the formatted dates\n",
    "    ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "    # Calculate Volatility (Line plot)\n",
    "    volatility = filtered_df[tenor].rolling(window=10).std()\n",
    "\n",
    "    # Plot Volatility (Line plot)\n",
    "    ax_volatility = axes[idx * 2 + 1]\n",
    "    ax_volatility.plot(volatility.index, volatility, color=colors[idx], linestyle='-', linewidth=2, label='Volatility')\n",
    "    ax_volatility.set_title(f'Volatility for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_volatility.set_ylabel('Volatility', fontsize=10)\n",
    "    ax_volatility.set_xlabel('Date', fontsize=10)\n",
    "    ax_volatility.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Every Thursday  Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# Filter the last three years of data\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all # Add more tenors as needed\n",
    "\n",
    "# Determine the number of rows and columns for the subplot grid\n",
    "num_tenors = len(tenors)\n",
    "num_cols = 2  # Number of columns in the grid\n",
    "num_rows = (num_tenors + num_cols - 1) // num_cols  # Calculate the number of rows needed\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 7 * num_rows), sharex=True)\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states for the entire dataset\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Add the predicted regimes to the original DataFrame\n",
    "    first_diff_df_cleaned['regime'] = np.nan\n",
    "    first_diff_df_cleaned.loc[first_diff_df_cleaned[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter the data for the last three years to include only Thursdays\n",
    "    thursday_data = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "    thursday_data = thursday_data[thursday_data.index.dayofweek == 3]\n",
    "    print(thursday_data.shape)\n",
    "\n",
    "    # Count the occurrences of each state for each Thursday\n",
    "    thursday_state_counts = thursday_data.groupby(thursday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Plot the state counts as a bar chart\n",
    "    ax = axes[idx]\n",
    "    thursday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax)\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    ax.set_title(f'Hidden State Counts on Thursdays for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=5)\n",
    "    ax.set_ylabel('Number of States', fontsize=10)\n",
    "    ax.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "for i in range(num_tenors, num_rows * num_cols):\n",
    "    fig.delaxes(axes[i])\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = mat_all  # Replace with your actual tenors\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))\n",
    "\n",
    "# Set up figures and axes for state counts and volatility separately\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Compute HMM and regimes\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter Tuesday data\n",
    "    tuesday_data = df_copy.loc[start_date:end_date]\n",
    "    tuesday_data = tuesday_data[tuesday_data.index.dayofweek == 4]\n",
    "\n",
    "    # Calculate state counts for each Tuesday\n",
    "    tuesday_state_counts = tuesday_data.groupby(tuesday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    tuesday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "    # Convert the index to datetime.date and then format\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(tuesday_state_counts.index)]\n",
    "\n",
    "    # Set the x-axis labels to the formatted dates\n",
    "    ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "    # Calculate Volatility (Line plot)\n",
    "    volatility = filtered_df[tenor].rolling(window=10).std()\n",
    "\n",
    "    # Plot Volatility (Line plot)\n",
    "    ax_volatility = axes[idx * 2 + 1]\n",
    "    ax_volatility.plot(volatility.index, volatility, color=colors[idx], linestyle='-', linewidth=2, label='Volatility')\n",
    "    ax_volatility.set_title(f'Volatility for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_volatility.set_ylabel('Volatility', fontsize=10)\n",
    "    ax_volatility.set_xlabel('Date', fontsize=10)\n",
    "    ax_volatility.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned.to_csv(r'H:\\Excel\\first_diff_df_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How different rolling and intraday volatility are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = mat_all  # Replace with your actual tenors\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))\n",
    "\n",
    "# Set up figures and axes for state counts, volatility, and intraday volatility separately\n",
    "fig, axes = plt.subplots(len(tenors) * 3, 1, figsize=(20, 10 * len(tenors)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Compute HMM and regimes\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter Tuesday data\n",
    "    tuesday_data = df_copy.loc[start_date:end_date]\n",
    "    tuesday_data = tuesday_data[tuesday_data.index.dayofweek == 3]\n",
    "\n",
    "    # Calculate state counts for each Tuesday\n",
    "    tuesday_state_counts = tuesday_data.groupby(tuesday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 3]\n",
    "    tuesday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "    # Convert the index to datetime.date and then format\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(tuesday_state_counts.index)]\n",
    "\n",
    "    # Set the x-axis labels to the formatted dates\n",
    "    ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "    # Calculate Volatility (Line plot)\n",
    "    volatility = filtered_df[tenor].rolling(window=10).std()\n",
    "\n",
    "    # Plot Volatility (Line plot)\n",
    "    ax_volatility = axes[idx * 3 + 1]\n",
    "    ax_volatility.plot(volatility.index, volatility, color=colors[idx], linestyle='-', linewidth=2, label='Volatility')\n",
    "    ax_volatility.set_title(f'Volatility for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_volatility.set_ylabel('Volatility', fontsize=10)\n",
    "    ax_volatility.set_xlabel('Date', fontsize=10)\n",
    "    ax_volatility.legend(loc='upper right', frameon=False)\n",
    "\n",
    "    # Calculate Intraday Volatility\n",
    "    intraday_volatility = filtered_df.groupby(filtered_df.index.date)[tenor].std()\n",
    "\n",
    "    # Plot Intraday Volatility (Line plot)\n",
    "    ax_intraday_volatility = axes[idx * 3 + 2]\n",
    "    ax_intraday_volatility.plot(intraday_volatility.index, intraday_volatility, color='orange', linestyle='-', linewidth=2, label='Intraday Volatility')\n",
    "    ax_intraday_volatility.set_title(f'Intraday Volatility for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_intraday_volatility.set_ylabel('Intraday Volatility', fontsize=10)\n",
    "    ax_intraday_volatility.set_xlabel('Date', fontsize=10)\n",
    "    ax_intraday_volatility.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantifying Uncertainty\n",
    "\n",
    "Definition: Entropy is a measure of the uncertainty or randomness in a system. For hidden states in an HMM, it represents the uncertainty in predicting which state the system will be in at a given time.\n",
    "\n",
    "\n",
    "Application: High entropy indicates a high degree of uncertainty or randomness, suggesting that the system is frequently changing states or that the state transitions are less predictable. Low entropy indicates a more predictable or stable system where the state transitions are more deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from scipy.stats import entropy\n",
    "\n",
    "    daily_state_counts = tuesday_state_counts.sum(axis=1)\n",
    "    print(daily_state_counts)\n",
    "\n",
    "\n",
    "    daily_state_probs = daily_state_counts / daily_state_counts.sum()\n",
    "    print(daily_state_probs)\n",
    "\n",
    "    entropy_value = entropy(daily_state_probs, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_value = entropy(tuesday_state_counts.T/tuesday_state_counts.sum(axis=1), base=2)\n",
    "print(entropy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy([0,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    daily_state_counts = tuesday_state_counts.sum(axis=1)\n",
    "    print(daily_state_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuesday_state_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned = first_diff_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = mat_all  # Replace with your actual tenors\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))\n",
    "\n",
    "# Set up figures and axes for state counts, volatility, and entropy separately\n",
    "fig, axes = plt.subplots(len(tenors) * 3, 1, figsize=(20, 10 * len(tenors)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Compute HMM and regimes\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter Tuesday data\n",
    "    tuesday_data = df_copy.loc[start_date:end_date]\n",
    "    tuesday_data = tuesday_data[tuesday_data.index.dayofweek == 3]\n",
    "\n",
    "    # Calculate state counts for each Tuesday\n",
    "    tuesday_state_counts = tuesday_data.groupby(tuesday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 3]\n",
    "    tuesday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "    # Convert the index to datetime.date and then format\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(tuesday_state_counts.index)]\n",
    "\n",
    "    # Set the x-axis labels to the formatted dates\n",
    "    ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "    # Calculate Volatility (Line plot)\n",
    "    volatility = filtered_df[tenor].rolling(window=10).std()\n",
    "\n",
    "    # Plot Volatility (Line plot)\n",
    "    ax_volatility = axes[idx * 3 + 1]\n",
    "    ax_volatility.plot(volatility.index, volatility, color=colors[idx], linestyle='-', linewidth=2, label='Volatility')\n",
    "    ax_volatility.set_title(f'Volatility for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_volatility.set_ylabel('Volatility', fontsize=10)\n",
    "    ax_volatility.set_xlabel('Date', fontsize=10)\n",
    "    ax_volatility.legend(loc='upper right', frameon=False)\n",
    "\n",
    "    \n",
    "    #daily_state_counts = tuesday_state_counts.sum(axis=1)\n",
    "    #daily_state_probs = daily_state_counts / daily_state_counts.sum()  \n",
    "    #entropy = -np.sum(daily_state_probs * np.log(daily_state_probs))\n",
    "    # \n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    \n",
    "    entropy_value = entropy(tuesday_state_counts.T/tuesday_state_counts.sum(axis=1), base=2)\n",
    "    \n",
    "    ax_entropy = axes[idx * 3 + 2]\n",
    "    ax_entropy.plot(tuesday_state_counts.index,  entropy_value, color='orange', linestyle='-', linewidth=2, label='Entropy')\n",
    "    ax_entropy.set_title(f'Entropy for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax_entropy.set_ylabel('Entropy', fontsize=10)\n",
    "    ax_entropy.set_xlabel('Date', fontsize=10)\n",
    "    ax_entropy.legend(loc='upper left', frameon=False)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# Filter the last three years of data\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Prepare a dictionary to store variance data and state counts for each tenor\n",
    "tenor_variance_dict = {}\n",
    "\n",
    "for tenor in tenors:\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states for the entire dataset\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Add the predicted regimes to the original DataFrame\n",
    "    first_diff_df_cleaned['regime'] = np.nan\n",
    "    first_diff_df_cleaned.loc[first_diff_df_cleaned[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter the data for the last three years to include only Thursdays\n",
    "    thursday_data = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "    thursday_data = thursday_data[thursday_data.index.dayofweek == 3]\n",
    "\n",
    "    # Count the occurrences of each state for each Thursday\n",
    "    thursday_state_counts = thursday_data.groupby(thursday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate the variance in counts for each day\n",
    "    variances = thursday_state_counts.var(axis=1)\n",
    "\n",
    "    # Combine variance and state counts\n",
    "    combined_df = thursday_state_counts.copy()\n",
    "    combined_df['variance'] = variances\n",
    "\n",
    "    # Store the combined data in the dictionary, sorted by variance\n",
    "    tenor_variance_dict[tenor] = combined_df.sort_values(by='variance', ascending=False).head(75)\n",
    "\n",
    "# Print the variance and state counts for each tenor\n",
    "for tenor, data in tenor_variance_dict.items():\n",
    "    print(f\"Top 75 Variance Days for {tenor}:\")\n",
    "    print(data.to_string())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# Filter the last three years of data\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Prepare a dictionary to store variance data and state counts for each tenor\n",
    "tenor_variance_dict = {}\n",
    "\n",
    "for tenor in tenors:\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states for the entire dataset\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Add the predicted regimes to the original DataFrame\n",
    "    first_diff_df_cleaned['regime'] = np.nan\n",
    "    first_diff_df_cleaned.loc[first_diff_df_cleaned[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter the data for the last three years to include only Thursdays\n",
    "    thursday_data = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "    thursday_data = thursday_data[thursday_data.index.dayofweek == 3]\n",
    "\n",
    "    # Count the occurrences of each state for each Thursday\n",
    "    thursday_state_counts = thursday_data.groupby(thursday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate the variance in counts for each day\n",
    "    variances = thursday_state_counts.var(axis=1)\n",
    "\n",
    "    # Combine variance and state counts\n",
    "    combined_df = thursday_state_counts.copy()\n",
    "    combined_df['variance'] = variances\n",
    "\n",
    "    # Store the combined data in the dictionary, sorted by variance\n",
    "    tenor_variance_dict[tenor] = combined_df.sort_values(by='variance', ascending=True).head(75)\n",
    "\n",
    "# Print the variance and state counts for each tenor\n",
    "for tenor, data in tenor_variance_dict.items():\n",
    "    print(f\"Top 75 Variance Days for {tenor}:\")\n",
    "    print(data.to_string())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# Filter the last three years of data\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all  # Add more tenors as needed\n",
    "\n",
    "# Prepare a dictionary to store variance data and state counts for each tenor\n",
    "tenor_variance_dict = {}\n",
    "\n",
    "for tenor in tenors:\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states for the entire dataset\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Add the predicted regimes to the original DataFrame\n",
    "    first_diff_df_cleaned['regime'] = np.nan\n",
    "    first_diff_df_cleaned.loc[first_diff_df_cleaned[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter the data for the last three years to include only Thursdays\n",
    "    thursday_data = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "    thursday_data = thursday_data[thursday_data.index.dayofweek == 3]\n",
    "\n",
    "    # Count the occurrences of each state for each Thursday\n",
    "    thursday_state_counts = thursday_data.groupby(thursday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate the variance in counts for each day\n",
    "    variances = thursday_state_counts.var(axis=1)\n",
    "\n",
    "    # Combine variance and state counts\n",
    "    combined_df = thursday_state_counts.copy()\n",
    "    combined_df['variance'] = variances\n",
    "\n",
    "    # Calculate the 50th and 75th percentiles\n",
    "    percentile_50 = variances.quantile(0.5)\n",
    "    percentile_75 = variances.quantile(0.75)\n",
    "\n",
    "    # Filter the combined DataFrame to include only rows within the 50th to 75th percentile range\n",
    "    filtered_combined_df = combined_df[(combined_df['variance'] >= percentile_50) & (combined_df['variance'] <= percentile_75)]\n",
    "\n",
    "    # Store the filtered combined data in the dictionary\n",
    "    tenor_variance_dict[tenor] = filtered_combined_df\n",
    "\n",
    "# Print the variance and state counts for each tenor within the 50th to 75th percentile range\n",
    "for tenor, data in tenor_variance_dict.items():\n",
    "    print(f\"50th to 75th Percentile Variance Days for {tenor}:\")\n",
    "    print(data.to_string())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoE MPC Announcement Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Parquet file using forward slashes\n",
    "file_path = 'C:/Users/srajan/Downloads/dfs'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df['BoE_MPC_Announcement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Parquet file\n",
    "file_path = r'C:\\Users\\srajan\\Downloads\\dfs'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the 'BoE_MPC_Announcement' column\n",
    "print(df['BoE_MPC_Announcement'])\n",
    "\n",
    "# Select the indices of non-NaT days\n",
    "non_nat_indices = df[df['BoE_MPC_Announcement'].notna()].index\n",
    "\n",
    "# Extract only the date part\n",
    "non_nat_dates = non_nat_indices.date\n",
    "\n",
    "# Create a DataFrame with the non-NaT dates as a column\n",
    "non_nat_dates_df = pd.DataFrame(non_nat_dates, columns=['Non_NaT_Dates']).reset_index(drop=True)\n",
    "\n",
    "# Display the number of non-NaT indices\n",
    "print(len(non_nat_indices))\n",
    "\n",
    "# Display the DataFrame with non-NaT dates\n",
    "print(non_nat_dates_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming first_diff_df_cleaned is already defined and has an index with date and time\n",
    "\n",
    "# Extract the date and time from the index\n",
    "first_diff_df_cleaned['Date'] = first_diff_df_cleaned.index.date\n",
    "first_diff_df_cleaned['Time'] = first_diff_df_cleaned.index.time\n",
    "\n",
    "# Reset the index to make the time a column\n",
    "first_diff_df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Reorder the columns to have Date and Time as the first two columns\n",
    "cols = ['Date', 'Time'] + [col for col in first_diff_df_cleaned.columns if col not in ['Date', 'Time']]\n",
    "first_diff_df_cleaned = first_diff_df_cleaned[cols]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(first_diff_df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nat_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Example tenors\n",
    "tenors = mat_all  # Replace with your actual tenors\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))\n",
    "\n",
    "\n",
    "filtered_df = first_diff_df_cleaned[first_diff_df_cleaned['Date'].isin(non_nat_dates_df['Non_NaT_Dates'])]\n",
    "\n",
    "print(filtered_df)\n",
    "\n",
    "# Set up figures and axes for state counts and volatility separately\n",
    "fig, axes = plt.subplots(len(tenors) * 1, 1, figsize=(20, 5 * len(tenors)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Compute HMM and regimes\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=1000)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Calculate state counts for each day\n",
    "    daily_state_counts = df_copy.groupby(df_copy['Date'])['regime'].value_counts().unstack(fill_value=0)\n",
    "    print(daily_state_counts.shape)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 1]\n",
    "    daily_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (Non-NaT Days)', fontsize=12, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "    # Convert the index to datetime.date and then format\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_state_counts.index)]\n",
    "\n",
    "    # Set the x-axis labels to the formatted dates\n",
    "    ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Set of Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned = first_diff_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "# Set up figures and axes for state counts only\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week), 1, figsize=(20, 5 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=1000, init_params='mc', random_state=2)\n",
    "    \n",
    "    model.startprob_ = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    model.transmat_ = np.array([\n",
    "        [0.7, 0.1, 0.1, 0.1],\n",
    "        [0.1, 0.7, 0.1, 0.1],\n",
    "        [0.1, 0.1, 0.7, 0.1],\n",
    "        [0.1, 0.1, 0.1, 0.7]\n",
    "    ])\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day of the week\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Calculate state counts for each specific day of the week\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) + day]\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "\n",
    "        # Set the x-axis labels to the formatted dates\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned = pd.read_csv(r'H:\\Excel\\first_diff_df_cleaned.csv', index_col=0, parse_dates=True)\n",
    "first_diff_df_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned = first_diff_df_cleaned.iloc[:, :-2]\n",
    "first_diff_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_all = ['yield_1Y',\n",
    " 'yield_2Y',\n",
    " 'yield_3Y',\n",
    " 'yield_4Y',\n",
    " 'yield_5Y',\n",
    " 'yield_6Y',\n",
    " 'yield_7Y',\n",
    " 'yield_8Y',\n",
    " 'yield_9Y',\n",
    " 'yield_10Y',\n",
    " 'yield_12Y',\n",
    " 'yield_15Y',\n",
    " 'yield_20Y',\n",
    " 'yield_25Y',\n",
    " 'yield_30Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thursday "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Assuming 'first_diff_df_cleaned' DataFrame and all relevant tenors are defined correctly\n",
    "# Assuming 'combined_df_cleaned' DataFrame is also defined correctly for plotting\n",
    "\n",
    "# Filter the last three years of data\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "\n",
    "# List of tenors you want to analyze\n",
    "tenors = mat_all # Add more tenors as needed\n",
    "\n",
    "# Determine the number of rows and columns for the subplot grid\n",
    "num_tenors = len(tenors)\n",
    "num_cols = 2  # Number of columns in the grid\n",
    "num_rows = (num_tenors + num_cols - 1) // num_cols  # Calculate the number of rows needed\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 7 * num_rows), sharex=True)\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Prepare the data for HMM\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize and fit the Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Predict the hidden states for the entire dataset\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Add the predicted regimes to the original DataFrame\n",
    "    first_diff_df_cleaned['regime'] = np.nan\n",
    "    first_diff_df_cleaned.loc[first_diff_df_cleaned[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Filter the data for the last three years to include only Thursdays\n",
    "    thursday_data = first_diff_df_cleaned.loc[start_date:end_date]\n",
    "    thursday_data = thursday_data[thursday_data.index.dayofweek == 3]\n",
    "    print(thursday_data.shape)\n",
    "\n",
    "    # Count the occurrences of each state for each Thursday\n",
    "    thursday_state_counts = thursday_data.groupby(thursday_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Plot the state counts as a bar chart\n",
    "    ax = axes[idx]\n",
    "    thursday_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax)\n",
    "\n",
    "    # Customize plot aesthetics\n",
    "    ax.set_title(f'Hidden State Counts on Thursdays for {tenor} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=5)\n",
    "    ax.set_ylabel('Number of States', fontsize=10)\n",
    "    ax.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "for i in range(num_tenors, num_rows * num_cols):\n",
    "    fig.delaxes(axes[i])\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoE_MPC_Announcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Example tenors\n",
    "tenors = mat_all  # Replace with your actual tenors\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))\n",
    "\n",
    "\n",
    "filtered_df = first_diff_df_cleaned[first_diff_df_cleaned['Date'].isin(non_nat_dates_df['Non_NaT_Dates'])]\n",
    "\n",
    "print(filtered_df)\n",
    "\n",
    "# Set up figures and axes for state counts and volatility separately\n",
    "fig, axes = plt.subplots(len(tenors) * 1, 1, figsize=(20, 5 * len(tenors)))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    # Compute HMM and regimes\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=1000)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Calculate state counts for each day\n",
    "    daily_state_counts = df_copy.groupby(df_copy['Date'])['regime'].value_counts().unstack(fill_value=0)\n",
    "    print(daily_state_counts.shape)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 1]\n",
    "    daily_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (Non-NaT Days)', fontsize=12, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "    # Convert the index to datetime.date and then format\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_state_counts.index)]\n",
    "\n",
    "    # Set the x-axis labels to the formatted dates\n",
    "    ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on transition probabilites and initial probabilites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Set Random Seed \n",
    "\n",
    "2. Initialize Model Parameters \n",
    "\n",
    "3. Parameter Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 3  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "# Set up figures and axes for state counts only\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week), 1, figsize=(20, 5 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=1000, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    np.random.seed(42)\n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.random.rand(n_hidden_states, n_hidden_states)\n",
    "    #transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    #np.fill_diagonal(transmat, 0.7) \n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day of the week\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Calculate state counts for each specific day of the week\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) + day]\n",
    "        #day_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=['green', 'blue', 'red', 'magenta'], ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "\n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "\n",
    "        # Set the x-axis labels to the formatted dates\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## printing state transitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week) * 2, 1, figsize=(20, 10 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "    \n",
    "    def count_transitions(regimes):\n",
    "        # Convert the list to a set of unique transitions\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Ensure regime column exists and calculate state transitions\n",
    "        if 'regime' not in day_data.columns:\n",
    "            continue\n",
    "\n",
    "        # Calculate state transitions\n",
    "        #day_data['prev_regime'] = day_data['regime'].shift()\n",
    "        #day_data['transition'] = (day_data['regime'] != day_data['prev_regime']).astype(int)  # Identify transitions\n",
    "        #daily_transitions = day_data.groupby(day_data.index.date)['transition'].sum()\n",
    "        daily_transitions = day_data.groupby(day_data.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "        # Calculate state counts\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) * 2 + day * 2]\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "        ax_state_counts.set_xticks(range(len(date_labels)))\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "\n",
    "        # Plot State Transitions (Bar plot)\n",
    "        ax_transitions = axes[idx * len(days_of_week) * 2 + day * 2 + 1]\n",
    "        daily_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "        ax_transitions.set_title(f'State Transitions for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_transitions.set_ylabel('Number of Transitions', fontsize=10)\n",
    "        ax_transitions.set_xlabel('Date', fontsize=10)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_transitions.index)]\n",
    "        ax_transitions.set_xticks(range(len(transition_date_labels)))\n",
    "        ax_transitions.set_xticklabels(transition_date_labels, rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(day_state_counts.head(10))\n",
    "#print(day_data['regime'].head(20))\n",
    "#print(daily_transitions.head(10))\n",
    "#day_data['transition'].head(10)\n",
    "# Calculate state transitions\n",
    "#day_data['regime'].shift()\n",
    "#day_data['transition'] = (day_data['regime'] != day_data['prev_regime']).astype(int)  # Identify transitions\n",
    "\n",
    "# Define a function to count transitions\n",
    "def count_transitions(regimes):\n",
    "    # Convert the list to a set of unique transitions\n",
    "    transitions = 0\n",
    "    for i in range(1, len(regimes)):\n",
    "        if regimes[i] != regimes[i - 1]:\n",
    "            transitions += 1\n",
    "    return transitions\n",
    "\n",
    "\n",
    "day_regime_grp = day_data.groupby(day_data.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "\n",
    "# Apply the function to the 'regimes' column\n",
    "#day_regime_grp['transitions'] = day_regime_grp['regimes']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoE announcement days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Parquet file\n",
    "file_path = r'C:\\Users\\srajan\\Downloads\\dfs'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the 'BoE_MPC_Announcement' column\n",
    "print(df['BoE_MPC_Announcement'])\n",
    "\n",
    "# Select the indices of non-NaT days\n",
    "non_nat_indices = df[df['BoE_MPC_Announcement'].notna()].index\n",
    "\n",
    "# Extract only the date part\n",
    "non_nat_dates = non_nat_indices.date\n",
    "\n",
    "# Create a DataFrame with the non-NaT dates as a column\n",
    "non_nat_dates_df = pd.DataFrame(non_nat_dates, columns=['Non_NaT_Dates']).reset_index(drop=True)\n",
    "\n",
    "# Display the number of non-NaT indices\n",
    "print(len(non_nat_indices))\n",
    "\n",
    "# Display the DataFrame with non-NaT dates\n",
    "print(non_nat_dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the 'Non_NaT_Dates' column to datetime format (if not already)\n",
    "non_nat_dates_df['Non_NaT_Dates'] = pd.to_datetime(non_nat_dates_df['Non_NaT_Dates'])\n",
    "\n",
    "# Define the cutoff date as a pd.Timestamp\n",
    "cutoff_date = pd.Timestamp('2023-08-08')\n",
    "\n",
    "# Filter the DataFrame for dates before the cutoff_date\n",
    "BoE_announcement_days = non_nat_dates_df[non_nat_dates_df['Non_NaT_Dates'] < cutoff_date]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(BoE_announcement_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data: Replace this with your actual BoE announcement dates\n",
    "# non_nat_dates_df = pd.DataFrame({'Non_NaT_Dates': ['2023-01-05', '2023-02-02', '2023-03-02', ...]})\n",
    "\n",
    "# Ensure 'Non_NaT_Dates' column is in datetime format\n",
    "non_nat_dates_df['Non_NaT_Dates'] = pd.to_datetime(non_nat_dates_df['Non_NaT_Dates'])\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = pd.Timestamp('2024-01-01')\n",
    "\n",
    "# Filter BoE announcement days before the cutoff_date\n",
    "BoE_announcement_days = non_nat_dates_df[non_nat_dates_df['Non_NaT_Dates'] < cutoff_date]['Non_NaT_Dates']\n",
    "\n",
    "# Extract the day of the week for each BoE announcement date\n",
    "boe_announcement_days_df = pd.DataFrame({\n",
    "    'Date': BoE_announcement_days,\n",
    "    'Day of Week': BoE_announcement_days.dt.day_name()\n",
    "})\n",
    "\n",
    "# Print all BoE announcement days\n",
    "with pd.option_context('display.max_rows', None):  # Display all rows\n",
    "    print(boe_announcement_days_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data: Replace this with your actual BoE announcement dates\n",
    "# non_nat_dates_df = pd.DataFrame({'Non_NaT_Dates': ['2023-01-05', '2023-02-02', '2023-03-02', ...]})\n",
    "\n",
    "# Ensure 'Non_NaT_Dates' column is in datetime format\n",
    "non_nat_dates_df['Non_NaT_Dates'] = pd.to_datetime(non_nat_dates_df['Non_NaT_Dates'])\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = pd.Timestamp('2024-01-01')\n",
    "\n",
    "# Filter BoE announcement days before the cutoff_date\n",
    "BoE_announcement_days = non_nat_dates_df[non_nat_dates_df['Non_NaT_Dates'] < cutoff_date]['Non_NaT_Dates']\n",
    "\n",
    "# Further filter BoE announcement days to include only those after 2020\n",
    "BoE_announcement_days = BoE_announcement_days[BoE_announcement_days.dt.year > 2019]\n",
    "\n",
    "# Extract the day of the week for each BoE announcement date\n",
    "boe_announcement_days_df = pd.DataFrame({\n",
    "    'Date': BoE_announcement_days,\n",
    "    'Day of Week': BoE_announcement_days.dt.day_name()\n",
    "})\n",
    "\n",
    "# Print all BoE announcement days\n",
    "with pd.option_context('display.max_rows', None):  # Display all rows\n",
    "    print(boe_announcement_days_df)\n",
    "\n",
    "# Assuming `filtered_df` is already defined and has a datetime index\n",
    "# Example initialization (replace with actual data):\n",
    "# filtered_df = pd.DataFrame(index=pd.date_range(start='2023-01-01', periods=10, freq='D'))\n",
    "\n",
    "# Extract the date from the index and add it as a new column\n",
    "filtered_df['Date'] = filtered_df.index.date\n",
    "\n",
    "# Convert BoE announcement days to a set of dates for faster lookup\n",
    "boe_dates_set = set(BoE_announcement_days.dt.date)\n",
    "\n",
    "# Create a new column to indicate if the date is a BoE announcement day\n",
    "filtered_df['Is_BoE_Announcement'] = filtered_df['Date'].apply(lambda x: x in boe_dates_set)\n",
    "\n",
    "# Create a DataFrame to show the results\n",
    "results_df = filtered_df[['Date', 'Is_BoE_Announcement']]\n",
    "\n",
    "# Count the number of matching days\n",
    "matching_days_count = filtered_df['Is_BoE_Announcement'].sum()\n",
    "\n",
    "# Print the DataFrame with the new column and the count of matching days\n",
    "print(results_df)  # To see the DataFrame with the new column\n",
    "print(f\"Number of matching days: {matching_days_count}\")\n",
    "csv_file_path = 'H:/Excel/results_df.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame has been saved to {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "# Convert the 'Non_NaT_Dates' column to datetime format (if not already)\n",
    "non_nat_dates_df['Non_NaT_Dates'] = pd.to_datetime(non_nat_dates_df['Non_NaT_Dates'])\n",
    "\n",
    "# Define the cutoff date and filter announcement days\n",
    "cutoff_date = pd.Timestamp('2024-01-01')\n",
    "BoE_announcement_days = non_nat_dates_df[non_nat_dates_df['Non_NaT_Dates'] < cutoff_date]['Non_NaT_Dates']\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week) * 2, 1, figsize=(20, 10 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "    def count_transitions(regimes):\n",
    "        # Convert the list to a set of unique transitions\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Ensure regime column exists and calculate state transitions\n",
    "        if 'regime' not in day_data.columns:\n",
    "            continue\n",
    "        \n",
    "        # Calculate state transitions\n",
    "        #day_data['prev_regime'] = day_data['regime'].shift()\n",
    "        #day_data['transition'] = (day_data['regime'] != day_data['prev_regime']).astype(int)  # Identify transitions\n",
    "        #daily_transitions = day_data.groupby(day_data.index.date)['transition'].sum()\n",
    "        daily_transitions = day_data.groupby(day_data.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "        # Calculate state counts\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) * 2 + day * 2]\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "        ax_state_counts.set_xticks(range(len(date_labels)))\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_announcement_days:\n",
    "            if announcement_date.date() in day_state_counts.index:\n",
    "                ax_state_counts.axvline(x=date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "        # Plot State Transitions (Bar plot)\n",
    "        ax_transitions = axes[idx * len(days_of_week) * 2 + day * 2 + 1]\n",
    "        daily_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "        ax_transitions.set_title(f'State Transitions for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_transitions.set_ylabel('Number of Transitions', fontsize=10)\n",
    "        ax_transitions.set_xlabel('Date', fontsize=10)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_transitions.index)]\n",
    "        ax_transitions.set_xticks(range(len(transition_date_labels)))\n",
    "        ax_transitions.set_xticklabels(transition_date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_announcement_days:\n",
    "            if announcement_date.date() in daily_transitions.index:\n",
    "                ax_transitions.axvline(x=transition_date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Days Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Parquet file\n",
    "file_path = r'C:\\Users\\srajan\\Downloads\\dfs'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Get the list of columns\n",
    "columns_list = df.columns.tolist()\n",
    "\n",
    "# Sort the list of columns\n",
    "sorted_columns_list = sorted(columns_list)\n",
    "\n",
    "# Convert the sorted list into a DataFrame\n",
    "sorted_columns_df = pd.DataFrame(sorted_columns_list, columns=['Column Names'])\n",
    "\n",
    "# Define the path to save the sorted columns list as a CSV file\n",
    "csv_file_path = r'C:\\Users\\srajan\\Downloads\\sorted_columns_list_1.csv'\n",
    "\n",
    "# Save the sorted list as a CSV file\n",
    "sorted_columns_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Sorted columns list saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Parquet file\n",
    "file_path = r'C:\\Users\\srajan\\Downloads\\dfs'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# List of columns to check for non-NaT dates\n",
    "columns_to_check = [\n",
    "    'BoE_MPC_Meeting_All', 'BoE_MPC_Meeting_Decision', 'BoE_MPC_Meeting_Deliberation',\n",
    "    'BoE_MPC_Meeting_Policy_Discussion', 'BoE_MPC_Meeting_Preannounced_Policy_Discussion',\n",
    "    'BoE_MPC_Meeting_Unannounced_Decision', 'ECB_Governing_Council_Meeting',\n",
    "    'ECB_LTRO_Announcement', 'ECB_MRO_Announcement', 'ECB_Monetary_Policy_Announcement',\n",
    "    'ECB_Monetary_Policy_Meeting', 'ECB_Monetary_Policy_Press_Conference_End',\n",
    "    'ECB_Monetary_Policy_Press_Conference_Start'\n",
    "]\n",
    "\n",
    "# Dictionary to store DataFrames for each column\n",
    "non_nat_dates_dict = {}\n",
    "\n",
    "# Iterate over each column and extract non-NaT dates\n",
    "for column in columns_to_check:\n",
    "    non_nat_indices = df[df[column].notna()].index\n",
    "    non_nat_dates = non_nat_indices.date\n",
    "    non_nat_dates_df = pd.DataFrame(non_nat_dates, columns=['Non_NaT_Dates']).reset_index(drop=True)\n",
    "    non_nat_dates_dict[column] = non_nat_dates_df\n",
    "\n",
    "    # Display the number of non-NaT indices for each column\n",
    "    print(f\"Number of Non-NaT indices for {column}: {len(non_nat_indices)}\")\n",
    "    print(non_nat_dates_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Parquet file\n",
    "file_path = r'C:\\Users\\srajan\\Downloads\\dfs'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# List of columns to check for non-NaT dates\n",
    "columns_to_check = [\n",
    "    'BoE_MPC_Meeting_All', 'BoE_MPC_Meeting_Decision', 'BoE_MPC_Meeting_Deliberation',\n",
    "    'BoE_MPC_Meeting_Policy_Discussion', 'BoE_MPC_Meeting_Preannounced_Policy_Discussion',\n",
    "    'BoE_MPC_Meeting_Unannounced_Decision', 'ECB_Governing_Council_Meeting',\n",
    "    'ECB_LTRO_Announcement', 'ECB_MRO_Announcement', 'ECB_Monetary_Policy_Announcement',\n",
    "    'ECB_Monetary_Policy_Meeting', 'ECB_Monetary_Policy_Press_Conference_End',\n",
    "    'ECB_Monetary_Policy_Press_Conference_Start'\n",
    "]\n",
    "\n",
    "# Define start and cutoff dates\n",
    "start_date = pd.Timestamp('2020-01-01')\n",
    "cutoff_date = pd.Timestamp('2024-01-01')\n",
    "\n",
    "# Dictionary to store lists of non-NaT dates for each column\n",
    "non_nat_dates_dict = {column: [] for column in columns_to_check}\n",
    "\n",
    "# Iterate over each column and extract non-NaT dates between the start and cutoff dates\n",
    "for column in columns_to_check:\n",
    "    non_nat_indices = df[df[column].notna()].index\n",
    "    non_nat_indices = non_nat_indices[(non_nat_indices >= start_date) & (non_nat_indices < cutoff_date)]\n",
    "    non_nat_dates = non_nat_indices.date\n",
    "    non_nat_dates_dict[column] = list(non_nat_dates)\n",
    "\n",
    "# Find the maximum length of lists to pad the shorter lists with None\n",
    "max_length = max(len(dates) for dates in non_nat_dates_dict.values())\n",
    "\n",
    "# Pad shorter lists with None\n",
    "for column, dates in non_nat_dates_dict.items():\n",
    "    non_nat_dates_dict[column] = dates + [None] * (max_length - len(dates))\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "non_nat_dates_df = pd.DataFrame(non_nat_dates_dict)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(non_nat_dates_df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = r'C:\\Users\\srajan\\Downloads\\non_nat_dates.csv'\n",
    "non_nat_dates_df.to_csv(csv_file_path, index=False)\n",
    "print(f\"Non-NaT dates saved to {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Parquet file\n",
    "file_path = r'C:\\Users\\srajan\\Downloads\\dfs'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# List of columns to check for non-NaT dates\n",
    "columns_to_check = [\n",
    "    'BoE_MPC_Meeting_All', 'BoE_MPC_Meeting_Decision', 'BoE_MPC_Meeting_Deliberation',\n",
    "    'BoE_MPC_Meeting_Policy_Discussion', 'BoE_MPC_Meeting_Preannounced_Policy_Discussion',\n",
    "    'BoE_MPC_Meeting_Unannounced_Decision', 'ECB_Governing_Council_Meeting',\n",
    "    'ECB_LTRO_Announcement', 'ECB_MRO_Announcement', 'ECB_Monetary_Policy_Announcement',\n",
    "    'ECB_Monetary_Policy_Meeting', 'ECB_Monetary_Policy_Press_Conference_End',\n",
    "    'ECB_Monetary_Policy_Press_Conference_Start'\n",
    "]\n",
    "\n",
    "# Define start and cutoff dates\n",
    "start_date = pd.Timestamp('2020-01-01')\n",
    "cutoff_date = pd.Timestamp('2024-01-01')\n",
    "\n",
    "# Dictionary to store DataFrames for each column\n",
    "non_nat_dates_dfs = {}\n",
    "\n",
    "# Iterate over each column and extract non-NaT dates between the start and cutoff dates\n",
    "for column in columns_to_check:\n",
    "    non_nat_indices = df[df[column].notna()].index\n",
    "    non_nat_indices = non_nat_indices[(non_nat_indices >= start_date) & (non_nat_indices < cutoff_date)]\n",
    "    non_nat_dates = non_nat_indices.date\n",
    "\n",
    "    # Create a DataFrame with the non-NaT dates and their corresponding day names\n",
    "    non_nat_dates_df = pd.DataFrame({\n",
    "        'Non_NaT_Dates': non_nat_dates,\n",
    "        'Day_Name': [date.strftime('%A') for date in non_nat_dates]\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    # Store the DataFrame in the dictionary\n",
    "    non_nat_dates_dfs[column] = non_nat_dates_df\n",
    "\n",
    "    # Display the DataFrame with non-NaT dates and their corresponding day names\n",
    "    print(f\"DataFrame for {column}:\")\n",
    "    print(non_nat_dates_df.head())\n",
    "\n",
    "    # Save each DataFrame to a CSV file\n",
    "    csv_file_path = f'C:\\\\Users\\\\srajan\\\\Downloads\\\\{column}_non_nat_dates_with_days.csv'\n",
    "    non_nat_dates_df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Non-NaT dates with days for {column} saved to {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoE_MPC_Meeting_Policy_Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read the CSV file into a DataFrame\n",
    "BoE_MPC_Meeting_Policy_Discussion = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\BoE_MPC_Meeting_Policy_Discussion_non_nat_dates_with_days.csv')\n",
    "\n",
    "# Convert the 'Non_NaT_Dates' column to datetime format\n",
    "BoE_MPC_Meeting_Policy_Discussion['Non_NaT_Dates'] = pd.to_datetime(BoE_MPC_Meeting_Policy_Discussion['Non_NaT_Dates'])\n",
    "\n",
    "# Display the DataFrame to verify the conversion\n",
    "print(BoE_MPC_Meeting_Policy_Discussion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoE_MPC_Meeting_Policy_Discussion = BoE_MPC_Meeting_Policy_Discussion['Non_NaT_Dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week) * 2, 1, figsize=(20, 10 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "    def count_transitions(regimes):\n",
    "        # Convert the list to a set of unique transitions\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Ensure regime column exists and calculate state transitions\n",
    "        if 'regime' not in day_data.columns:\n",
    "            continue\n",
    "\n",
    "        # Calculate state transitions\n",
    "        #day_data['prev_regime'] = day_data['regime'].shift()\n",
    "        #day_data['transition'] = (day_data['regime'] != day_data['prev_regime']).astype(int)  # Identify transitions\n",
    "        #daily_transitions = day_data.groupby(day_data.index.date)['transition'].sum()\n",
    "        daily_transitions = day_data.groupby(day_data.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "        # Calculate state counts\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) * 2 + day * 2]\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "        ax_state_counts.set_xticks(range(len(date_labels)))\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_MPC_Meeting_Policy_Discussion:\n",
    "            if announcement_date.date() in day_state_counts.index:\n",
    "                ax_state_counts.axvline(x=date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "        # Plot State Transitions (Bar plot)\n",
    "        ax_transitions = axes[idx * len(days_of_week) * 2 + day * 2 + 1]\n",
    "        daily_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "        ax_transitions.set_title(f'State Transitions for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_transitions.set_ylabel('Number of Transitions', fontsize=10)\n",
    "        ax_transitions.set_xlabel('Date', fontsize=10)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_transitions.index)]\n",
    "        ax_transitions.set_xticks(range(len(transition_date_labels)))\n",
    "        ax_transitions.set_xticklabels(transition_date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_MPC_Meeting_Policy_Discussion:\n",
    "            if announcement_date.date() in daily_transitions.index:\n",
    "                ax_transitions.axvline(x=transition_date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoE_MPC_Meeting_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read the CSV file into a DataFrame\n",
    "BoE_MPC_Meeting_All = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\BoE_MPC_Meeting_All_non_nat_dates_with_days.csv')\n",
    "\n",
    "# Convert the 'Non_NaT_Dates' column to datetime format\n",
    "BoE_MPC_Meeting_All['Non_NaT_Dates'] = pd.to_datetime(BoE_MPC_Meeting_All['Non_NaT_Dates'])\n",
    "\n",
    "# Display the DataFrame to verify the conversion\n",
    "print(BoE_MPC_Meeting_All)\n",
    "BoE_MPC_Meeting_All = BoE_MPC_Meeting_All['Non_NaT_Dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week) * 2, 1, figsize=(20, 10 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "    def count_transitions(regimes):\n",
    "        # Convert the list to a set of unique transitions\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Ensure regime column exists and calculate state transitions\n",
    "        if 'regime' not in day_data.columns:\n",
    "            continue\n",
    "\n",
    "        # Calculate state transitions\n",
    "        #day_data['prev_regime'] = day_data['regime'].shift()\n",
    "        #day_data['transition'] = (day_data['regime'] != day_data['prev_regime']).astype(int)  # Identify transitions\n",
    "        #daily_transitions = day_data.groupby(day_data.index.date)['transition'].sum()\n",
    "        daily_transitions = day_data.groupby(day_data.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "        # Calculate state counts\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) * 2 + day * 2]\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "        ax_state_counts.set_xticks(range(len(date_labels)))\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_MPC_Meeting_All:\n",
    "            if announcement_date.date() in day_state_counts.index:\n",
    "                ax_state_counts.axvline(x=date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "        # Plot State Transitions (Bar plot)\n",
    "        ax_transitions = axes[idx * len(days_of_week) * 2 + day * 2 + 1]\n",
    "        daily_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "        ax_transitions.set_title(f'State Transitions for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_transitions.set_ylabel('Number of Transitions', fontsize=10)\n",
    "        ax_transitions.set_xlabel('Date', fontsize=10)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_transitions.index)]\n",
    "        ax_transitions.set_xticks(range(len(transition_date_labels)))\n",
    "        ax_transitions.set_xticklabels(transition_date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_MPC_Meeting_All:\n",
    "            if announcement_date.date() in daily_transitions.index:\n",
    "                ax_transitions.axvline(x=transition_date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoE_MPC_Meeting_Deliberation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read the CSV file into a DataFrame\n",
    "BoE_MPC_Meeting_Deliberation = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\BoE_MPC_Meeting_Deliberation_non_nat_dates_with_days.csv')\n",
    "\n",
    "# Convert the 'Non_NaT_Dates' column to datetime format\n",
    "BoE_MPC_Meeting_Deliberation['Non_NaT_Dates'] = pd.to_datetime(BoE_MPC_Meeting_Deliberation['Non_NaT_Dates'])\n",
    "\n",
    "# Display the DataFrame to verify the conversion\n",
    "print(BoE_MPC_Meeting_Deliberation)\n",
    "BoE_MPC_Meeting_Deliberation = BoE_MPC_Meeting_Deliberation['Non_NaT_Dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week) * 2, 1, figsize=(20, 10 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "    def count_transitions(regimes):\n",
    "        # Convert the list to a set of unique transitions\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Ensure regime column exists and calculate state transitions\n",
    "        if 'regime' not in day_data.columns:\n",
    "            continue\n",
    "\n",
    "        # Calculate state transitions\n",
    "        #day_data['prev_regime'] = day_data['regime'].shift()\n",
    "        #day_data['transition'] = (day_data['regime'] != day_data['prev_regime']).astype(int)  # Identify transitions\n",
    "        #daily_transitions = day_data.groupby(day_data.index.date)['transition'].sum()\n",
    "        daily_transitions = day_data.groupby(day_data.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "        # Calculate state counts\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) * 2 + day * 2]\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "        ax_state_counts.set_xticks(range(len(date_labels)))\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "            if announcement_date.date() in day_state_counts.index:\n",
    "                ax_state_counts.axvline(x=date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "        # Plot State Transitions (Bar plot)\n",
    "        ax_transitions = axes[idx * len(days_of_week) * 2 + day * 2 + 1]\n",
    "        daily_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "        ax_transitions.set_title(f'State Transitions for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_transitions.set_ylabel('Number of Transitions', fontsize=10)\n",
    "        ax_transitions.set_xlabel('Date', fontsize=10)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_transitions.index)]\n",
    "        ax_transitions.set_xticks(range(len(transition_date_labels)))\n",
    "        ax_transitions.set_xticklabels(transition_date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "            if announcement_date.date() in daily_transitions.index:\n",
    "                ax_transitions.axvline(x=transition_date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOE Announcement vs.  BoE_MPC_Meeting_Deliberation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week) * 2, 1, figsize=(20, 10 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "    def count_transitions(regimes):\n",
    "        # Convert the list to a set of unique transitions\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Ensure regime column exists and calculate state transitions\n",
    "        if 'regime' not in day_data.columns:\n",
    "            continue\n",
    "\n",
    "        # Calculate state transitions\n",
    "        #day_data['prev_regime'] = day_data['regime'].shift()\n",
    "        #day_data['transition'] = (day_data['regime'] != day_data['prev_regime']).astype(int)  # Identify transitions\n",
    "        #daily_transitions = day_data.groupby(day_data.index.date)['transition'].sum()\n",
    "        daily_transitions = day_data.groupby(day_data.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "        # Calculate state counts\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) * 2 + day * 2]\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "        ax_state_counts.set_xticks(range(len(date_labels)))\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "            if announcement_date.date() in day_state_counts.index:\n",
    "                ax_state_counts.axvline(x=date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "        \n",
    "        for announcement_date in BoE_announcement_days:\n",
    "            if announcement_date.date() in day_state_counts.index:\n",
    "                ax_state_counts.axvline(x=date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='blue', linestyle='--')\n",
    "\n",
    "        # Plot State Transitions (Bar plot)\n",
    "        ax_transitions = axes[idx * len(days_of_week) * 2 + day * 2 + 1]\n",
    "        daily_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "        ax_transitions.set_title(f'State Transitions for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_transitions.set_ylabel('Number of Transitions', fontsize=10)\n",
    "        ax_transitions.set_xlabel('Date', fontsize=10)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_transitions.index)]\n",
    "        ax_transitions.set_xticks(range(len(transition_date_labels)))\n",
    "        ax_transitions.set_xticklabels(transition_date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "            if announcement_date.date() in daily_transitions.index:\n",
    "                ax_transitions.axvline(x=transition_date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "        \n",
    "          # Mark BoE announcement days\n",
    "        for announcement_date in BoE_announcement_days:\n",
    "            if announcement_date.date() in daily_transitions.index:\n",
    "                ax_transitions.axvline(x=transition_date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='blue', linestyle='--')\n",
    "                \n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECB_Governing_Council_Meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read the CSV file into a DataFrame\n",
    "ECB_Governing_Council_Meeting = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\ECB_Governing_Council_Meeting_non_nat_dates_with_days.csv')\n",
    "\n",
    "# Convert the 'Non_NaT_Dates' column to datetime format\n",
    "ECB_Governing_Council_Meeting['Non_NaT_Dates'] = pd.to_datetime(ECB_Governing_Council_Meeting['Non_NaT_Dates'])\n",
    "\n",
    "# Display the DataFrame to verify the conversion\n",
    "print(ECB_Governing_Council_Meeting)\n",
    "ECB_Governing_Council_Meeting = ECB_Governing_Council_Meeting['Non_NaT_Dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Calculate start_date and end_date\n",
    "end_date = first_diff_df_cleaned.index.max()\n",
    "start_date = end_date - pd.DateOffset(years=3)\n",
    "\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Days of the week mapping\n",
    "days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday'}\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * len(days_of_week) * 2, 1, figsize=(20, 10 * len(tenors) * len(days_of_week)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = first_diff_df_cleaned[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = first_diff_df_cleaned.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "    def count_transitions(regimes):\n",
    "        # Convert the list to a set of unique transitions\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    for day, day_name in days_of_week.items():\n",
    "        # Filter data for the specific day\n",
    "        day_data = df_copy.loc[start_date:end_date]\n",
    "        day_data = day_data[day_data.index.dayofweek == day]\n",
    "\n",
    "        # Ensure regime column exists and calculate state transitions\n",
    "        if 'regime' not in day_data.columns:\n",
    "            continue\n",
    "\n",
    "        # Calculate state transitions\n",
    "        #day_data['prev_regime'] = day_data['regime'].shift()\n",
    "        #day_data['transition'] = (day_data['regime'] != day_data['prev_regime']).astype(int)  # Identify transitions\n",
    "        #daily_transitions = day_data.groupby(day_data.index.date)['transition'].sum()\n",
    "        daily_transitions = day_data.groupby(day_data.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "        # Calculate state counts\n",
    "        day_state_counts = day_data.groupby(day_data.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot State Counts (Bar plot)\n",
    "        ax_state_counts = axes[idx * len(days_of_week) * 2 + day * 2]\n",
    "        day_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "        ax_state_counts.set_title(f'State Counts for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_state_counts.set_ylabel('Number of States', fontsize=10)\n",
    "        ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(day_state_counts.index)]\n",
    "        ax_state_counts.set_xticks(range(len(date_labels)))\n",
    "        ax_state_counts.set_xticklabels(date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in ECB_Governing_Council_Meeting:\n",
    "            if announcement_date.date() in day_state_counts.index:\n",
    "                ax_state_counts.axvline(x=date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "        # Plot State Transitions (Bar plot)\n",
    "        ax_transitions = axes[idx * len(days_of_week) * 2 + day * 2 + 1]\n",
    "        daily_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "        ax_transitions.set_title(f'State Transitions for {tenor} on {day_name} (Last Three Years)', fontsize=12, fontweight='bold')\n",
    "        ax_transitions.set_ylabel('Number of Transitions', fontsize=10)\n",
    "        ax_transitions.set_xlabel('Date', fontsize=10)\n",
    "        \n",
    "        # Convert the index to datetime.date and then format\n",
    "        transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(daily_transitions.index)]\n",
    "        ax_transitions.set_xticks(range(len(transition_date_labels)))\n",
    "        ax_transitions.set_xticklabels(transition_date_labels, rotation=45)\n",
    "        \n",
    "        # Mark BoE announcement days\n",
    "        for announcement_date in ECB_Governing_Council_Meeting:\n",
    "            if announcement_date.date() in daily_transitions.index:\n",
    "                ax_transitions.axvline(x=transition_date_labels.index(announcement_date.date().strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is there a lead-lag impact of announcement days on state transitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoE_announcement_days Vs. BoE_MPC_Meeting_Deliberation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Mark BoE announcement days\n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='blue', linestyle='--')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Mark BoE announcement days\n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='blue', linestyle='--')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Mark BoE announcement days\n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='blue', linestyle='--')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Mark BoE announcement days\n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='blue', linestyle='--')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL Days_analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read the CSV file into a DataFrame\n",
    "BoE_MPC_Meeting_All = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\BoE_MPC_Meeting_All_non_nat_dates_with_days.csv')\n",
    "BoE_MPC_Meeting_Decision = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\BoE_MPC_Meeting_Decision_non_nat_dates_with_days.csv')\n",
    "BoE_MPC_Meeting_Deliberation = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\BoE_MPC_Meeting_Deliberation_non_nat_dates_with_days.csv')\n",
    "BoE_MPC_Meeting_Policy_Discussion = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\BoE_MPC_Meeting_Policy_Discussion_non_nat_dates_with_days.csv')\n",
    "\n",
    "# Convert the 'Non_NaT_Dates' column to datetime format\n",
    "BoE_MPC_Meeting_All['Non_NaT_Dates'] = pd.to_datetime(BoE_MPC_Meeting_All['Non_NaT_Dates'])\n",
    "BoE_MPC_Meeting_Decision['Non_NaT_Dates'] =pd.to_datetime(BoE_MPC_Meeting_Decision['Non_NaT_Dates'])\n",
    "BoE_MPC_Meeting_Deliberation['Non_NaT_Dates'] =pd.to_datetime(BoE_MPC_Meeting_Deliberation['Non_NaT_Dates'])\n",
    "BoE_MPC_Meeting_Policy_Discussion['Non_NaT_Dates'] = pd.to_datetime (BoE_MPC_Meeting_Policy_Discussion['Non_NaT_Dates'])\n",
    "\n",
    "\n",
    "BoE_MPC_Meeting_All = BoE_MPC_Meeting_All['Non_NaT_Dates']\n",
    "BoE_MPC_Meeting_Decision = BoE_MPC_Meeting_Decision['Non_NaT_Dates']\n",
    "BoE_MPC_Meeting_Deliberation = BoE_MPC_Meeting_Deliberation['Non_NaT_Dates']\n",
    "BoE_MPC_Meeting_Policy_Discussion = BoE_MPC_Meeting_Policy_Discussion['Non_NaT_Dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time transition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "    # Assuming all_days_transitions is a pandas Series\n",
    "    non_zero_transitions = all_days_transitions[all_days_transitions > 0]\n",
    "\n",
    "    # Print the shape of the filtered Series\n",
    "    print(non_zero_transitions.shape)\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from hmmlearn import hmm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# DataFrame to store days and times with state transitions > 0\n",
    "transitions_df = pd.DataFrame(columns=['DateTime', 'Tenor', 'Transitions'])\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state transitions for all days\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "    significant_transitions = all_days_transitions[all_days_transitions > 0]\n",
    "    print(significant_transitions)\n",
    "\n",
    "    # Find the indices in df_copy where transitions happen\n",
    "    transition_indices = df_copy.index[df_copy['regime'].diff().fillna(0) != 0]\n",
    "    \n",
    "    # Store the results in the DataFrame\n",
    "    new_transitions = pd.DataFrame({'DateTime': transition_indices, 'Tenor': tenor, 'Transitions': 1})\n",
    "    transitions_df = pd.concat([transitions_df, new_transitions], ignore_index=True)\n",
    "\n",
    "print(transitions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the 'DateTime' column to datetime if it's not already\n",
    "transitions_df['DateTime'] = pd.to_datetime(transitions_df['DateTime'])\n",
    "\n",
    "# Extract the date and time components\n",
    "transitions_df['Date'] = transitions_df['DateTime'].dt.date\n",
    "transitions_df['Time'] = transitions_df['DateTime'].dt.time\n",
    "\n",
    "# Convert the time to hours for plotting purposes\n",
    "transitions_df['Hour'] = transitions_df['DateTime'].dt.hour + transitions_df['DateTime'].dt.minute / 60.0\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(transitions_df['Date'], transitions_df['Hour'], c='red', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time (Hours)')\n",
    "plt.title('State Transitions by Date and Time')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a concentration of transitions around certain hours (e.g., 8:00 AM, 9:00 AM, 12:00 PM, 1:00 PM, and 3:00 PM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming transitions_df and BoE_announcement_days are already defined\n",
    "\n",
    "# Convert the 'DateTime' column to datetime if it's not already\n",
    "transitions_df['DateTime'] = pd.to_datetime(transitions_df['DateTime'])\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "filtered_announcement_days = [day for day in BoE_announcement_days if start_date <= day <= end_date]\n",
    "\n",
    "# Extract the date and time components\n",
    "transitions_df['Date'] = transitions_df['DateTime'].dt.date\n",
    "transitions_df['Time'] = transitions_df['DateTime'].dt.time\n",
    "\n",
    "# Convert the time to hours for plotting purposes\n",
    "transitions_df['Hour'] = transitions_df['DateTime'].dt.hour + transitions_df['DateTime'].dt.minute / 60.0\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(transitions_df['Date'], transitions_df['Hour'], c='red', marker='x', label='State Transitions')\n",
    "\n",
    "# Mark BoE announcement days\n",
    "for announcement_date in filtered_announcement_days:\n",
    "    plt.axvline(x=announcement_date.date(), color='blue', linestyle='--', label='BoE Announcement' if announcement_date == filtered_announcement_days[0] else \"\")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time (Hours)')\n",
    "plt.title('State Transitions by Date and Time')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from hmmlearn import hmm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# DataFrame to store days and times with state transitions > 0\n",
    "transitions_df = pd.DataFrame(columns=['DateTime', 'Tenor', 'Transitions', 'State'])\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state transitions for all days\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "    significant_transitions = all_days_transitions[all_days_transitions > 0]\n",
    "\n",
    "    # Find the indices in df_copy where transitions happen and the state labels\n",
    "    transition_indices = df_copy.index[df_copy['regime'].diff().fillna(0) != 0]\n",
    "    transition_states = df_copy['regime'][df_copy['regime'].diff().fillna(0) != 0]\n",
    "    \n",
    "    # Store the results in the DataFrame\n",
    "    new_transitions = pd.DataFrame({'DateTime': transition_indices, 'Tenor': tenor, 'Transitions': 1, 'State': transition_states.values})\n",
    "    transitions_df = pd.concat([transitions_df, new_transitions], ignore_index=True)\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(transitions_df['DateTime'], transitions_df['DateTime'].dt.hour + transitions_df['DateTime'].dt.minute / 60.0, c=transitions_df['State'], cmap='viridis', marker='x', label='State Transitions')\n",
    "\n",
    "# Mark BoE announcement days\n",
    "for announcement_date in filtered_announcement_days:\n",
    "    plt.axvline(x=announcement_date, color='blue', linestyle='--', label='BoE Announcement' if announcement_date == filtered_announcement_days[0] else \"\")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time (Hours)')\n",
    "plt.title('State Transitions by Date and Time')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.colorbar(label='State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from hmmlearn import hmm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# DataFrame to store days and times with state transitions > 0\n",
    "transitions_df = pd.DataFrame(columns=['DateTime', 'Tenor', 'Transitions', 'State'])\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state transitions for all days\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "    significant_transitions = all_days_transitions[all_days_transitions > 0]\n",
    "\n",
    "    # Find the indices in df_copy where transitions happen and the state labels\n",
    "    transition_indices = df_copy.index[df_copy['regime'].diff().fillna(0) != 0]\n",
    "    transition_states = df_copy['regime'][df_copy['regime'].diff().fillna(0) != 0]\n",
    "    \n",
    "    # Store the results in the DataFrame\n",
    "    new_transitions = pd.DataFrame({'DateTime': transition_indices, 'Tenor': tenor, 'Transitions': 1, 'State': transition_states.values})\n",
    "    transitions_df = pd.concat([transitions_df, new_transitions], ignore_index=True)\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(transitions_df['DateTime'], transitions_df['DateTime'].dt.hour + transitions_df['DateTime'].dt.minute / 60.0, c=transitions_df['State'], cmap='viridis', marker='x', label='State Transitions')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time (Hours)')\n",
    "plt.title('State Transitions by Date and Time')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.colorbar(label='State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from hmmlearn import hmm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# DataFrame to store days and times with state transitions > 0\n",
    "transitions_df = pd.DataFrame(columns=['DateTime', 'Tenor', 'Transitions', 'State'])\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state transitions for all days\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "    significant_transitions = all_days_transitions[all_days_transitions > 0]\n",
    "\n",
    "    # Find the indices in df_copy where transitions happen and the state labels\n",
    "    transition_indices = df_copy.index[df_copy['regime'].diff().fillna(0) != 0]\n",
    "    transition_states = df_copy['regime'][df_copy['regime'].diff().fillna(0) != 0]\n",
    "    \n",
    "    # Filter to include only transitions to states 1 and 3\n",
    "    filtered_transitions = transition_states[(transition_states == 1) | (transition_states == 3)]\n",
    "    filtered_indices = transition_indices[(transition_states == 1) | (transition_states == 3)]\n",
    "    \n",
    "    # Store the results in the DataFrame\n",
    "    new_transitions = pd.DataFrame({'DateTime': filtered_indices, 'Tenor': tenor, 'Transitions': 1, 'State': filtered_transitions.values})\n",
    "    transitions_df = pd.concat([transitions_df, new_transitions], ignore_index=True)\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(transitions_df['DateTime'], transitions_df['DateTime'].dt.hour + transitions_df['DateTime'].dt.minute / 60.0, c=transitions_df['State'], cmap='viridis', marker='x', label='State Transitions')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Time (Hours)')\n",
    "plt.title('State Transitions by Date and Time')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.colorbar(label='State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read the CSV file into a DataFrame\n",
    "ECB_Governing_Council_Meeting = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\ECB_Governing_Council_Meeting_non_nat_dates_with_days.csv')\n",
    "ECB_LTRO_Announcement= pd.read_csv(r'C:\\Users\\srajan\\Downloads\\ECB_LTRO_Announcement_non_nat_dates_with_days.csv')\n",
    "ECB_Monetary_Policy_Announcement = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\ECB_Monetary_Policy_Announcement_non_nat_dates_with_days.csv')\n",
    "ECB_Monetary_Policy_Meeting = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\ECB_Monetary_Policy_Meeting_non_nat_dates_with_days.csv')\n",
    "ECB_MRO_Announcement = pd.read_csv(r'C:\\Users\\srajan\\Downloads\\ECB_MRO_Announcement_non_nat_dates_with_days.csv')\n",
    "\n",
    "# Convert the 'Non_NaT_Dates' column to datetime format\n",
    "ECB_Governing_Council_Meeting['Non_NaT_Dates'] = pd.to_datetime(ECB_Governing_Council_Meeting['Non_NaT_Dates'])\n",
    "ECB_LTRO_Announcement['Non_NaT_Dates'] =pd.to_datetime(ECB_LTRO_Announcement['Non_NaT_Dates'])\n",
    "ECB_Monetary_Policy_Announcement['Non_NaT_Dates'] =pd.to_datetime(ECB_Monetary_Policy_Announcement['Non_NaT_Dates'])\n",
    "ECB_Monetary_Policy_Meeting['Non_NaT_Dates'] = pd.to_datetime (ECB_Monetary_Policy_Meeting['Non_NaT_Dates'])\n",
    "ECB_MRO_Announcement['Non_NaT_Dates'] = pd.to_datetime (ECB_MRO_Announcement['Non_NaT_Dates'])\n",
    "\n",
    "\n",
    "ECB_Governing_Council_Meeting = ECB_Governing_Council_Meeting['Non_NaT_Dates']\n",
    "ECB_LTRO_Announcement = ECB_LTRO_Announcement['Non_NaT_Dates']\n",
    "ECB_Monetary_Policy_Announcement = ECB_Monetary_Policy_Announcement['Non_NaT_Dates']\n",
    "ECB_Monetary_Policy_Meeting = ECB_Monetary_Policy_Meeting['Non_NaT_Dates']\n",
    "ECB_MRO_Announcement=ECB_MRO_Announcement['Non_NaT_Dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create a dictionary to store handles and labels for the legend\n",
    "line_handles = []\n",
    "line_labels = []\n",
    "\n",
    "# Track which labels have been added\n",
    "added_labels = set()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Mark significant dates with colors and labels\n",
    "    for announcement_date in ECB_Governing_Council_Meeting:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            line = ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "            if 'ECB_Governing_Council_Meeting' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Governing_Council_Meeting')\n",
    "                added_labels.add('ECB_Governing_Council_Meeting')\n",
    "\n",
    "    for meeting_date in ECB_LTRO_Announcement:\n",
    "        if meeting_date.date() in all_days_state_counts.index:\n",
    "            line = ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(meeting_date.date()), color='blue', linestyle='--')\n",
    "            if 'ECB_LTRO_Announcement' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_LTRO_Announcement')\n",
    "                added_labels.add('ECB_LTRO_Announcement')\n",
    "\n",
    "    for date in ECB_Monetary_Policy_Announcement:\n",
    "        if date.date() in all_days_state_counts.index:\n",
    "            line = ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(date.date()), color='green', linestyle='--')\n",
    "            if 'ECB_Monetary_Policy_Announcement' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Monetary_Policy_Announcement')\n",
    "                added_labels.add('ECB_Monetary_Policy_Announcement')\n",
    "\n",
    "    for date in ECB_Monetary_Policy_Meeting:\n",
    "        if date.date() in all_days_state_counts.index:\n",
    "            line = ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(date.date()), color='purple', linestyle='--')\n",
    "            if 'ECB_Monetary_Policy_Meeting' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Monetary_Policy_Meeting')\n",
    "                added_labels.add('ECB_Monetary_Policy_Meeting')\n",
    "\n",
    "    for date in ECB_MRO_Announcement:\n",
    "        if date.date() in all_days_state_counts.index:\n",
    "            line = ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(date.date()), color='crimson', linestyle='--')\n",
    "            if 'ECB_MRO_Announcement' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_MRO_Announcement')\n",
    "                added_labels.add('ECB_MRO_Announcement')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Mark significant dates with colors and labels\n",
    "    for announcement_date in ECB_Governing_Council_Meeting:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            line = ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "            if 'ECB_Governing_Council_Meeting' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Governing_Council_Meeting')\n",
    "                added_labels.add('ECB_Governing_Council_Meeting')\n",
    "\n",
    "    for meeting_date in ECB_LTRO_Announcement:\n",
    "        if meeting_date.date() in all_days_transitions.index:\n",
    "            line = ax_transitions.axvline(x=all_days_transitions.index.tolist().index(meeting_date.date()), color='blue', linestyle='--')\n",
    "            if 'ECB_LTRO_Announcement' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_LTRO_Announcement')\n",
    "                added_labels.add('ECB_LTRO_Announcement')\n",
    "\n",
    "    for date in ECB_Monetary_Policy_Announcement:\n",
    "        if date.date() in all_days_transitions.index:\n",
    "            line = ax_transitions.axvline(x=all_days_transitions.index.tolist().index(date.date()), color='green', linestyle='--')\n",
    "            if 'ECB_Monetary_Policy_Announcement' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Monetary_Policy_Announcement')\n",
    "                added_labels.add('ECB_Monetary_Policy_Announcement')\n",
    "\n",
    "    for date in ECB_Monetary_Policy_Meeting:\n",
    "        if date.date() in all_days_transitions.index:\n",
    "            line = ax_transitions.axvline(x=all_days_transitions.index.tolist().index(date.date()), color='purple', linestyle='--')\n",
    "            if 'ECB_Monetary_Policy_Meeting' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Monetary_Policy_Meeting')\n",
    "                added_labels.add('ECB_Monetary_Policy_Meeting')\n",
    "\n",
    "    for date in ECB_MRO_Announcement:\n",
    "        if date.date() in all_days_transitions.index:\n",
    "            line = ax_transitions.axvline(x=all_days_transitions.index.tolist().index(date.date()), color='crimson', linestyle='--')\n",
    "            if 'ECB_MRO_Announcement' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_MRO_Announcement')\n",
    "                added_labels.add('ECB_MRO_Announcement')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Add legend for vertical lines in State Transitions plot\n",
    "    ax_transitions.legend(handles=line_handles, labels=line_labels, loc='upper right', frameon=False)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create a dictionary to store handles and labels for the legend\n",
    "line_handles = []\n",
    "line_labels = []\n",
    "\n",
    "# Track which labels have been added\n",
    "added_labels = set()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Mark significant dates with colors and labels\n",
    "    for announcement_date in ECB_Governing_Council_Meeting:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            line = ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "            if 'ECB_Governing_Council_Meeting' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Governing_Council_Meeting')\n",
    "                added_labels.add('ECB_Governing_Council_Meeting')\n",
    "\n",
    "    for date in ECB_Monetary_Policy_Announcement:\n",
    "        if date.date() in all_days_state_counts.index:\n",
    "            line = ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(date.date()), color='green', linestyle='--')\n",
    "            if 'ECB_Monetary_Policy_Announcement' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Monetary_Policy_Announcement')\n",
    "                added_labels.add('ECB_Monetary_Policy_Announcement')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Mark significant dates with colors and labels\n",
    "    for announcement_date in ECB_Governing_Council_Meeting:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            line = ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "            if 'ECB_Governing_Council_Meeting' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Governing_Council_Meeting')\n",
    "                added_labels.add('ECB_Governing_Council_Meeting')\n",
    "\n",
    "\n",
    "    for date in ECB_Monetary_Policy_Announcement:\n",
    "        if date.date() in all_days_transitions.index:\n",
    "            line = ax_transitions.axvline(x=all_days_transitions.index.tolist().index(date.date()), color='green', linestyle='--')\n",
    "            if 'ECB_Monetary_Policy_Announcement' not in added_labels:\n",
    "                line_handles.append(line)\n",
    "                line_labels.append('ECB_Monetary_Policy_Announcement')\n",
    "                added_labels.add('ECB_Monetary_Policy_Announcement')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Add legend for vertical lines in State Transitions plot\n",
    "    ax_transitions.legend(handles=line_handles, labels=line_labels, loc='upper right', frameon=False)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECB_Governing_Council_Meeting,ECB_Monetary_Policy_Announcement,ECB_LTRO_Announcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regime Properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assume first_diff_df_cleaned is properly defined in your environment\n",
    "# start_date and end_date defined\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors and number of hidden states\n",
    "tenors = ['yield_10Y']\n",
    "n_hidden_states = 4\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Initialize the HMM model\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat /= transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model and predict the regimes\n",
    "for tenor in tenors:\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "    filtered_df['regime'] = regimes  # Add regimes to the dataframe\n",
    "\n",
    "    # Calculate and print the mean and variance of the regimes\n",
    "    mean_regime = np.mean(regimes)\n",
    "    variance_regime = np.var(regimes)\n",
    "    print(f\"Mean of regimes for {tenor}: {mean_regime}\")\n",
    "    print(f\"Variance of regimes for {tenor}: {variance_regime}\")\n",
    "\n",
    "    # Optional: Detailed stats per regime\n",
    "    regime_stats = filtered_df.groupby('regime')[tenor].agg(['mean', 'var'])\n",
    "    print(f\"Stats per regime for {tenor}:\\n{regime_stats}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df_copy' is your DataFrame after assigning regimes\n",
    "# Ensure the index is in datetime format\n",
    "df_copy.index = pd.to_datetime(df_copy.index)\n",
    "\n",
    "# Filter the DataFrame to find dates where the green regime (e.g., regime 2) was present\n",
    "green_regime_dates = df_copy[df_copy['regime'] == 2]\n",
    "\n",
    "# Get unique dates to avoid duplicates\n",
    "unique_dates = green_regime_dates.index.unique()\n",
    "\n",
    "# Extract the year and month from these dates\n",
    "unique_dates_month_year = unique_dates.to_period('M').unique()  # This converts the datetime index to PeriodIndex with monthly frequency\n",
    "\n",
    "# Print the unique month and year combinations\n",
    "print(unique_dates_month_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "from scipy.stats import entropy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Set up figures and axes for state counts, transitions, and entropy\n",
    "fig, axes = plt.subplots(len(tenors) * 3, 1, figsize=(20, 15 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy_values = entropy((all_days_state_counts.T / all_days_state_counts.sum(axis=1)).values, base=2)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 3]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 3 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot Entropy (Line plot)\n",
    "    ax_entropy = axes[idx * 3 + 2]\n",
    "    ax_entropy.plot(pd.to_datetime(all_days_state_counts.index), entropy_values, color='orange', linestyle='-', linewidth=2, label='Entropy')\n",
    "    ax_entropy.set_title(f'Entropy for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_entropy.set_ylabel('Entropy', fontsize=12)\n",
    "    ax_entropy.set_xlabel('Date', fontsize=12)\n",
    "    ax_entropy.legend(loc='upper left', frameon=False)\n",
    "    ax_entropy.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marked with BoE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "from scipy.stats import entropy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# BoE announcement days (ensure BoE_announcement_days is a list of datetime objects)\n",
    "BoE_announcement_days = pd.to_datetime(BoE_announcement_days)\n",
    "\n",
    "# Set up figures and axes for state counts, transitions, and entropy\n",
    "fig, axes = plt.subplots(len(tenors) * 3, 1, figsize=(20, 15 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy_values = entropy((all_days_state_counts.T / all_days_state_counts.sum(axis=1)).values, base=2)\n",
    "    entropy_dates = pd.to_datetime(all_days_state_counts.index)\n",
    "    \n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 3]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Mark BoE announcement days\n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=date_labels.index(announcement_date.strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 3 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Mark BoE announcement days\n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=transition_date_labels.index(announcement_date.strftime('%Y-%m-%d')), color='red', linestyle='--')\n",
    "\n",
    "    # Plot Entropy (Line plot)\n",
    "    ax_entropy = axes[idx * 3 + 2]\n",
    "    ax_entropy.plot(entropy_dates, entropy_values, color='orange', linestyle='-', linewidth=2, label='Entropy')\n",
    "    ax_entropy.set_title(f'Entropy for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_entropy.set_ylabel('Entropy', fontsize=12)\n",
    "    ax_entropy.set_xlabel('Date', fontsize=12)\n",
    "    ax_entropy.legend(loc='upper left', frameon=False)\n",
    "    ax_entropy.grid(True)\n",
    "\n",
    "    # Mark BoE announcement days on entropy plot\n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date in entropy_dates:\n",
    "            ax_entropy.axvline(x=announcement_date, color='red', linestyle='--')\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "from scipy.stats import entropy, ttest_ind, wilcoxon\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' and 'BoE_announcement_days' are defined somewhere in your environment\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Initialize lists to store results for statistical tests\n",
    "announcement_days_counts = []\n",
    "non_announcement_days_counts = []\n",
    "\n",
    "announcement_days_transitions = []\n",
    "non_announcement_days_transitions = []\n",
    "\n",
    "# Perform analysis for each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "# Ensure BoE_announcement_days is a Series of dates\n",
    "BoE_announcement_days = pd.to_datetime(BoE_announcement_days)\n",
    "\n",
    "# Ensure the index of all_days_state_counts and all_days_transitions is in datetime format\n",
    "all_days_state_counts.index = pd.to_datetime(all_days_state_counts.index)\n",
    "all_days_transitions.index = pd.to_datetime(all_days_transitions.index)\n",
    "\n",
    "# Separate announcement and non-announcement days\n",
    "for date in all_days_state_counts.index:\n",
    "    if date in BoE_announcement_days.values:\n",
    "        announcement_days_counts.append(all_days_state_counts.loc[date].sum().sum())\n",
    "        announcement_days_transitions.append(all_days_transitions.loc[date])\n",
    "    else:\n",
    "        non_announcement_days_counts.append(all_days_state_counts.loc[date].sum().sum())\n",
    "        non_announcement_days_transitions.append(all_days_transitions.loc[date])\n",
    "\n",
    "# Convert lists to numpy arrays for statistical tests\n",
    "announcement_days_counts = np.array(announcement_days_counts)\n",
    "non_announcement_days_counts = np.array(non_announcement_days_counts)\n",
    "announcement_days_transitions = np.array(announcement_days_transitions)\n",
    "non_announcement_days_transitions = np.array(non_announcement_days_transitions)\n",
    "\n",
    "# Perform T-tests or Wilcoxon tests\n",
    "t_test_counts = ttest_ind(announcement_days_counts, non_announcement_days_counts, equal_var=False)\n",
    "t_test_transitions = ttest_ind(announcement_days_transitions, non_announcement_days_transitions, equal_var=False)\n",
    "#wilcoxon_test_counts = wilcoxon(announcement_days_counts, non_announcement_days_counts)\n",
    "#wilcoxon_test_transitions = wilcoxon(announcement_days_transitions, non_announcement_days_transitions)\n",
    "\n",
    "# Output the test results\n",
    "print(\"T-test for State Counts: \", t_test_counts)\n",
    "print(\"T-test for State Transitions: \", t_test_transitions)\n",
    "#print(\"Wilcoxon Test for State Counts: \", wilcoxon_test_counts)\n",
    "#print(\"Wilcoxon Test for State Transitions: \", wilcoxon_test_transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "from scipy.stats import entropy, ttest_ind, wilcoxon\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'first_diff_df_cleaned' and 'BoE_announcement_days' are defined somewhere in your environment\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Initialize lists to store results for statistical tests\n",
    "announcement_days_counts = []\n",
    "non_announcement_days_counts = []\n",
    "\n",
    "announcement_days_transitions = []\n",
    "non_announcement_days_transitions = []\n",
    "\n",
    "# Perform analysis for each tenor\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "# Ensure BoE_announcement_days is a Series of dates\n",
    "BoE_announcement_days = pd.to_datetime(BoE_announcement_days)\n",
    "\n",
    "# Ensure the index of all_days_state_counts and all_days_transitions is in datetime format\n",
    "all_days_state_counts.index = pd.to_datetime(all_days_state_counts.index)\n",
    "all_days_transitions.index = pd.to_datetime(all_days_transitions.index)\n",
    "\n",
    "# Separate announcement and non-announcement days\n",
    "for date in all_days_state_counts.index:\n",
    "    if date in BoE_announcement_days.values:\n",
    "        announcement_days_counts.append(all_days_state_counts.loc[date].sum().sum())\n",
    "        announcement_days_transitions.append(all_days_transitions.loc[date])\n",
    "    else:\n",
    "        non_announcement_days_counts.append(all_days_state_counts.loc[date].sum().sum())\n",
    "        non_announcement_days_transitions.append(all_days_transitions.loc[date])\n",
    "\n",
    "# Convert lists to numpy arrays for statistical tests\n",
    "announcement_days_counts = np.array(announcement_days_counts)\n",
    "non_announcement_days_counts = np.array(non_announcement_days_counts)\n",
    "announcement_days_transitions = np.array(announcement_days_transitions)\n",
    "non_announcement_days_transitions = np.array(non_announcement_days_transitions)\n",
    "\n",
    "# Perform T-tests or Wilcoxon tests\n",
    "t_test_counts = ttest_ind(announcement_days_counts, non_announcement_days_counts, equal_var=False)\n",
    "t_test_transitions = ttest_ind(announcement_days_transitions, non_announcement_days_transitions, equal_var=False)\n",
    "\n",
    "# Output the test results\n",
    "print(\"T-test for State Counts: \", t_test_counts)\n",
    "print(\"T-test for State Transitions: \", t_test_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic tenor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "file_path = 'H:/Excel/combined_df_1.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "combined_df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Compute the synthetic tenor\n",
    "combined_df['synthetic_tenor'] = combined_df['yield_30Y'] + combined_df['yield_2Y'] - 2 * combined_df['yield_5Y']\n",
    "\n",
    "# Calculate the first difference of the synthetic tenor\n",
    "combined_df['synthetic_tenor_diff'] = combined_df['synthetic_tenor'].diff()\n",
    "\n",
    "# Create a new dataframe with the first differences of the synthetic tenor\n",
    "synthetic_tenor_diff_df = combined_df[['synthetic_tenor_diff']]\n",
    "\n",
    "# Drop any rows with NaN values\n",
    "synthetic_tenor_diff_df = synthetic_tenor_diff_df.dropna()\n",
    "\n",
    "# Display the first few rows of the new dataframe\n",
    "print(synthetic_tenor_diff_df.head())\n",
    "\n",
    "# Save the new dataframe to a CSV file\n",
    "synthetic_tenor_diff_df.to_csv('synthetic_tenor_diff.csv')\n",
    "\n",
    "# Get the column names of the new dataframe\n",
    "column_names = synthetic_tenor_diff_df.columns.tolist()\n",
    "print(\"Column names:\", column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "file_path = 'H:/Excel/combined_df_1.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "combined_df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Convert the index to datetime format\n",
    "combined_df.index = pd.to_datetime(combined_df.index)\n",
    "\n",
    "# Compute the synthetic tenor\n",
    "combined_df['synthetic_tenor'] = combined_df['yield_30Y'] + combined_df['yield_2Y'] - 2 * combined_df['yield_5Y']\n",
    "\n",
    "# Calculate the first difference of the synthetic tenor\n",
    "combined_df['synthetic_tenor_diff'] = combined_df['synthetic_tenor'].diff()\n",
    "\n",
    "# Drop any rows with NaN values\n",
    "first_diff_df_cleaned = combined_df[['synthetic_tenor_diff']].dropna()\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['synthetic_tenor_diff']\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "    \n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Mark BoE announcement days\n",
    "    for announcement_date in BoE_announcement_days:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "file_path = 'H:/Excel/combined_df_1.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "combined_df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Convert the index to datetime format\n",
    "combined_df.index = pd.to_datetime(combined_df.index)\n",
    "\n",
    "# Compute the synthetic tenor\n",
    "combined_df['synthetic_tenor'] = combined_df['yield_30Y'] + combined_df['yield_2Y'] - 2 * combined_df['yield_5Y']\n",
    "\n",
    "# Calculate the first difference of the synthetic tenor\n",
    "combined_df['synthetic_tenor_diff'] = combined_df['synthetic_tenor'].diff()\n",
    "\n",
    "# Drop any rows with NaN values\n",
    "first_diff_df_cleaned = combined_df[['synthetic_tenor_diff']].dropna()\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['synthetic_tenor_diff']\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "    \n",
    "    for announcement_date in BoE_MPC_Meeting_All:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='green', linestyle='--')\n",
    "    \n",
    "    for announcement_date in BoE_MPC_Meeting_Decision:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='blue', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Policy_Discussion:\n",
    "        if announcement_date.date() in all_days_state_counts.index:\n",
    "            ax_state_counts.axvline(x=all_days_state_counts.index.tolist().index(announcement_date.date()), color='purple', linestyle='--')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Mark BoE announcement days\n",
    "    for announcement_date in BoE_MPC_Meeting_All:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='green', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Decision:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='red', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Deliberation:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='blue', linestyle='--')\n",
    "\n",
    "    for announcement_date in BoE_MPC_Meeting_Policy_Discussion:\n",
    "        if announcement_date.date() in all_days_transitions.index:\n",
    "            ax_transitions.axvline(x=all_days_transitions.index.tolist().index(announcement_date.date()), color='purple', linestyle='--')\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Example tenors and number of hidden states\n",
    "tenors = ['synthetic_tenor_diff']\n",
    "n_hidden_states = 4\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Initialize the HMM model\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat /= transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model and predict the regimes\n",
    "for tenor in tenors:\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "    filtered_df['regime'] = regimes  # Add regimes to the dataframe\n",
    "\n",
    "    # Calculate and print the mean and variance of the regimes\n",
    "    mean_regime = np.mean(regimes)\n",
    "    variance_regime = np.var(regimes)\n",
    "    print(f\"Mean of regimes for {tenor}: {mean_regime}\")\n",
    "    print(f\"Variance of regimes for {tenor}: {variance_regime}\")\n",
    "\n",
    "    # Optional: Detailed stats per regime\n",
    "    regime_stats = filtered_df.groupby('regime')[tenor].agg(['mean', 'var'])\n",
    "    print(f\"Stats per regime for {tenor}:\\n{regime_stats}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df_copy' is your DataFrame after assigning regimes\n",
    "# Ensure the index is in datetime format\n",
    "df_copy.index = pd.to_datetime(df_copy.index)\n",
    "\n",
    "# Filter the DataFrame to find dates where the green regime (e.g., regime 2) was present\n",
    "green_regime_dates = df_copy[df_copy['regime'] == 0]\n",
    "\n",
    "# Get unique dates to avoid duplicates\n",
    "unique_dates = green_regime_dates.index.unique()\n",
    "\n",
    "# Extract the year and month from these dates\n",
    "unique_dates_month_year = unique_dates.to_period('M').unique()  # This converts the datetime index to PeriodIndex with monthly frequency\n",
    "\n",
    "# Print the unique month and year combinations\n",
    "print(unique_dates_month_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df_copy' is your DataFrame after assigning regimes\n",
    "# Ensure the index is in datetime format\n",
    "df_copy.index = pd.to_datetime(df_copy.index)\n",
    "\n",
    "# Filter the DataFrame to find dates where the green regime (e.g., regime 2) was present\n",
    "green_regime_dates = df_copy[df_copy['regime'] == 1]\n",
    "\n",
    "# Get unique dates to avoid duplicates\n",
    "unique_dates = green_regime_dates.index.unique()\n",
    "\n",
    "# Extract the year and month from these dates\n",
    "unique_dates_month_year = unique_dates.to_period('M').unique()  # This converts the datetime index to PeriodIndex with monthly frequency\n",
    "\n",
    "# Print the unique month and year combinations\n",
    "print(unique_dates_month_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Load your data\n",
    "# Assuming `synthetic_tenor_diff_df` is a pandas DataFrame containing your data\n",
    "# Normalize your data\n",
    "data = (first_diff_df_cleaned['yield_10Y'] - first_diff_df_cleaned['yield_10Y'].min()) / (first_diff_df_cleaned['yield_10Y'].max() - first_diff_df_cleaned['yield_10Y'].min())\n",
    "data = data.values.astype(np.float32).reshape(-1, 1)\n",
    "# Create DataLoader\n",
    "tensor_data = torch.tensor(data)\n",
    "dataset = TensorDataset(tensor_data)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = data.shape[1]\n",
    "hidden_dim = 256\n",
    "latent_dim = 20\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "# Initialize model, optimizer\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)  # Learning rate scheduler\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        data = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(dataloader.dataset)}')\n",
    "\n",
    "# Generating new data points\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, latent_dim)\n",
    "    sample = model.decode(z).cpu()\n",
    "\n",
    "# Visualizing some of the generated data points\n",
    "sample = sample.numpy()\n",
    "for i in range(5):\n",
    "    plt.plot(sample[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Load your data\n",
    "# Assuming `first_diff_df_cleaned` is a pandas DataFrame containing your data\n",
    "# Normalize your data\n",
    "data = (first_diff_df_cleaned['yield_10Y'] - first_diff_df_cleaned['yield_10Y'].min()) / (first_diff_df_cleaned['yield_10Y'].max() - first_diff_df_cleaned['yield_10Y'].min())\n",
    "data = data.values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "tensor_data = torch.tensor(data)\n",
    "dataset = TensorDataset(tensor_data)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = data.shape[1]\n",
    "hidden_dim = 256\n",
    "latent_dim = 20\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "# Initialize model, optimizer, scheduler\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)  # Learning rate scheduler\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        data = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(dataloader.dataset)}')\n",
    "\n",
    "# Extract and store the hidden states for each time period\n",
    "model.eval()\n",
    "all_mu = []\n",
    "all_logvar = []\n",
    "with torch.no_grad():\n",
    "    for data in tensor_data:\n",
    "        data = data.unsqueeze(0)  # Add batch dimension\n",
    "        mu, logvar = model.encode(data)\n",
    "        all_mu.append(mu)\n",
    "        all_logvar.append(logvar)\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "all_mu = torch.cat(all_mu).cpu().numpy()\n",
    "all_logvar = torch.cat(all_logvar).cpu().numpy()\n",
    "\n",
    "# Create a DataFrame to store the hidden states for each time period\n",
    "hidden_states_df = pd.DataFrame(all_mu, index=first_diff_df_cleaned.index, columns=[f'latent_dim_{i+1}' for i in range(latent_dim)])\n",
    "\n",
    "print(\"\\nHidden States DataFrame:\")\n",
    "print(hidden_states_df.head())\n",
    "\n",
    "# Save the hidden states to a CSV file\n",
    "hidden_states_df.to_csv('hidden_states.csv')\n",
    "\n",
    "# Plotting the first two latent dimensions\n",
    "if latent_dim > 2:\n",
    "    all_mu_reduced = PCA(n_components=2).fit_transform(all_mu)\n",
    "else:\n",
    "    all_mu_reduced = all_mu\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_mu_reduced[:, 0], all_mu_reduced[:, 1], alpha=0.5)\n",
    "plt.title('Latent Space Visualization')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Load your data\n",
    "# Assuming `first_diff_df_cleaned` is a pandas DataFrame containing your data\n",
    "# Normalize your data\n",
    "data = (first_diff_df_cleaned['yield_10Y'] - first_diff_df_cleaned['yield_10Y'].min()) / (first_diff_df_cleaned['yield_10Y'].max() - first_diff_df_cleaned['yield_10Y'].min())\n",
    "data = data.values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "tensor_data = torch.tensor(data)\n",
    "dataset = TensorDataset(tensor_data)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = data.shape[1]\n",
    "hidden_dim = 256\n",
    "latent_dim = 20\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "# Initialize model, optimizer, scheduler\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)  # Learning rate scheduler\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        data = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(dataloader.dataset)}')\n",
    "\n",
    "# Extract and store the hidden states for each time period\n",
    "model.eval()\n",
    "all_mu = []\n",
    "all_logvar = []\n",
    "with torch.no_grad():\n",
    "    for data in tensor_data:\n",
    "        data = data.unsqueeze(0)  # Add batch dimension\n",
    "        mu, logvar = model.encode(data)\n",
    "        all_mu.append(mu)\n",
    "        all_logvar.append(logvar)\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "all_mu = torch.cat(all_mu).cpu().numpy()\n",
    "all_logvar = torch.cat(all_logvar).cpu().numpy()\n",
    "\n",
    "# Create a DataFrame to store the hidden states for each time period\n",
    "hidden_states_df = pd.DataFrame(all_mu, index=first_diff_df_cleaned.index, columns=[f'latent_dim_{i+1}' for i in range(latent_dim)])\n",
    "\n",
    "print(\"\\nHidden States DataFrame:\")\n",
    "print(hidden_states_df.head())\n",
    "\n",
    "# Apply PCA to reduce the latent dimensions to 4\n",
    "pca = PCA(n_components=4)\n",
    "final_four_reduced = pca.fit_transform(hidden_states_df)\n",
    "\n",
    "# Create a DataFrame to store the reduced dimensions\n",
    "final_four_reduced_df = pd.DataFrame(final_four_reduced, index=hidden_states_df.index, columns=['PCA1', 'PCA2', 'PCA3', 'PCA4'])\n",
    "\n",
    "# Apply k-means clustering to categorize the latent states\n",
    "num_states = 4  # Number of clusters/states\n",
    "kmeans = KMeans(n_clusters=num_states, random_state=0).fit(final_four_reduced_df)\n",
    "states = kmeans.labels_\n",
    "\n",
    "# Add the state labels to the DataFrame\n",
    "final_four_reduced_df['state'] = states\n",
    "\n",
    "# Compute the state counts for each day\n",
    "state_counts_daily = final_four_reduced_df.groupby(final_four_reduced_df.index.date)['state'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Filter the state counts by the specified date range\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "state_counts_daily.index =pd.to_datetime(state_counts_daily.index )\n",
    "state_counts_daily_filtered = state_counts_daily[(state_counts_daily.index >= start_date) & (state_counts_daily.index <= end_date)]\n",
    "\n",
    "# Print the filtered state counts for each day\n",
    "print(\"\\nFiltered State Counts Daily:\")\n",
    "print(state_counts_daily_filtered)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "state_counts_daily_filtered.plot(kind='bar', stacked=True, ax=ax)\n",
    "plt.title('State Counts Per Day (Filtered)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='State')\n",
    "\n",
    "# Reduce number of x-axis labels\n",
    "num_labels = 10\n",
    "date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(state_counts_daily_filtered.index)]\n",
    "ax.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "ax.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Load your data\n",
    "# Assuming `first_diff_df_cleaned` is a pandas DataFrame containing your data\n",
    "# Normalize your data\n",
    "data = (first_diff_df_cleaned['yield_10Y'] - first_diff_df_cleaned['yield_10Y'].min()) / (first_diff_df_cleaned['yield_10Y'].max() - first_diff_df_cleaned['yield_10Y'].min())\n",
    "data = data.values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "tensor_data = torch.tensor(data)\n",
    "dataset = TensorDataset(tensor_data)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = data.shape[1]\n",
    "hidden_dim = 256\n",
    "latent_dim = 20\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "# Initialize model, optimizer, scheduler\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)  # Learning rate scheduler\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        data = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(dataloader.dataset)}')\n",
    "\n",
    "# Extract and store the hidden states for each time period\n",
    "model.eval()\n",
    "all_mu = []\n",
    "all_logvar = []\n",
    "with torch.no_grad():\n",
    "    for data in tensor_data:\n",
    "        data = data.unsqueeze(0)  # Add batch dimension\n",
    "        mu, logvar = model.encode(data)\n",
    "        all_mu.append(mu)\n",
    "        all_logvar.append(logvar)\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "all_mu = torch.cat(all_mu).cpu().numpy()\n",
    "all_logvar = torch.cat(all_logvar).cpu().numpy()\n",
    "\n",
    "# Create a DataFrame to store the hidden states for each time period\n",
    "hidden_states_df = pd.DataFrame(all_mu, index=first_diff_df_cleaned.index, columns=[f'latent_dim_{i+1}' for i in range(latent_dim)])\n",
    "\n",
    "print(\"\\nHidden States DataFrame:\")\n",
    "print(hidden_states_df.head())\n",
    "\n",
    "# Apply PCA to reduce the latent dimensions to 4\n",
    "pca = PCA(n_components=4)\n",
    "final_four_reduced = pca.fit_transform(hidden_states_df)\n",
    "\n",
    "# Create a DataFrame to store the reduced dimensions\n",
    "final_four_reduced_df = pd.DataFrame(final_four_reduced, index=hidden_states_df.index, columns=['PCA1', 'PCA2', 'PCA3', 'PCA4'])\n",
    "\n",
    "# Apply k-means clustering to categorize the latent states\n",
    "num_states = 4  # Number of clusters/states\n",
    "kmeans = KMeans(n_clusters=num_states, random_state=0).fit(final_four_reduced_df)\n",
    "states = kmeans.labels_\n",
    "\n",
    "# Add the state labels to the DataFrame\n",
    "final_four_reduced_df['state'] = states\n",
    "\n",
    "# Map the states back to the original data\n",
    "first_diff_df_cleaned['state'] = states\n",
    "\n",
    "\n",
    "# Compute the state counts for each day\n",
    "state_counts_daily = final_four_reduced_df.groupby(final_four_reduced_df.index.date)['state'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Filter the state counts by the specified date range\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "state_counts_daily.index = pd.to_datetime(state_counts_daily.index)\n",
    "state_counts_daily_filtered = state_counts_daily[(state_counts_daily.index >= start_date) & (state_counts_daily.index <= end_date)]\n",
    "\n",
    "# Print the filtered state counts for each day\n",
    "print(\"\\nFiltered State Counts Daily:\")\n",
    "print(state_counts_daily_filtered)\n",
    "\n",
    "# Define function to count transitions\n",
    "def count_transitions(regimes):\n",
    "    transitions = 0\n",
    "    for i in range(1, len(regimes)):\n",
    "        if regimes[i] != regimes[i - 1]:\n",
    "            transitions += 1\n",
    "    return transitions\n",
    "\n",
    "# Compute transitions per day\n",
    "df_copy = final_four_reduced_df.copy()\n",
    "df_copy['regime'] = states\n",
    "all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "# Plot the filtered number of transitions per day\n",
    "\n",
    "# Plot the number of transitions per day\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "all_days_transitions.index = pd.to_datetime(all_days_transitions.index)\n",
    "all_days_transitions[all_days_transitions.index > '2022-02-02'].plot(kind='bar', ax=ax, color='black')\n",
    "plt.title('Number of Transitions Per Day After 2022-02-02')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Transitions')\n",
    "\n",
    "# Reduce number of x-axis labels\n",
    "num_labels = 10\n",
    "date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "ax.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "ax.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plotting the filtered state counts for each day as a stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "state_counts_daily_filtered.plot(kind='bar', stacked=True, ax=ax)\n",
    "plt.title('State Counts Per Day (Filtered)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='State')\n",
    "\n",
    "# Reduce number of x-axis labels\n",
    "num_labels = 10\n",
    "date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(state_counts_daily_filtered.index)]\n",
    "ax.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "ax.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days_transitions.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Chapter Addition : **Yield Returns Around Announcement Days**\n",
    "\n",
    "### Methodologies Yet to be Incorporated\n",
    "\n",
    "1. **Restricted HMM with PCA / PPCA**\n",
    "2. **IOHMM**\n",
    "3. **Autoregressive HMM**/ **RNN / MAMBA**\n",
    "5. **VAE (Finetuning is Remaining)**\n",
    "6. **Residual Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IO HMM - S&P , Vix  for different sample periods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned = pd.read_csv(r'H:\\Excel\\first_diff_df_cleaned.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'first_diff_df_cleaned' is your DataFrame\n",
    "mat_all = first_diff_df_cleaned.columns.tolist()\n",
    "\n",
    "# Remove the last two columns from the list\n",
    "mat_all = mat_all[:-2]\n",
    "\n",
    "# Filter the DataFrame to retain only the columns in the updated 'mat_all'\n",
    "first_diff_df_cleaned = first_diff_df_cleaned[mat_all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-06')\n",
    "end_date = pd.to_datetime('2023-07-11')\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    " 'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    " 'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    " 'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Number of hidden states for the HMM\n",
    "n_hidden_states = 4  # Adjustable depending on desired complexity of the model\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df[tenors].dropna().values\n",
    "\n",
    "# Initialize and fit the HMM with a specified random state for reproducibility\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "\n",
    "# Set uniform initial probabilities\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "\n",
    "# Initialize transition matrix with more likelihood of staying in the same state\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "\n",
    "# Normalize the transition matrix to ensure each row sums to 1\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X)\n",
    "\n",
    "# Predict regimes based on the fitted model\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Add regime information back to the DataFrame (initialize column to handle NaN entries)\n",
    "filtered_df['regime'] = np.nan\n",
    "filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df[filtered_df['regime'] == i].copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of each DataFrame for each regime\n",
    "shapes = {key: df.shape for key, df in regime_dfs.items() if not df.empty}\n",
    "print(shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_dfs['regime_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-06')\n",
    "end_date = pd.to_datetime('2023-07-11')\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    " 'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    " 'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    " 'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Number of hidden states for the HMM\n",
    "n_hidden_states = 4  # Adjustable depending on desired complexity of the model\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df[tenors].dropna().values\n",
    "\n",
    "# Initialize and fit the HMM with a specified random state for reproducibility\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "\n",
    "# Set uniform initial probabilities and transition probabilities as discussed\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Predict regimes based on the fitted model\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Add regime information back to the DataFrame\n",
    "filtered_df['regime'] = np.nan\n",
    "filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df[filtered_df['regime'] == i].copy()\n",
    "\n",
    "# Dictionary to store average variances for comparison\n",
    "average_variances = {}\n",
    "\n",
    "# Calculate and print mean and variance for each tenor within each regime, and average mean and variance for the regime\n",
    "for regime, df in regime_dfs.items():\n",
    "    print(f\"Stats for {regime}:\")\n",
    "    stats = df[tenors].agg(['mean', 'var'])\n",
    "    print(stats)\n",
    "\n",
    "    # Calculate average mean and variance across all tenors for the regime\n",
    "    avg_mean = stats.loc['mean'].mean()\n",
    "    avg_var = stats.loc['var'].mean()\n",
    "    average_variances[regime] = avg_var\n",
    "    \n",
    "    print(f\"Average Mean for {regime}: {avg_mean}\")\n",
    "    print(f\"Average Variance for {regime}: {avg_var}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Identify and print the regime with the highest and lowest average variance\n",
    "highest_var_regime = max(average_variances, key=average_variances.get)\n",
    "lowest_var_regime = min(average_variances, key=average_variances.get)\n",
    "\n",
    "print(f\"Regime with the highest average variance: {highest_var_regime} (Variance: {average_variances[highest_var_regime]})\")\n",
    "print(f\"Regime with the lowest average variance: {lowest_var_regime} (Variance: {average_variances[lowest_var_regime]})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import principalcomponents as pc  # Make sure this is your PCA module\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-06')\n",
    "end_date = pd.to_datetime('2023-07-11')\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    " 'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    " 'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    " 'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Number of hidden states for the HMM\n",
    "n_hidden_states = 4\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df[tenors].dropna().values\n",
    "\n",
    "# Initialize and fit the HMM with a specified random state for reproducibility\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict regimes based on the fitted model\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Add regime information back to the DataFrame\n",
    "filtered_df['regime'] = np.nan\n",
    "filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df[filtered_df['regime'] == i].copy()\n",
    "\n",
    "# Perform PCA on each regime and plot the results\n",
    "for regime, df in regime_dfs.items():\n",
    "    if not df.empty:\n",
    "        pc_model = pc.PCA(spot=df[tenors], maturities=tenors, k=3)  # Your PCA function needs spot and maturities parameters\n",
    "        pc_vect = pc_model.eig_vect_k\n",
    "        \n",
    "        # Debug: Print the structure of pc_vect to understand its format\n",
    "        print(f\"Structure of pc_vect for regime {regime}: {type(pc_vect)}\")\n",
    "        print(f\"Contents of pc_vect for regime {regime}: {pc_vect}\")\n",
    "\n",
    "        # Ensure pc_vect is a list or an ndarray and has the expected length\n",
    "        if isinstance(pc_vect, (list, np.ndarray)) and len(pc_vect) >= 3:\n",
    "            # Plotting eigenvectors\n",
    "            fig, ax = plt.subplots(figsize=(15, 8))\n",
    "            colors = cm.rainbow(np.linspace(0, 1, 3))\n",
    "            for idx, color in enumerate(colors):\n",
    "                ax.plot(pc_vect[idx], c=color, label=f\"Component {idx + 1}\", linewidth=2)\n",
    "            \n",
    "            ax.set_title(f\"Eigenvector Loadings for Regime {regime}\")\n",
    "            ax.set_xlabel(\"Maturities\")\n",
    "            ax.legend(title=\"Components\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Invalid or insufficient data in pc_vect for regime {regime}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'regime_dfs' contains all regimes as DataFrames\n",
    "# and 'mat_all' represents the maturities associated with each column of the DataFrames\n",
    "\n",
    "# Function to generate colors\n",
    "def rainbow(categories):\n",
    "    c_scale = cm.rainbow(np.linspace(0, 1, len(categories)))\n",
    "    c_dict = {}\n",
    "    for i, c in zip(categories, c_scale):\n",
    "        c_dict[i] = c\n",
    "    return c_dict\n",
    "\n",
    "# Prepare subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25, 11))  # Adjust the grid size based on the number of regimes\n",
    "axes = axes.flatten()  # Flatten to iterate easily if you have more than two axes\n",
    "\n",
    "# Perform PCA for each regime\n",
    "for idx, (regime, df) in enumerate(regime_dfs.items()):\n",
    "    # Drop the last column\n",
    "    df = df.iloc[:, :-1]\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # Adjust components as necessary\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    \n",
    "    # Get eigenvectors for the top components\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plotting each component's loading per maturity\n",
    "    color = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "    for i in range(pca.n_components_):\n",
    "        axes[idx].plot(mat_all, eigenvectors[i], color=color[i], label=f'PC_{i+1}', linewidth=2)\n",
    "    \n",
    "    # Formatting each subplot\n",
    "    axes[idx].set_title(f'Eigenvector Loadings for {regime}')\n",
    "    axes[idx].set_xlabel('Maturities')\n",
    "    axes[idx].legend(title=\"Components\")\n",
    "    \n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'regime_dfs' contains all regimes as DataFrames\n",
    "# and 'mat_all' represents the maturities associated with each column of the DataFrames\n",
    "\n",
    "# Prepare a single plot for comparison\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Regimes to compare\n",
    "regimes_to_plot = ['regime_1', 'regime_2']\n",
    "\n",
    "# Set up a color map and style cycle\n",
    "color_map = plt.get_cmap('tab10')  # Get a color map from matplotlib\n",
    "styles = ['-', '--']  # Line styles for differentiation\n",
    "\n",
    "# Perform PCA and plot for each selected regime\n",
    "for idx, regime in enumerate(regimes_to_plot):\n",
    "    df = regime_dfs[regime].iloc[:, :-1]  # Drop the last column\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # We use 3 components\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plot each component's loading per maturity\n",
    "    for i in range(pca.n_components_):\n",
    "        ax.plot(mat_all, eigenvectors[i], color=color_map(idx * 3 + i), linestyle=styles[idx % len(styles)],\n",
    "                label=f'{regime} PC_{i+1}', linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('Comparison of PCA Eigenvector Loadings between Regime 1 and Regime 2')\n",
    "ax.set_xlabel('Maturities')\n",
    "ax.legend(title=\"Regimes and Components\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC1 (orange for both regimes) shows how the first principal component contributes across maturities. For both regimes, PC1 generally increases with maturity, suggesting it might capture the overall level of yield curve./n \n",
    "\n",
    "PC2 (blue for both regimes) indicates the second principal component. In both regimes, PC2 starts at a negative contribution for short maturities, increases around mid-maturities, and then decreases slightly for the longest maturities. This might represent the curvature of the yield curve./n \n",
    "\n",
    "PC3 (green for both regimes) shows more variation, especially towards the long maturities, and is used to capture additional nuances in the yield curve data that PC1 and PC2 do not. /n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-06')\n",
    "end_date = pd.to_datetime('2023-07-11')\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Number of hidden states for the HMM\n",
    "n_hidden_states = 4  # Adjustable depending on desired complexity of the model\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df[tenors].dropna().values\n",
    "\n",
    "# Initialize and fit the HMM with a specified random state for reproducibility\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "\n",
    "# Set uniform initial probabilities and transition probabilities as discussed\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Predict regimes based on the fitted model\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Add regime information back to the DataFrame\n",
    "filtered_df['regime'] = np.nan\n",
    "filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df[filtered_df['regime'] == i].copy()\n",
    "\n",
    "# Function to generate colors\n",
    "def rainbow(categories):\n",
    "    c_scale = cm.rainbow(np.linspace(0, 1, len(categories)))\n",
    "    c_dict = {}\n",
    "    for i, c in zip(categories, c_scale):\n",
    "        c_dict[i] = c\n",
    "    return c_dict\n",
    "\n",
    "# Prepare subplots for all regimes\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25, 11))  # Adjust the grid size based on the number of regimes\n",
    "axes = axes.flatten()  # Flatten to iterate easily if you have more than two axes\n",
    "\n",
    "# Perform PCA for each regime\n",
    "for idx, (regime, df) in enumerate(regime_dfs.items()):\n",
    "    # Ensure that the DataFrame only has the tenors columns\n",
    "    df = df[tenors].dropna()\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # Adjust components as necessary\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    \n",
    "    # Get eigenvectors for the top components\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plotting each component's loading per maturity\n",
    "    color = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "    for i in range(pca.n_components_):\n",
    "        axes[idx].plot(tenors, eigenvectors[i], color=color[i], label=f'PC_{i+1}', linewidth=2)\n",
    "    \n",
    "    # Formatting each subplot\n",
    "    axes[idx].set_title(f'Eigenvector Loadings for {regime}')\n",
    "    axes[idx].set_xlabel('Maturities')\n",
    "    axes[idx].legend(title=\"Components\")\n",
    "    \n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare a single plot for comparison between Regime 1 and Regime 2\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Regimes to compare\n",
    "regimes_to_plot = ['regime_1', 'regime_2']\n",
    "\n",
    "# Set up a color map and style cycle\n",
    "color_map = plt.get_cmap('tab10')  # Get a color map from matplotlib\n",
    "styles = ['-', '--']  # Line styles for differentiation\n",
    "\n",
    "# Perform PCA and plot for each selected regime\n",
    "for idx, regime in enumerate(regimes_to_plot):\n",
    "    df = regime_dfs[regime][tenors].dropna()  # Ensure DataFrame has the right columns and no NaNs\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # We use 3 components\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plot each component's loading per maturity\n",
    "    for i in range(pca.n_components_):\n",
    "        ax.plot(tenors, eigenvectors[i], color=color_map(idx * 3 + i), linestyle=styles[idx % len(styles)],\n",
    "                label=f'{regime} PC_{i+1}', linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('Comparison of PCA Eigenvector Loadings between Regime 1 and Regime 2')\n",
    "ax.set_xlabel('Maturities')\n",
    "ax.legend(title=\"Regimes and Components\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-06')\n",
    "end_date = pd.to_datetime('2023-07-11')\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Number of hidden states for the HMM\n",
    "n_hidden_states = 4  # Adjustable depending on desired complexity of the model\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df[tenors].dropna().values\n",
    "\n",
    "# Initialize and fit the HMM with a specified random state for reproducibility\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "\n",
    "# Set uniform initial probabilities and transition probabilities as discussed\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Predict regimes based on the fitted model\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Add regime information back to the DataFrame\n",
    "filtered_df['regime'] = np.nan\n",
    "filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df[filtered_df['regime'] == i].copy()\n",
    "\n",
    "# Function to generate colors\n",
    "def rainbow(categories):\n",
    "    c_scale = cm.rainbow(np.linspace(0, 1, len(categories)))\n",
    "    c_dict = {}\n",
    "    for i, c in zip(categories, c_scale):\n",
    "        c_dict[i] = c\n",
    "    return c_dict\n",
    "\n",
    "# Function to calculate the explained variance of PCA components\n",
    "def explained_variance(df, tenors, n_components=3):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principalComponents = pca.fit_transform(df[tenors].dropna())\n",
    "    return np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Iterative process to re-estimate HMM parameters and perform PCA\n",
    "max_iterations = 100\n",
    "convergence_threshold = 1e-4\n",
    "previous_explained_variance = 0\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    # Predict regimes based on the current model\n",
    "    regimes = model.predict(X)\n",
    "    \n",
    "    # Add regime information back to the DataFrame\n",
    "    filtered_df['regime'] = np.nan\n",
    "    filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "    \n",
    "    # Store data for each regime in a dictionary for easy access\n",
    "    for i in range(n_hidden_states):\n",
    "        regime_name = f'regime_{i}'\n",
    "        regime_dfs[regime_name] = filtered_df[filtered_df['regime'] == i].copy()\n",
    "    \n",
    "    # Perform PCA for each regime and calculate the total explained variance\n",
    "    total_explained_variance = 0\n",
    "    for regime, df in regime_dfs.items():\n",
    "        total_explained_variance += explained_variance(df, tenors)\n",
    "    \n",
    "    # Check for convergence\n",
    "    delta = abs(total_explained_variance - previous_explained_variance)\n",
    "    if delta < convergence_threshold:\n",
    "        print(f\"Converged after {iteration + 1} iterations with delta: {delta}\")\n",
    "        break\n",
    "    previous_explained_variance = total_explained_variance\n",
    "    \n",
    "    # Refit the HMM model\n",
    "    model.fit(X)\n",
    "\n",
    "# Prepare subplots for all regimes\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25, 11))  # Adjust the grid size based on the number of regimes\n",
    "axes = axes.flatten()  # Flatten to iterate easily if you have more than two axes\n",
    "\n",
    "# Perform PCA for each regime\n",
    "for idx, (regime, df) in enumerate(regime_dfs.items()):\n",
    "    # Ensure that the DataFrame only has the tenors columns\n",
    "    df = df[tenors].dropna()\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # Adjust components as necessary\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    \n",
    "    # Get eigenvectors for the top components\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plotting each component's loading per maturity\n",
    "    color = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "    for i in range(pca.n_components_):\n",
    "        axes[idx].plot(tenors, eigenvectors[i], color=color[i], label=f'PC_{i+1}', linewidth=2)\n",
    "    \n",
    "    # Formatting each subplot\n",
    "    axes[idx].set_title(f'Eigenvector Loadings for {regime}')\n",
    "    axes[idx].set_xlabel('Maturities')\n",
    "    axes[idx].legend(title=\"Components\")\n",
    "    \n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare a single plot for comparison between Regime 1 and Regime 2\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Regimes to compare\n",
    "regimes_to_plot = ['regime_1', 'regime_2']\n",
    "\n",
    "# Set up a color map and style cycle\n",
    "color_map = plt.get_cmap('tab10')  # Get a color map from matplotlib\n",
    "styles = ['-', '--']  # Line styles for differentiation\n",
    "\n",
    "# Perform PCA and plot for each selected regime\n",
    "for idx, regime in enumerate(regimes_to_plot):\n",
    "    df = regime_dfs[regime][tenors].dropna()  # Ensure DataFrame has the right columns and no NaNs\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # We use 3 components\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plot each component's loading per maturity\n",
    "    for i in range(pca.n_components_):\n",
    "        ax.plot(tenors, eigenvectors[i], color=color_map(idx * 3 + i), linestyle=styles[idx % len(styles)],\n",
    "                label=f'{regime} PC_{i+1}', linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('Comparison of PCA Eigenvector Loadings between Regime 1 and Regime 2')\n",
    "ax.set_xlabel('Maturities')\n",
    "ax.legend(title=\"Regimes and Components\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-06')\n",
    "end_date = pd.to_datetime('2023-07-11')\n",
    "\n",
    "# Sample DataFrame `first_diff_df_cleaned` for context\n",
    "# This should be replaced with your actual DataFrame\n",
    "# first_diff_df_cleaned = pd.read_csv('your_data.csv', index_col='date', parse_dates=True)\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Function to generate colors\n",
    "def rainbow(categories):\n",
    "    c_scale = cm.rainbow(np.linspace(0, 1, len(categories)))\n",
    "    c_dict = {i: c for i, c in zip(categories, c_scale)}\n",
    "    return c_dict\n",
    "\n",
    "# Function to calculate the explained variance of PCA components\n",
    "def explained_variance(df, tenors, n_components=3):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principalComponents = pca.fit_transform(df[tenors].dropna())\n",
    "    return np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Initialize HMM model\n",
    "def initialize_hmm(n_hidden_states):\n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "    model.transmat_ = transmat\n",
    "    return model\n",
    "\n",
    "# Update transition matrix dynamically\n",
    "def update_transition_matrix(model, regime_dfs, n_hidden_states):\n",
    "    # Example dynamic update function (can be customized)\n",
    "    new_transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(new_transmat, 0.7)\n",
    "    new_transmat = new_transmat / new_transmat.sum(axis=1, keepdims=True)\n",
    "    model.transmat_ = new_transmat\n",
    "\n",
    "# Fit HMM model and iteratively update\n",
    "def fit_hmm_iteratively(X, n_hidden_states, max_iterations=100, convergence_threshold=1e-4):\n",
    "    model = initialize_hmm(n_hidden_states)\n",
    "    previous_explained_variance = 0\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        model.fit(X)  # Fit the model to the data\n",
    "        \n",
    "        # Predict regimes and update transition matrix dynamically\n",
    "        regimes = model.predict(X)\n",
    "        update_transition_matrix(model, regimes, n_hidden_states)\n",
    "\n",
    "        filtered_df['regime'] = np.nan\n",
    "        filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "\n",
    "        regime_dfs = {f'regime_{i}': filtered_df[filtered_df['regime'] == i].copy() for i in range(n_hidden_states)}\n",
    "\n",
    "        total_explained_variance = sum(explained_variance(df, tenors) for df in regime_dfs.values())\n",
    "        \n",
    "        delta = abs(total_explained_variance - previous_explained_variance)\n",
    "        if delta < convergence_threshold:\n",
    "            print(f\"Converged after {iteration + 1} iterations with delta: {delta}\")\n",
    "            break\n",
    "\n",
    "        previous_explained_variance = total_explained_variance\n",
    "\n",
    "    return model, regime_dfs\n",
    "\n",
    "# Prepare feature matrix by dropping rows with NaNs\n",
    "X = filtered_df[tenors].dropna().values\n",
    "\n",
    "# Fit HMM model and get regime DataFrames\n",
    "n_hidden_states = 4  # Number of hidden states\n",
    "model, regime_dfs = fit_hmm_iteratively(X, n_hidden_states)\n",
    "\n",
    "# Function to plot PCA eigenvectors\n",
    "def plot_pca_loadings(regime_dfs, tenors):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25, 11))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (regime, df) in enumerate(regime_dfs.items()):\n",
    "        df = df[tenors].dropna()\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(df)\n",
    "        eigenvectors = pca.components_\n",
    "        \n",
    "        color = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "        for i in range(pca.n_components_):\n",
    "            axes[idx].plot(tenors, eigenvectors[i], color=color[i], label=f'PC_{i+1}', linewidth=2)\n",
    "        \n",
    "        axes[idx].set_title(f'Eigenvector Loadings for {regime}')\n",
    "        axes[idx].set_xlabel('Maturities')\n",
    "        axes[idx].legend(title=\"Components\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to compare PCA results between two regimes\n",
    "def compare_regimes(regime_dfs, tenors, regime1, regime2):\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    regimes_to_plot = [regime1, regime2]\n",
    "    \n",
    "    color_map = plt.get_cmap('tab10')\n",
    "    styles = ['-', '--']\n",
    "\n",
    "    for idx, regime in enumerate(regimes_to_plot):\n",
    "        df = regime_dfs[regime][tenors].dropna()\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(df)\n",
    "        eigenvectors = pca.components_\n",
    "\n",
    "        for i in range(pca.n_components_):\n",
    "            ax.plot(tenors, eigenvectors[i], color=color_map(idx * 3 + i), linestyle=styles[idx % len(styles)],\n",
    "                    label=f'{regime} PC_{i+1}', linewidth=2)\n",
    "    \n",
    "    ax.set_title('Comparison of PCA Eigenvector Loadings between Regime 1 and Regime 2')\n",
    "    ax.set_xlabel('Maturities')\n",
    "    ax.legend(title=\"Regimes and Components\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot PCA loadings for all regimes\n",
    "plot_pca_loadings(regime_dfs, tenors)\n",
    "\n",
    "# Compare PCA loadings between Regime 1 and Regime 2\n",
    "compare_regimes(regime_dfs, tenors, 'regime_1', 'regime_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regime fit after convergence in explained variance and also regime convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-06')\n",
    "end_date = pd.to_datetime('2023-07-11')\n",
    "\n",
    "# Sample DataFrame `first_diff_df_cleaned` for context\n",
    "# This should be replaced with your actual DataFrame\n",
    "# first_diff_df_cleaned = pd.read_csv('your_data.csv', index_col='date', parse_dates=True)\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Function to generate colors\n",
    "def rainbow(categories):\n",
    "    c_scale = cm.rainbow(np.linspace(0, 1, len(categories)))\n",
    "    c_dict = {i: c for i, c in zip(categories, c_scale)}\n",
    "    return c_dict\n",
    "\n",
    "# Function to calculate the explained variance of PCA components\n",
    "def explained_variance(df, tenors, n_components=3):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principalComponents = pca.fit_transform(df[tenors].dropna())\n",
    "    return np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Initialize HMM model\n",
    "def initialize_hmm(n_hidden_states):\n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "    model.transmat_ = transmat\n",
    "    return model\n",
    "\n",
    "# Fit HMM model and iteratively update\n",
    "def fit_hmm_iteratively(X, n_hidden_states, max_iterations=100, convergence_threshold=1e-4):\n",
    "    model = initialize_hmm(n_hidden_states)\n",
    "    previous_explained_variance = 0\n",
    "    previous_regimes = np.zeros_like(X[:, 0])  # Initialize previous_regimes with zeros\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        model.fit(X)  # Fit the model to the data\n",
    "        regimes = model.predict(X)  # Predict regimes\n",
    "\n",
    "        # Add regime information back to the DataFrame\n",
    "        filtered_df['regime'] = np.nan\n",
    "        filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "\n",
    "        # Store data for each regime in a dictionary\n",
    "        regime_dfs = {f'regime_{i}': filtered_df[filtered_df['regime'] == i].copy() for i in range(n_hidden_states)}\n",
    "\n",
    "        # Perform PCA for each regime and calculate the total explained variance\n",
    "        total_explained_variance = sum(explained_variance(df, tenors) for df in regime_dfs.values())\n",
    "\n",
    "        # Check for convergence\n",
    "        delta = abs(total_explained_variance - previous_explained_variance)\n",
    "        regime_change = np.array_equal(regimes, previous_regimes)\n",
    "        if delta < convergence_threshold and regime_change:\n",
    "            print(f\"Converged after {iteration + 1} iterations with delta: {delta}\")\n",
    "            break\n",
    "\n",
    "        previous_explained_variance = total_explained_variance\n",
    "        previous_regimes = regimes  # Update previous_regimes with current regimes\n",
    "        print(previous_regimes.shape)\n",
    "\n",
    "    return model, regime_dfs\n",
    "\n",
    "# Prepare feature matrix by dropping rows with NaNs\n",
    "X = filtered_df[tenors].dropna().values\n",
    "\n",
    "# Fit HMM model and get regime DataFrames\n",
    "n_hidden_states = 4  # Number of hidden states\n",
    "model, regime_dfs = fit_hmm_iteratively(X, n_hidden_states)\n",
    "\n",
    "# Function to plot PCA eigenvectors\n",
    "def plot_pca_loadings(regime_dfs, tenors):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25, 11))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (regime, df) in enumerate(regime_dfs.items()):\n",
    "        df = df[tenors].dropna()\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(df)\n",
    "        eigenvectors = pca.components_\n",
    "\n",
    "        color = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "        for i in range(pca.n_components_):\n",
    "            axes[idx].plot(tenors, eigenvectors[i], color=color[i], label=f'PC_{i+1}', linewidth=2)\n",
    "\n",
    "        axes[idx].set_title(f'Eigenvector Loadings for {regime}')\n",
    "        axes[idx].set_xlabel('Maturities')\n",
    "        axes[idx].legend(title=\"Components\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to compare PCA results between two regimes\n",
    "def compare_regimes(regime_dfs, tenors, regime1, regime2):\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    regimes_to_plot = [regime1, regime2]\n",
    "\n",
    "    color_map = plt.get_cmap('tab10')\n",
    "    styles = ['-', '--']\n",
    "\n",
    "    for idx, regime in enumerate(regimes_to_plot):\n",
    "        df = regime_dfs[regime][tenors].dropna()\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(df)\n",
    "        eigenvectors = pca.components_\n",
    "\n",
    "        for i in range(pca.n_components_):\n",
    "            ax.plot(tenors, eigenvectors[i], color=color_map(idx * 3 + i), linestyle=styles[idx % len(styles)],\n",
    "                    label=f'{regime} PC_{i+1}', linewidth=2)\n",
    "\n",
    "    ax.set_title('Comparison of PCA Eigenvector Loadings between Regime 1 and Regime 2')\n",
    "    ax.set_xlabel('Maturities')\n",
    "    ax.legend(title=\"Regimes and Components\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot PCA loadings for all regimes\n",
    "plot_pca_loadings(regime_dfs, tenors)\n",
    "\n",
    "# Compare PCA loadings between Regime 1 and Regime 2\n",
    "compare_regimes(regime_dfs, tenors, 'regime_1', 'regime_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting on training data - Not working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-06')\n",
    "end_date = pd.to_datetime('2022-11-24')\n",
    "\n",
    "# Sample DataFrame `first_diff_df_cleaned` for context\n",
    "# This should be replaced with your actual DataFrame\n",
    "# first_diff_df_cleaned = pd.read_csv('your_data.csv', index_col='date', parse_dates=True)\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Function to generate colors\n",
    "def rainbow(categories):\n",
    "    c_scale = cm.rainbow(np.linspace(0, 1, len(categories)))\n",
    "    c_dict = {i: c for i, c in zip(categories, c_scale)}\n",
    "    return c_dict\n",
    "\n",
    "# Function to calculate the explained variance of PCA components\n",
    "def explained_variance(df, tenors, n_components=3):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principalComponents = pca.fit_transform(df[tenors].dropna())\n",
    "    return np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Initialize HMM model\n",
    "def initialize_hmm(n_hidden_states):\n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "    model.transmat_ = transmat\n",
    "    return model\n",
    "\n",
    "# Fit HMM model and iteratively update\n",
    "def fit_hmm_iteratively(X, n_hidden_states, max_iterations=100, convergence_threshold=1e-4):\n",
    "    model = initialize_hmm(n_hidden_states)\n",
    "    previous_explained_variance = 0\n",
    "    previous_regimes = np.zeros_like(X[:, 0])  # Initialize previous_regimes with zeros\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        model.fit(X)  # Fit the model to the data\n",
    "        regimes = model.predict(X)  # Predict regimes\n",
    "\n",
    "        # Add regime information back to the DataFrame\n",
    "        filtered_df['regime'] = np.nan\n",
    "        filtered_df.loc[filtered_df[tenors].dropna().index, 'regime'] = regimes\n",
    "\n",
    "        # Store data for each regime in a dictionary\n",
    "        regime_dfs = {f'regime_{i}': filtered_df[filtered_df['regime'] == i].copy() for i in range(n_hidden_states)}\n",
    "\n",
    "        # Perform PCA for each regime and calculate the total explained variance\n",
    "        total_explained_variance = sum(explained_variance(df, tenors) for df in regime_dfs.values())\n",
    "\n",
    "        # Check for convergence\n",
    "        delta = abs(total_explained_variance - previous_explained_variance)\n",
    "        regime_change = np.array_equal(regimes, previous_regimes)\n",
    "        if delta < convergence_threshold and regime_change:\n",
    "            print(f\"Converged after {iteration + 1} iterations with delta: {delta}\")\n",
    "            break\n",
    "\n",
    "        previous_explained_variance = total_explained_variance\n",
    "        previous_regimes = regimes  # Update previous_regimes with current regimes\n",
    "        print(previous_regimes.shape)\n",
    "\n",
    "    return model, regime_dfs\n",
    "\n",
    "# Prepare feature matrix by dropping rows with NaNs\n",
    "X = filtered_df[tenors].dropna().values\n",
    "\n",
    "# Fit HMM model and get regime DataFrames\n",
    "n_hidden_states = 4  # Number of hidden states\n",
    "model, regime_dfs = fit_hmm_iteratively(X, n_hidden_states)\n",
    "\n",
    "# Function to plot PCA eigenvectors\n",
    "def plot_pca_loadings(regime_dfs, tenors):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25, 11))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (regime, df) in enumerate(regime_dfs.items()):\n",
    "        df = df[tenors].dropna()\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(df)\n",
    "        eigenvectors = pca.components_\n",
    "\n",
    "        color = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "        for i in range(pca.n_components_):\n",
    "            axes[idx].plot(tenors, eigenvectors[i], color=color[i], label=f'PC_{i+1}', linewidth=2)\n",
    "\n",
    "        axes[idx].set_title(f'Eigenvector Loadings for {regime}')\n",
    "        axes[idx].set_xlabel('Maturities')\n",
    "        axes[idx].legend(title=\"Components\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to compare PCA results between two regimes\n",
    "def compare_regimes(regime_dfs, tenors, regime1, regime2):\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    regimes_to_plot = [regime1, regime2]\n",
    "\n",
    "    color_map = plt.get_cmap('tab10')\n",
    "    styles = ['-', '--']\n",
    "\n",
    "    for idx, regime in enumerate(regimes_to_plot):\n",
    "        df = regime_dfs[regime][tenors].dropna()\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(df)\n",
    "        eigenvectors = pca.components_\n",
    "\n",
    "        for i in range(pca.n_components_):\n",
    "            ax.plot(tenors, eigenvectors[i], color=color_map(idx * 3 + i), linestyle=styles[idx % len(styles)],\n",
    "                    label=f'{regime} PC_{i+1}', linewidth=2)\n",
    "\n",
    "    ax.set_title('Comparison of PCA Eigenvector Loadings between Regime 1 and Regime 2')\n",
    "    ax.set_xlabel('Maturities')\n",
    "    ax.legend(title=\"Regimes and Components\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot PCA loadings for all regimes\n",
    "plot_pca_loadings(regime_dfs, tenors)\n",
    "\n",
    "# Compare PCA loadings between Regime 1 and Regime 2\n",
    "compare_regimes(regime_dfs, tenors, 'regime_1', 'regime_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned_test = first_diff_df_cleaned[first_diff_df_cleaned.index >= '2022-11-24 11:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import principalcomponents as pc\n",
    "# Use out-of-sample function to derive model test yields fitting on eigen-vectors from train-set\n",
    "pc_yields_oos = pc.pca_oos(eig_vect_train =eigenvectors, spot_test = first_diff_df_cleaned_test)\n",
    "pc_yields_oos.iloc[:5,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "e_oos = (pc_yields_oos - combined_df_cleaned_test)**2\n",
    "rmse_oos = pd.Series(data = e_oos.T.mean())**0.5\n",
    "rmse_oos[0] = rmse[rmse_oos.index[0]]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,6))\n",
    "color = rainbow([\"in-sample\",\"out-of-sample\"])\n",
    "\n",
    "# Plots\n",
    "ax.plot (rmse, c=color[\"in-sample\"], label=\"in-sample\")\n",
    "ax.plot (rmse_oos, c=color[\"out-of-sample\"], label=\"out-of-sample\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_title (\"Goodness-of-fit (k = \"+ str(k) + \")\")\n",
    "ax.set_ylabel (\"RMSE\")\n",
    "ax.xaxis.set_major_formatter(years_fmt)\n",
    "ax.xaxis.set_major_locator(years_loc)\n",
    "ax.xaxis.set_minor_locator(months_loc)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Mogeng/IOHMM/blob/master/examples/notebooks/UnSupervisedIOHMM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://luisdamiano.github.io/work/gsoc17_iohmm_financial_time_series.html#filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data\n",
    "    model.fit(X)\n",
    "    regimes = model.predict(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = regimes\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Set up figures and axes for state counts and transitions\n",
    "fig, axes = plt.subplots(len(tenors) * 2, 1, figsize=(20, 10 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data using the EM algorithm\n",
    "    model.fit(X)\n",
    "\n",
    "    # Use the Forward-Backward algorithm to compute the posterior probabilities\n",
    "    logprob, posteriors = model.score_samples(X)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'regime'] = np.argmax(posteriors, axis=1)\n",
    "\n",
    "    def count_transitions(regimes):\n",
    "        transitions = 0\n",
    "        for i in range(1, len(regimes)):\n",
    "            if regimes[i] != regimes[i - 1]:\n",
    "                transitions += 1\n",
    "        return transitions\n",
    "\n",
    "    # Calculate state counts and transitions for all days\n",
    "    all_days_state_counts = df_copy.groupby(df_copy.index.date)['regime'].value_counts().unstack(fill_value=0)\n",
    "    all_days_transitions = df_copy.groupby(df_copy.index.date)['regime'].apply(list).apply(count_transitions)\n",
    "\n",
    "    # Plot State Counts (Bar plot)\n",
    "    ax_state_counts = axes[idx * 2]\n",
    "    all_days_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_state_counts)\n",
    "    ax_state_counts.set_title(f'State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_state_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_state_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_state_counts.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_state_counts.index)]\n",
    "    ax_state_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_state_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot State Transitions (Bar plot)\n",
    "    ax_transitions = axes[idx * 2 + 1]\n",
    "    all_days_transitions.plot(kind='bar', color='black', ax=ax_transitions)\n",
    "    ax_transitions.set_title(f'State Transitions for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_transitions.set_ylabel('Number of Transitions', fontsize=12)\n",
    "    ax_transitions.set_xlabel('Date', fontsize=12)\n",
    "    ax_transitions.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    transition_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(all_days_transitions.index)]\n",
    "    ax_transitions.set_xticks(range(0, len(transition_date_labels), len(transition_date_labels) // num_labels))\n",
    "    ax_transitions.set_xticklabels(transition_date_labels[::len(transition_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logprob )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posteriors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df .shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Example tenors\n",
    "tenors = ['yield_10Y']  # Replace with your actual tenors\n",
    "\n",
    "# Number of hidden states\n",
    "n_hidden_states = 4  # Set to the desired number of hidden states\n",
    "\n",
    "# Set up colors\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_hidden_states))\n",
    "\n",
    "# Set up figures and axes for state counts and differences\n",
    "fig, axes = plt.subplots(len(tenors) * 3, 1, figsize=(20, 15 * len(tenors)))  # Adjusted height\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    X = filtered_df[tenor].dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "    \n",
    "    # Initialize start probabilities and transition matrix\n",
    "    model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "    \n",
    "    # Initialize transition matrix with equal probabilities\n",
    "    transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "    np.fill_diagonal(transmat, 0.7)\n",
    "    \n",
    "    # Normalize the transition matrix to ensure each row sums to 1\n",
    "    row_sums = transmat.sum(axis=1, keepdims=True)\n",
    "    transmat = transmat / row_sums\n",
    "    \n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    # Fit the HMM to the data using the EM algorithm\n",
    "    model.fit(X)\n",
    "\n",
    "    # Use the Viterbi algorithm to find the most likely sequence of hidden states\n",
    "    viterbi_states = model.predict(X)\n",
    "\n",
    "    # Use the Forward-Backward algorithm to compute the posterior probabilities\n",
    "    logprob, posteriors = model.score_samples(X)\n",
    "    forward_states = np.argmax(posteriors, axis=1)\n",
    "\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_copy = filtered_df.copy()\n",
    "    df_copy['viterbi_regime'] = np.nan\n",
    "    df_copy['forward_regime'] = np.nan\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'viterbi_regime'] = viterbi_states\n",
    "    df_copy.loc[df_copy[tenor].dropna().index, 'forward_regime'] = forward_states\n",
    "\n",
    "    # Calculate state counts for all days (Viterbi)\n",
    "    viterbi_state_counts = df_copy.groupby(df_copy.index.date)['viterbi_regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate state counts for all days (Forward-Backward)\n",
    "    forward_state_counts = df_copy.groupby(df_copy.index.date)['forward_regime'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Identify the dates where the state counts differ\n",
    "    differing_dates = viterbi_state_counts.index[viterbi_state_counts.values.argmax(axis=1) != forward_state_counts.values.argmax(axis=1)]\n",
    "\n",
    "    # Plot Viterbi State Counts (Bar plot)\n",
    "    ax_viterbi_counts = axes[idx * 3]\n",
    "    viterbi_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_viterbi_counts)\n",
    "    ax_viterbi_counts.set_title(f'Viterbi State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_viterbi_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_viterbi_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_viterbi_counts.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    num_labels = 10\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(viterbi_state_counts.index)]\n",
    "    ax_viterbi_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_viterbi_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Plot Forward-Backward State Counts (Bar plot)\n",
    "    ax_forward_counts = axes[idx * 3 + 1]\n",
    "    forward_state_counts.plot(kind='bar', stacked=True, color=colors, ax=ax_forward_counts)\n",
    "    ax_forward_counts.set_title(f'Forward-Backward State Counts for {tenor} (From {start_date.date()} to {end_date.date()})', fontsize=15, fontweight='bold')\n",
    "    ax_forward_counts.set_ylabel('Number of States', fontsize=12)\n",
    "    ax_forward_counts.legend(title='Regimes', loc='upper left', frameon=False)\n",
    "    ax_forward_counts.grid(True)\n",
    "\n",
    "    # Reduce number of x-axis labels\n",
    "    date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(forward_state_counts.index)]\n",
    "    ax_forward_counts.set_xticks(range(0, len(date_labels), len(date_labels) // num_labels))\n",
    "    ax_forward_counts.set_xticklabels(date_labels[::len(date_labels) // num_labels], rotation=45)\n",
    "\n",
    "    # Highlight the regions where state counts differ\n",
    "    ax_difference = axes[idx * 3 + 2]\n",
    "    ax_difference.set_title(f'Regions with Different State Counts for {tenor}', fontsize=15, fontweight='bold')\n",
    "    ax_difference.set_ylabel('State', fontsize=12)\n",
    "    ax_difference.set_xlabel('Date', fontsize=12)\n",
    "    ax_difference.grid(True)\n",
    "\n",
    "    # Create a bar plot highlighting the differences\n",
    "    differences = pd.DataFrame(index=viterbi_state_counts.index)\n",
    "    differences['Difference'] = 0\n",
    "    differences.loc[differing_dates, 'Difference'] = 1\n",
    "    differences.plot(kind='bar', color='red', ax=ax_difference, legend=False)\n",
    "\n",
    "    # Reduce number of x-axis labels for differences plot\n",
    "    difference_date_labels = [date.strftime('%Y-%m-%d') for date in pd.to_datetime(differences.index)]\n",
    "    ax_difference.set_xticks(range(0, len(difference_date_labels), len(difference_date_labels) // num_labels))\n",
    "    ax_difference.set_xticklabels(difference_date_labels[::len(difference_date_labels) // num_labels], rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_differing_dates = len(differing_dates)\n",
    "total_dates = len(viterbi_state_counts.index)\n",
    "proportion_differing = num_differing_dates / total_dates\n",
    "\n",
    "  \n",
    "print(f'Number of differing dates: {num_differing_dates}')\n",
    "print(f'Total number of dates: {total_dates}')\n",
    "print(f'Proportion of period with differing state counts: {proportion_differing:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " differing_dates = viterbi_state_counts.index[viterbi_state_counts.values.argmax(axis=1) != forward_state_counts.values.argmax(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_state_counts.values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_state_counts.values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrame representing state counts\n",
    "data = {\n",
    "    'state_0': [3, 1, 0],\n",
    "    'state_1': [1, 2, 5],\n",
    "    'state_2': [4, 1, 0]\n",
    "}\n",
    "index = ['2022-02-02', '2022-02-03', '2022-02-04']\n",
    "viterbi_state_counts = pd.DataFrame(data, index=index)\n",
    "\n",
    "print(\"State Counts DataFrame:\")\n",
    "print(viterbi_state_counts)\n",
    "\n",
    "# Find the most frequent state for each date\n",
    "most_frequent_states = viterbi_state_counts.values.argmax(axis=1)\n",
    "print(\"\\nMost Frequent States:\")\n",
    "print(most_frequent_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/49880279/how-to-solve-basic-hmm-problems-with-hmmlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM to calculate the Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "timeseries = first_diff_df_cleaned['yield_10Y'].values.astype('float32')\n",
    "\n",
    "# train-test split for time series\n",
    "train_size = int(len(timeseries) * 0.67)\n",
    "test_size = len(timeseries) - train_size\n",
    "train, test = timeseries[:train_size], timeseries[train_size:]\n",
    "\n",
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - lookback):\n",
    "        feature = dataset[i:i + lookback]\n",
    "        target = dataset[i + lookback]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X).unsqueeze(-1), torch.tensor(y).unsqueeze(-1)\n",
    "\n",
    "lookback = 4\n",
    "X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the LSTMModel class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x, return_hidden=False):\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        if return_hidden:\n",
    "            return x, hn\n",
    "        x = self.linear(x[:, -1, :])  # Use the last LSTM output for prediction\n",
    "        return x\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, optimizer, loss_fn, train_loader, test_loader, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            train_rmse = evaluate_model(model, train_loader)\n",
    "            test_rmse = evaluate_model(model, test_loader)\n",
    "            print(f\"Epoch {epoch+1}: Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            predictions = model(x_batch)\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            y_pred.extend(predictions.numpy())\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "\n",
    "# Define the lookback period\n",
    "lookback_period = 10\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter configurations\n",
    "configs = [\n",
    "    {'hidden_size': 50, 'num_layers': 1, 'batch_size': 8, 'learning_rate': 0.001},\n",
    "    {'hidden_size': 100, 'num_layers': 2, 'batch_size': 16, 'learning_rate': 0.001},\n",
    "    {'hidden_size': 50, 'num_layers': 1, 'batch_size': 32, 'learning_rate': 0.01},\n",
    "    {'hidden_size': 100, 'num_layers': 2, 'batch_size': 8, 'learning_rate': 0.01},\n",
    "]\n",
    "\n",
    "# Train and evaluate models with different hyperparameters\n",
    "for config in configs:\n",
    "    print(f\"Evaluating configuration: {config}\")\n",
    "    model = LSTMModel(hidden_size=config['hidden_size'], num_layers=config['num_layers'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=config['batch_size'])\n",
    "    test_loader = data.DataLoader(data.TensorDataset(X_test, y_test), shuffle=False, batch_size=config['batch_size'])\n",
    "    \n",
    "    train_model(model, optimizer, loss_fn, train_loader, test_loader, num_epochs=100)\n",
    "    train_rmse = evaluate_model(model, train_loader)\n",
    "    test_rmse = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"Final Train RMSE: {train_rmse:.4f}, Final Test RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model, data_loader):\n",
    "    model.eval()\n",
    "    all_hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in data_loader:\n",
    "            _, hidden_states = model(X_batch, return_hidden=True)\n",
    "            all_hidden_states.append(hidden_states.squeeze().cpu().numpy())\n",
    "    return np.concatenate(all_hidden_states, axis=0)\n",
    "\n",
    "# Create data loaders for train and test sets without shuffling\n",
    "train_loader_no_shuffle = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=False, batch_size=1)\n",
    "test_loader_no_shuffle = data.DataLoader(data.TensorDataset(X_test, y_test), shuffle=False, batch_size=1)\n",
    "\n",
    "# Get hidden states for train and test sets\n",
    "train_hidden_states = get_hidden_states(model, train_loader_no_shuffle)\n",
    "test_hidden_states = get_hidden_states(model, test_loader_no_shuffle)\n",
    "\n",
    "print(\"Train hidden states shape:\", train_hidden_states.shape)\n",
    "print(\"Test hidden states shape:\", test_hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Shift train predictions for plotting\n",
    "    train_plot = np.ones_like(timeseries) * np.nan\n",
    "    y_pred_train = model(X_train)\n",
    "    train_plot[lookback:train_size] = y_pred_train.squeeze().cpu().numpy()\n",
    "\n",
    "    # Shift test predictions for plotting\n",
    "    test_plot = np.ones_like(timeseries) * np.nan\n",
    "    y_pred_test = model(X_test)\n",
    "    test_plot[train_size+lookback:len(timeseries)] = y_pred_test.squeeze().cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(timeseries, label='Original Data')\n",
    "plt.plot(train_plot, c='r', label='Train Predictions')\n",
    "plt.plot(test_plot, c='g', label='Test Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Simulating the speed.csv dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data_size = 100  # Number of data points\n",
    "\n",
    "# Generate random data for the 'rt' column, which we assume is the target variable\n",
    "rt = np.random.normal(loc=50, scale=10, size=data_size)  # Normal distribution\n",
    "\n",
    "# Create a DataFrame\n",
    "speed = pd.DataFrame({\n",
    "    'rt': rt\n",
    "})\n",
    "\n",
    "# Normalize 'rt' to bring it to a more common scale (not always necessary but often a good practice)\n",
    "scaler = MinMaxScaler()\n",
    "speed['rt'] = scaler.fit_transform(speed[['rt']])\n",
    "\n",
    "print(speed.head())\n",
    "\n",
    "# Placeholder for IOHMM import (simulated as we cannot run it)\n",
    "# from IOHMM import UnSupervisedIOHMM, OLS, CrossEntropyMNL\n",
    "\n",
    "# Initialize the model with two hidden states (assuming the import works)\n",
    "SHMM = UnSupervisedIOHMM(num_states=2, max_EM_iter=200, EM_tol=1e-6)\n",
    "\n",
    "# Set models for emissions, transitions, and initial probabilities\n",
    "SHMM.set_models(\n",
    "    model_emissions=[OLS()],\n",
    "    model_transition=CrossEntropyMNL(solver='lbfgs'),\n",
    "    model_initial=CrossEntropyMNL(solver='lbfgs')\n",
    ")\n",
    "\n",
    "# Specify inputs (no covariates in this simple example)\n",
    "SHMM.set_inputs(covariates_initial=[], covariates_transition=[], covariates_emissions=[[]])\n",
    "\n",
    "# Define the output target column from the dataframe\n",
    "SHMM.set_outputs([['rt']])\n",
    "\n",
    "# Set the data for the model\n",
    "SHMM.set_data([speed])\n",
    "\n",
    "# Simulating training (will not execute without actual library and method definitions)\n",
    "try:\n",
    "    SHMM.train()\n",
    "    print(\"Model training completed successfully.\")\n",
    "    # Assume we print model parameters or visualize outputs here\n",
    "except Exception as e:\n",
    "    print(\"Failed during training:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Assuming each state has a list of models, and we are interested in the first model of each state\n",
    "    # Print the coefficients for the OLS model for each hidden state\n",
    "    print(\"Coefficients for State 0:\", SHMM.model_emissions[0][0].coef)\n",
    "    print(\"Coefficients for State 1:\", SHMM.model_emissions[1][0].coef)\n",
    "    \n",
    "    # Print the scale/dispersion for the OLS model for each hidden state\n",
    "    print(\"Scale/Dispersion for State 0:\", np.sqrt(SHMM.model_emissions[0][0].dispersion))\n",
    "    print(\"Scale/Dispersion for State 1:\", np.sqrt(SHMM.model_emissions[1][0].dispersion))\n",
    "    \n",
    "    # Assuming the transition model does not depend on the input features in this simple case\n",
    "    empty_input = np.array([[]])\n",
    "    print(\"Transition Probabilities from State 0:\", np.exp(SHMM.model_transition.predict_log_proba(empty_input)[0]))\n",
    "    print(\"Transition Probabilities from State 1:\", np.exp(SHMM.model_transition.predict_log_proba(empty_input)[1]))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error in extracting or printing model parameters:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Print the coefficients for the OLS model for each hidden state\n",
    "    print(\"Coefficients for State 0:\", SHMM.model_emissions[0][0].coef)\n",
    "    print(\"Coefficients for State 1:\", SHMM.model_emissions[1][0].coef)\n",
    "    \n",
    "    # Print the scale/dispersion for the OLS model for each hidden state\n",
    "    print(\"Scale/Dispersion for State 0:\", np.sqrt(SHMM.model_emissions[0][0].dispersion))\n",
    "    print(\"Scale/Dispersion for State 1:\", np.sqrt(SHMM.model_emissions[1][0].dispersion))\n",
    "    \n",
    "    # Try to get transition probabilities between hidden states\n",
    "    empty_input = np.array([[]])\n",
    "    if hasattr(SHMM.model_transition, 'predict_proba'):\n",
    "        print(\"Transition Probabilities from State 0:\", SHMM.model_transition.predict_proba(empty_input))\n",
    "    else:\n",
    "        print(\"No predict_proba method available. Check the correct method for transition probabilities.\")\n",
    "except Exception as e:\n",
    "    print(\"Error in extracting or printing model parameters:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class PPCA:\n",
    "    \"\"\"\n",
    "    Probabilistic PCA Model\n",
    "    Ref: Bishop, C. M. & Tipping, M. E. (2001). Probabilistic Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "        self.mu_mle_ = None\n",
    "        self.sample_variance_ = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"PPCA(n_samples={self.n_samples_};\"\n",
    "            f\"n_features={self.n_features_};\"\n",
    "            f\"latent_dim={self.latent_dim})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        self.n_samples_ = x.shape[0]\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.mu_mle_ = self.x.mean(axis=0)\n",
    "        self.sample_variance_ = self.sample_variance()\n",
    "        return self\n",
    "\n",
    "    def sample_variance(self) -> np.ndarray:\n",
    "        S = (1 / self.n_samples_) * (self.x - self.mu_mle_).T @ (self.x - self.mu_mle_)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        return S\n",
    "\n",
    "    def eigen_decomposition(self):\n",
    "        vals, vecs = np.linalg.eigh(self.sample_variance_)\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        idx = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[idx], vecs[:, idx]\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        return vals, vecs\n",
    "\n",
    "    def model_params(self):\n",
    "        vals, vecs = self.eigen_decomposition()\n",
    "        variance_mle = (\n",
    "            abs(1 / (self.n_features_ - self.latent_dim))\n",
    "            * vals[-self.latent_dim :].sum()\n",
    "        )\n",
    "        W_mle = vecs[:, : self.latent_dim] @ np.diag(\n",
    "            (vals[: self.latent_dim] - variance_mle) ** 0.5\n",
    "        )\n",
    "        C_mle = W_mle @ W_mle.T + variance_mle * np.identity(self.n_features_)\n",
    "        return variance_mle, W_mle, C_mle\n",
    "\n",
    "    def marginal(self):\n",
    "        self.variance_mle, self.W_mle, self.C_mle = self.model_params()\n",
    "        return multivariate_normal(\n",
    "            mean=self.mu_mle_,\n",
    "            cov=self.C_mle,\n",
    "            allow_singular=True,\n",
    "        )\n",
    "\n",
    "    def sample(self, size: int):\n",
    "        return self.marginal().rvs(size=size)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    data = np.random.randn(100, 5)  # Generate some random data\n",
    "    ppca = PPCA(latent_dim=2)\n",
    "    ppca.fit(data)\n",
    "    print(ppca)\n",
    "    samples = ppca.sample(10)  # Generate 10 samples from the model\n",
    "    print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    data = np.random.randn(100, 5)  # Generate some random data\n",
    "    ppca = PPCA(latent_dim=3)\n",
    "    ppca.fit(data)\n",
    "    print(ppca)\n",
    "    samples = ppca.sample(10)  # Generate 10 samples from the model\n",
    "    print(\"Samples:\\n\", samples)\n",
    "\n",
    "    transformed_data = ppca.transform(data)  # Transform data to get the principal components\n",
    "    print(\"First three principal components:\\n\", transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class PPCA:\n",
    "    \"\"\"\n",
    "    Probabilistic PCA Model\n",
    "    Ref: Bishop, C. M. & Tipping, M. E. (2001). Probabilistic Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "        self.mu_mle_ = None\n",
    "        self.sample_variance_ = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"PPCA(n_samples={self.n_samples_};\"\n",
    "            f\"n_features={self.n_features_};\"\n",
    "            f\"latent_dim={self.latent_dim})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        self.n_samples_ = x.shape[0]\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.mu_mle_ = self.x.mean(axis=0)\n",
    "        self.sample_variance_ = self.sample_variance()\n",
    "        return self\n",
    "\n",
    "    def sample_variance(self) -> np.ndarray:\n",
    "        S = (1 / self.n_samples_) * (self.x - self.mu_mle_).T @ (self.x - self.mu_mle_)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        return S\n",
    "\n",
    "    def eigen_decomposition(self):\n",
    "        vals, vecs = np.linalg.eigh(self.sample_variance_)\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        idx = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[idx], vecs[:, idx]\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        return vals, vecs\n",
    "\n",
    "    def model_params(self):\n",
    "        vals, vecs = self.eigen_decomposition()\n",
    "        variance_mle = (\n",
    "            abs(1 / (self.n_features_ - self.latent_dim))\n",
    "            * vals[-self.latent_dim :].sum()\n",
    "        )\n",
    "        W_mle = vecs[:, : self.latent_dim] @ np.diag(\n",
    "            (vals[: self.latent_dim] - variance_mle) ** 0.5\n",
    "        )\n",
    "        C_mle = W_mle @ W_mle.T + variance_mle * np.identity(self.n_features_)\n",
    "        print(\"W_mle:\\n\", W_mle)\n",
    "        print(\"variance_mle:\", variance_mle)\n",
    "        print(\"C_mle:\\n\", C_mle)\n",
    "        assert not np.any(np.isnan(W_mle)), \"W_mle contains NaNs\"\n",
    "        assert not np.any(np.isnan(variance_mle)), \"variance_mle contains NaNs\"\n",
    "        assert not np.any(np.isnan(C_mle)), \"C_mle contains NaNs\"\n",
    "        assert not np.any(np.isinf(W_mle)), \"W_mle contains Infs\"\n",
    "        assert not np.any(np.isinf(variance_mle)), \"variance_mle contains Infs\"\n",
    "        assert not np.any(np.isinf(C_mle)), \"C_mle contains Infs\"\n",
    "        return variance_mle, W_mle, C_mle\n",
    "\n",
    "    def marginal(self):\n",
    "        self.variance_mle, self.W_mle, self.C_mle = self.model_params()\n",
    "        return multivariate_normal(\n",
    "            mean=self.mu_mle_,\n",
    "            cov=self.C_mle,\n",
    "            allow_singular=True,\n",
    "        )\n",
    "\n",
    "    def sample(self, size: int):\n",
    "        return self.marginal().rvs(size=size)\n",
    "\n",
    "    def transform(self, x: np.ndarray):\n",
    "        centered_x = x - self.mu_mle_\n",
    "        Z = np.linalg.inv(self.W_mle.T @ self.W_mle + self.variance_mle * np.eye(self.latent_dim)) @ self.W_mle.T @ centered_x.T\n",
    "        return Z.T\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    # Simulate a dataset similar to first_diff_df_cleaned with shape (12118, 17)\n",
    "    data = np.random.randn(12118, 17)\n",
    "    ppca = PPCA(latent_dim=3)\n",
    "    ppca.fit(data)\n",
    "    print(ppca)\n",
    "    samples = ppca.sample(10)  # Generate 10 samples from the model\n",
    "    print(\"Samples:\\n\", samples)\n",
    "\n",
    "    transformed_data = ppca.transform(data)  # Transform data to get the principal components\n",
    "    print(\"First three principal components:\\n\", transformed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class PPCA:\n",
    "    \"\"\"\n",
    "    Probabilistic PCA Model\n",
    "    Ref: Bishop, C. M. & Tipping, M. E. (2001). Probabilistic Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "        self.mu_mle_ = None\n",
    "        self.sample_variance_ = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"PPCA(n_samples={self.n_samples_};\"\n",
    "            f\"n_features={self.n_features_};\"\n",
    "            f\"latent_dim={self.latent_dim})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        self.n_samples_ = x.shape[0]\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.mu_mle_ = self.x.mean(axis=0)\n",
    "        self.sample_variance_ = self.sample_variance()\n",
    "        return self\n",
    "\n",
    "    def sample_variance(self) -> np.ndarray:\n",
    "        S = (1 / self.n_samples_) * (self.x - self.mu_mle_).T @ (self.x - self.mu_mle_)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        return S\n",
    "\n",
    "    def eigen_decomposition(self):\n",
    "        vals, vecs = np.linalg.eigh(self.sample_variance_)\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        idx = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[idx], vecs[:, idx]\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        return vals, vecs\n",
    "\n",
    "    def model_params(self):\n",
    "        vals, vecs = self.eigen_decomposition()\n",
    "        variance_mle = (\n",
    "            abs(1 / (self.n_features_ - self.latent_dim))\n",
    "            * vals[-self.latent_dim:].sum()\n",
    "        )\n",
    "        W_mle = vecs[:, :self.latent_dim] @ np.diag(\n",
    "            (vals[:self.latent_dim] - variance_mle) ** 0.5\n",
    "        )\n",
    "        C_mle = W_mle @ W_mle.T + variance_mle * np.identity(self.n_features_)\n",
    "        print(\"W_mle:\\n\", W_mle)\n",
    "        print(\"variance_mle:\", variance_mle)\n",
    "        print(\"C_mle:\\n\", C_mle)\n",
    "        assert not np.any(np.isnan(W_mle)), \"W_mle contains NaNs\"\n",
    "        assert not np.any(np.isnan(variance_mle)), \"variance_mle contains NaNs\"\n",
    "        assert not np.any(np.isnan(C_mle)), \"C_mle contains NaNs\"\n",
    "        assert not np.any(np.isinf(W_mle)), \"W_mle contains Infs\"\n",
    "        assert not np.any(np.isinf(variance_mle)), \"variance_mle contains Infs\"\n",
    "        assert not np.any(np.isinf(C_mle)), \"C_mle contains Infs\"\n",
    "        return variance_mle, W_mle, C_mle\n",
    "\n",
    "    def marginal(self):\n",
    "        self.variance_mle, self.W_mle, self.C_mle = self.model_params()\n",
    "        return multivariate_normal(\n",
    "            mean=self.mu_mle_,\n",
    "            cov=self.C_mle,\n",
    "            allow_singular=True,\n",
    "        )\n",
    "\n",
    "    def sample(self, size: int):\n",
    "        return self.marginal().rvs(size=size)\n",
    "\n",
    "    def transform(self, x: np.ndarray):\n",
    "        centered_x = x - self.mu_mle_\n",
    "        Z = np.linalg.inv(self.W_mle.T @ self.W_mle + self.variance_mle * np.eye(self.latent_dim)) @ self.W_mle.T @ centered_x.T\n",
    "        return Z.T\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    # Simulate a dataset similar to first_diff_df_cleaned with shape (12118, 17)\n",
    "    data = np.random.randn(12118, 17)\n",
    "    ppca = PPCA(latent_dim=3)\n",
    "    ppca.fit(data)\n",
    "    print(ppca)\n",
    "    samples = ppca.sample(10)  # Generate 10 samples from the model\n",
    "    print(\"Samples:\\n\", samples)\n",
    "\n",
    "    transformed_data = ppca.transform(data)  # Transform data to get the principal components\n",
    "    print(\"First three principal components for each sample:\\n\", transformed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class PPCA:\n",
    "    \"\"\"\n",
    "    Probabilistic PCA Model\n",
    "    Ref: Bishop, C. M. & Tipping, M. E. (2001). Probabilistic Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "        self.mu_mle_ = None\n",
    "        self.sample_variance_ = None\n",
    "        self.eig_vect = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"PPCA(n_samples={self.n_samples_};\"\n",
    "            f\"n_features={self.n_features_};\"\n",
    "            f\"latent_dim={self.latent_dim})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        self.n_samples_ = x.shape[0]\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.mu_mle_ = self.x.mean(axis=0)\n",
    "        self.sample_variance_ = self.sample_variance()\n",
    "        self.eig_vect = self.eigen_decomposition()[1]\n",
    "        return self\n",
    "\n",
    "    def sample_variance(self) -> np.ndarray:\n",
    "        S = (1 / self.n_samples_) * (self.x - self.mu_mle_).T @ (self.x - self.mu_mle_)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        return S\n",
    "\n",
    "    def eigen_decomposition(self):\n",
    "        vals, vecs = np.linalg.eigh(self.sample_variance_)\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        idx = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[idx], vecs[:, idx]\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        return vals, vecs\n",
    "\n",
    "    def model_params(self):\n",
    "        vals, vecs = self.eigen_decomposition()\n",
    "        variance_mle = (\n",
    "            abs(1 / (self.n_features_ - self.latent_dim))\n",
    "            * vals[-self.latent_dim :].sum()\n",
    "        )\n",
    "        W_mle = vecs[:, : self.latent_dim] @ np.diag(\n",
    "            (vals[: self.latent_dim] - variance_mle) ** 0.5\n",
    "        )\n",
    "        C_mle = W_mle @ W_mle.T + variance_mle * np.identity(self.n_features_)\n",
    "        return variance_mle, W_mle, C_mle\n",
    "\n",
    "    def marginal(self):\n",
    "        self.variance_mle, self.W_mle, self.C_mle = self.model_params()\n",
    "        return multivariate_normal(\n",
    "            mean=self.mu_mle_,\n",
    "            cov=self.C_mle,\n",
    "            allow_singular=True,\n",
    "        )\n",
    "\n",
    "    def sample(self, size: int):\n",
    "        return self.marginal().rvs(size=size)\n",
    "\n",
    "    def get_top_eigenvectors(self, k: int):\n",
    "        return self.eig_vect[:, :k]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    data = first_diff_df_cleaned  # Generate some random data\n",
    "    ppca = PPCA(latent_dim=3)\n",
    "    ppca.fit(data)\n",
    "    print(ppca)\n",
    "\n",
    "    top_k_eigenvectors = ppca.get_top_eigenvectors(3)\n",
    "    print(\"Top 3 eigenvectors:\\n\", top_k_eigenvectors)\n",
    "\n",
    "    # Plot the top k eigenvectors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(top_k_eigenvectors.shape[1]):\n",
    "        plt.plot(top_k_eigenvectors[:, i], label=f'PC_{i+1}', linewidth=2)\n",
    "    plt.title('Top 3 Principal Components from PPCA')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Component Values')\n",
    "    plt.legend(title=\"Components\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class PPCA:\n",
    "    \"\"\"\n",
    "    Probabilistic PCA Model\n",
    "    Ref: Bishop, C. M. & Tipping, M. E. (2001). Probabilistic Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "        self.mu_mle_ = None\n",
    "        self.sample_variance_ = None\n",
    "        self.eig_vect = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"PPCA(n_samples={self.n_samples_};\"\n",
    "            f\"n_features={self.n_features_};\"\n",
    "            f\"latent_dim={self.latent_dim})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        self.n_samples_ = x.shape[0]\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.mu_mle_ = self.x.mean(axis=0)\n",
    "        self.sample_variance_ = self.sample_variance()\n",
    "        self.eig_vect = self.eigen_decomposition()[1]\n",
    "        return self\n",
    "\n",
    "    def sample_variance(self) -> np.ndarray:\n",
    "        S = (1 / self.n_samples_) * (self.x - self.mu_mle_).T @ (self.x - self.mu_mle_)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        return S\n",
    "\n",
    "    def eigen_decomposition(self):\n",
    "        vals, vecs = np.linalg.eigh(self.sample_variance_)\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        idx = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[idx], vecs[:, idx]\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        return vals, vecs\n",
    "\n",
    "    def model_params(self):\n",
    "        vals, vecs = self.eigen_decomposition()\n",
    "        variance_mle = (\n",
    "            abs(1 / (self.n_features_ - self.latent_dim))\n",
    "            * vals[-self.latent_dim :].sum()\n",
    "        )\n",
    "        W_mle = vecs[:, : self.latent_dim] @ np.diag(\n",
    "            (vals[: self.latent_dim] - variance_mle) ** 0.5\n",
    "        )\n",
    "        C_mle = W_mle @ W_mle.T + variance_mle * np.identity(self.n_features_)\n",
    "        return variance_mle, W_mle, C_mle\n",
    "\n",
    "    def marginal(self):\n",
    "        self.variance_mle, self.W_mle, self.C_mle = self.model_params()\n",
    "        return multivariate_normal(\n",
    "            mean=self.mu_mle_,\n",
    "            cov=self.C_mle,\n",
    "            allow_singular=True,\n",
    "        )\n",
    "\n",
    "    def sample(self, size: int):\n",
    "        return self.marginal().rvs(size=size)\n",
    "\n",
    "    def get_top_eigenvectors(self, k: int):\n",
    "        return self.eig_vect[:, :k]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming first_diff_df_cleaned is a DataFrame with column names\n",
    "\n",
    "    data = first_diff_df_cleaned\n",
    "    ppca = PPCA(latent_dim=3)\n",
    "    ppca.fit(data)\n",
    "    print(ppca)\n",
    "\n",
    "    top_k_eigenvectors_ppca = ppca.get_top_eigenvectors(3)\n",
    "    print(\"Top 3 eigenvectors from PPCA:\\n\", top_k_eigenvectors_ppca)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    features = first_diff_df_cleaned.columns\n",
    "    for i in range(3):\n",
    "        plt.plot(features, top_k_eigenvectors_ppca[:, i], label=f'PPCA PC_{i+1}', linestyle='--', linewidth=2)\n",
    "        #plt.plot(features, top_k_eigenvectors_pca[:, i], label=f'PCA PC_{i+1}', linewidth=2)\n",
    "    plt.title('Comparison of Principal Components from PPCA and PCA')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Component Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Components\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import principalcomponents as PCA\n",
    "\n",
    "\n",
    "pc_model_diff=pc.PCA(spot = first_diff_df_cleaned,maturities=mat_all,k=3)\n",
    "k=3\n",
    "pc_scores_diff   = pc_model_diff.eig_scores_k\n",
    "pc_vect_diff     = pc_model_diff.eig_vect_k\n",
    "pc_vect_inv_diff = pc_model_diff.eig_vect_inv_k\n",
    "pc_yields_diff   = pc_model_diff.yields\n",
    "pc_idx_diff    = pc_model_diff.idx[:k]\n",
    "pc_idx2_diff    = pc_model_diff.idx[:3]\n",
    "\n",
    "pc_vect_diff\n",
    "\n",
    "from matplotlib.pyplot import cm\n",
    "def rainbow(categories):\n",
    "    \"\"\"\n",
    "    This function generates a dictionary of color codes for each category.\n",
    "    \"\"\"\n",
    "    c_scale = cm.rainbow(np.linspace(0,1,len(categories)))\n",
    "    c_dict = {}\n",
    "\n",
    "    for i,c in zip(categories,c_scale):\n",
    "        c_dict[i] = c\n",
    "        \n",
    "    return c_dict\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\n",
    "color = cm.rainbow(np.linspace(0,1,3))\n",
    "\n",
    "for i,c in zip(pc_idx_diff, color):\n",
    "    ax.plot(pc_vect_diff[i], c=c, label=i, linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title (\"Eigenvector Loadings\")\n",
    "ax.set_xlabel (\"Maturities\")\n",
    "ax.legend(title=\"Components\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_vect_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "\n",
    "class PPCA:\n",
    "    \"\"\"\n",
    "    Probabilistic PCA Model\n",
    "    Ref: Bishop, C. M. & Tipping, M. E. (2001). Probabilistic Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "        self.mu_mle_ = None\n",
    "        self.sample_variance_ = None\n",
    "        self.eig_vect = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"PPCA(n_samples={self.n_samples_};\"\n",
    "            f\"n_features={self.n_features_};\"\n",
    "            f\"latent_dim={self.latent_dim})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        self.n_samples_ = x.shape[0]\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.mu_mle_ = self.x.mean(axis=0)\n",
    "        self.sample_variance_ = self.sample_variance()\n",
    "        self.eig_vect = self.eigen_decomposition()[1]\n",
    "        return self\n",
    "\n",
    "    def sample_variance(self) -> np.ndarray:\n",
    "        S = (1 / self.n_samples_) * (self.x - self.mu_mle_).T @ (self.x - self.mu_mle_)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        return S\n",
    "\n",
    "    def eigen_decomposition(self):\n",
    "        vals, vecs = np.linalg.eigh(self.sample_variance_)\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        idx = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[idx], vecs[:, idx]\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        return vals, vecs\n",
    "\n",
    "    def model_params(self):\n",
    "        vals, vecs = self.eigen_decomposition()\n",
    "        variance_mle = (\n",
    "            abs(1 / (self.n_features_ - self.latent_dim))\n",
    "            * vals[-self.latent_dim:].sum()\n",
    "        )\n",
    "        W_mle = vecs[:, :self.latent_dim] @ np.diag(\n",
    "            (vals[:self.latent_dim] - variance_mle) ** 0.5\n",
    "        )\n",
    "        C_mle = W_mle @ W_mle.T + variance_mle * np.identity(self.n_features_)\n",
    "        return variance_mle, W_mle, C_mle\n",
    "\n",
    "    def marginal(self):\n",
    "        self.variance_mle, self.W_mle, self.C_mle = self.model_params()\n",
    "        return multivariate_normal(\n",
    "            mean=self.mu_mle_,\n",
    "            cov=self.C_mle,\n",
    "            allow_singular=True,\n",
    "        )\n",
    "\n",
    "    def sample(self, size: int):\n",
    "        return self.marginal().rvs(size=size)\n",
    "\n",
    "    def get_top_eigenvectors(self, k: int):\n",
    "        return self.eig_vect[:, :k]\n",
    "\n",
    "\n",
    "class PCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spot, maturities, k):\n",
    "        self.spot = spot\n",
    "        self.mat = maturities\n",
    "        self.k = k\n",
    "        self.pca_cov()\n",
    "        self.pca_eig()\n",
    "        self.pca_scores()\n",
    "        self.pca_backtrans()\n",
    "\n",
    "    def pca_cov(self):\n",
    "        self.cov_matr = np.array(self.spot).T\n",
    "        self.cov_matr = np.cov(self.cov_matr, bias=True)\n",
    "        self.cov_matr = pd.DataFrame(data=self.cov_matr, columns=self.mat, index=self.mat)\n",
    "\n",
    "    def pca_eig(self):\n",
    "        eig = np.linalg.eig(self.cov_matr)\n",
    "        self.idx = list([\"PC_\" + str(i) for i in range(1, eig[0].shape[0] + 1)])\n",
    "        self.eig_vals = pd.DataFrame(eig[0].real, columns=[\"eig_val\"], index=self.idx)\n",
    "        self.eig_vals[\"eig_val_rel\"] = self.eig_vals[\"eig_val\"].apply(lambda x: x / self.eig_vals[\"eig_val\"].sum())\n",
    "        self.eig_vals[\"eig_val_abs\"] = self.eig_vals[\"eig_val_rel\"].cumsum()\n",
    "        self.eig_vect = pd.DataFrame(eig[1].real, index=self.mat, columns=self.idx)\n",
    "        self.eig_vect_k = self.eig_vect.iloc[:, :self.k]\n",
    "\n",
    "    def pca_scores(self):\n",
    "        self.eig_scores = np.matrix(self.spot) * np.matrix(self.eig_vect)\n",
    "        self.eig_scores = pd.DataFrame(data=self.eig_scores, columns=self.idx, index=pd.to_datetime(self.spot.index))\n",
    "        self.eig_scores_k = self.eig_scores.iloc[:, :self.k]\n",
    "\n",
    "    def pca_backtrans(self):\n",
    "        self.eig_vect_inv = pd.DataFrame(data=np.linalg.inv(np.matrix(self.eig_vect)), columns=self.mat, index=self.idx)\n",
    "        self.eig_vect_inv_k = self.eig_vect_inv.iloc[:self.k, :]\n",
    "        self.yields = np.matrix(self.eig_scores_k) * np.matrix(self.eig_vect_inv_k)\n",
    "        self.yields = pd.DataFrame(data=self.yields, columns=self.mat, index=self.eig_scores_k.index)\n",
    "\n",
    "    def pca_oos(self, eig_vect_train, spot_test):\n",
    "        eig_scores_oos = np.matrix(spot_test) * np.matrix(eig_vect_train)\n",
    "        eig_scores_oos = pd.DataFrame(data=eig_scores_oos, columns=self.idx, index=pd.to_datetime(spot_test.index))\n",
    "        eig_scores_oos_k = eig_scores_oos.iloc[:, :self.k]\n",
    "        eig_vect_inv_oos = pd.DataFrame(data=np.linalg.inv(np.matrix(eig_vect_train)), columns=self.mat, index=self.idx)\n",
    "        eig_vect_inv_oos_k = eig_vect_inv_oos.iloc[:self.k, :]\n",
    "        yields_oos = np.matrix(eig_scores_oos_k) * np.matrix(eig_vect_inv_oos_k)\n",
    "        yields_oos = pd.DataFrame(data=yields_oos, columns=self.mat, index=eig_scores_oos_k.index)\n",
    "        return yields_oos\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming first_diff_df_cleaned is a DataFrame\n",
    "    # = pd.DataFrame(np.random.randn(12118, 17), columns=[f'Feature_{i+1}' for i in range(17)])  # Simulate your dataset\n",
    "\n",
    "    data = first_diff_df_cleaned\n",
    "\n",
    "    # PPCA\n",
    "    ppca = PPCA(latent_dim=3)\n",
    "    ppca.fit(data)\n",
    "    print(ppca)\n",
    "    top_k_eigenvectors_ppca = ppca.get_top_eigenvectors(3)\n",
    "    print(\"Top 3 eigenvectors from PPCA:\\n\", top_k_eigenvectors_ppca)\n",
    "\n",
    "    # PCA using scikit-learn\n",
    "    pca_sklearn = SklearnPCA(n_components=3)\n",
    "    pca_sklearn.fit(data)\n",
    "    top_k_eigenvectors_pca_sklearn = pca_sklearn.components_.T\n",
    "    print(\"Top 3 eigenvectors from PCA (scikit-learn):\\n\", top_k_eigenvectors_pca_sklearn)\n",
    "\n",
    "    # PCA custom\n",
    "    pca_custom = PCA(first_diff_df_cleaned, first_diff_df_cleaned.columns.tolist(), 3)\n",
    "    top_k_eigenvectors_pca_custom = pca_custom.eig_vect_k\n",
    "    print(\"Top 3 eigenvectors from custom PCA:\\n\", top_k_eigenvectors_pca_custom)\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    features = first_diff_df_cleaned.columns\n",
    "    for i in range(3):\n",
    "        plt.plot(features, top_k_eigenvectors_ppca[:, i], label=f'PPCA PC_{i+1}', linestyle='--', linewidth=2)\n",
    "        plt.plot(features, top_k_eigenvectors_pca_sklearn[:, i], label=f'PCA (scikit-learn) PC_{i+1}', linewidth=2)\n",
    "        plt.plot(features, top_k_eigenvectors_pca_custom.iloc[:, i], label=f'Custom PCA PC_{i+1}', linestyle=':', linewidth=2)\n",
    "    plt.title('Comparison of Principal Components from PPCA, PCA (scikit-learn), and Custom PCA')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Component Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Components\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "features = first_diff_df_cleaned.columns\n",
    "\n",
    "# Loop through the first 3 principal components\n",
    "for i in range(3):\n",
    "    plt.plot(features, top_k_eigenvectors_ppca[:, i], label=f'PPCA PC_{i+1}', linestyle='--', linewidth=2)\n",
    "    # Multiply the third principal component by -1 for the scikit-learn PCA\n",
    "    if i == 2:\n",
    "        plt.plot(features, -top_k_eigenvectors_pca_sklearn[:, i], label=f'PCA (scikit-learn) PC_{i+1}', linewidth=2)\n",
    "    else:\n",
    "        plt.plot(features, top_k_eigenvectors_pca_sklearn[:, i], label=f'PCA (scikit-learn) PC_{i+1}', linewidth=2)\n",
    "    plt.plot(features, top_k_eigenvectors_pca_custom.iloc[:, i], label=f'Custom PCA PC_{i+1}', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.title('Comparison of Principal Components from PPCA, PCA (scikit-learn), and Custom PCA')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Component Values')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Components\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "\n",
    "class PPCA:\n",
    "    \"\"\"\n",
    "    Probabilistic PCA Model\n",
    "    Ref: Bishop, C. M. & Tipping, M. E. (2001). Probabilistic Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "        self.mu_mle_ = None\n",
    "        self.sample_variance_ = None\n",
    "        self.eig_vect = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"PPCA(n_samples={self.n_samples_};\"\n",
    "            f\"n_features={self.n_features_};\"\n",
    "            f\"latent_dim={self.latent_dim})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        self.n_samples_ = x.shape[0]\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.mu_mle_ = self.x.mean(axis=0)\n",
    "        self.sample_variance_ = self.sample_variance()\n",
    "        self.eig_vect = self.eigen_decomposition()[1]\n",
    "        return self\n",
    "\n",
    "    def sample_variance(self) -> np.ndarray:\n",
    "        S = (1 / self.n_samples_) * (self.x - self.mu_mle_).T @ (self.x - self.mu_mle_)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        return S\n",
    "\n",
    "    def eigen_decomposition(self):\n",
    "        vals, vecs = np.linalg.eigh(self.sample_variance_)\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        idx = vals.argsort()[::-1]\n",
    "        vals, vecs = vals[idx], vecs[:, idx]\n",
    "        assert np.allclose(self.sample_variance_ @ vecs, vecs @ np.diag(vals))\n",
    "        return vals, vecs\n",
    "\n",
    "    def model_params(self):\n",
    "        vals, vecs = self.eigen_decomposition()\n",
    "        variance_mle = (\n",
    "            abs(1 / (self.n_features_ - self.latent_dim))\n",
    "            * vals[-self.latent_dim:].sum()\n",
    "        )\n",
    "        W_mle = vecs[:, :self.latent_dim] @ np.diag(\n",
    "            (vals[:self.latent_dim] - variance_mle) ** 0.5\n",
    "        )\n",
    "        C_mle = W_mle @ W_mle.T + variance_mle * np.identity(self.n_features_)\n",
    "        return variance_mle, W_mle, C_mle\n",
    "\n",
    "    def marginal(self):\n",
    "        self.variance_mle, self.W_mle, self.C_mle = self.model_params()\n",
    "        return multivariate_normal(\n",
    "            mean=self.mu_mle_,\n",
    "            cov=self.C_mle,\n",
    "            allow_singular=True,\n",
    "        )\n",
    "\n",
    "    def sample(self, size: int):\n",
    "        return self.marginal().rvs(size=size)\n",
    "\n",
    "    def get_top_eigenvectors(self, k: int):\n",
    "        return self.eig_vect[:, :k]\n",
    "\n",
    "class CustomPCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spot, maturities, k):\n",
    "        self.spot = spot\n",
    "        self.mat = maturities\n",
    "        self.k = k\n",
    "        self.pca_cov()\n",
    "        self.pca_eig()\n",
    "        self.pca_scores()\n",
    "        self.pca_backtrans()\n",
    "\n",
    "    def pca_cov(self):\n",
    "        self.cov_matr = np.array(self.spot).T\n",
    "        self.cov_matr = np.cov(self.cov_matr, bias=True)\n",
    "        self.cov_matr = pd.DataFrame(data=self.cov_matr, columns=self.mat, index=self.mat)\n",
    "\n",
    "    def pca_eig(self):\n",
    "        eig = np.linalg.eig(self.cov_matr)\n",
    "        self.idx = list([\"PC_\" + str(i) for i in range(1, eig[0].shape[0] + 1)])\n",
    "        self.eig_vals = pd.DataFrame(eig[0].real, columns=[\"eig_val\"], index=self.idx)\n",
    "        self.eig_vals[\"eig_val_rel\"] = self.eig_vals[\"eig_val\"].apply(lambda x: x / self.eig_vals[\"eig_val\"].sum())\n",
    "        self.eig_vals[\"eig_val_abs\"] = self.eig_vals[\"eig_val_rel\"].cumsum()\n",
    "        self.eig_vect = pd.DataFrame(eig[1].real, index=self.mat, columns=self.idx)\n",
    "        self.eig_vect_k = self.eig_vect.iloc[:, :self.k]\n",
    "\n",
    "    def pca_scores(self):\n",
    "        self.eig_scores = np.matrix(self.spot) * np.matrix(self.eig_vect)\n",
    "        self.eig_scores = pd.DataFrame(data=self.eig_scores, columns=self.idx, index=pd.to_datetime(self.spot.index))\n",
    "        self.eig_scores_k = self.eig_scores.iloc[:, :self.k]\n",
    "\n",
    "    def pca_backtrans(self):\n",
    "        self.eig_vect_inv = pd.DataFrame(data=np.linalg.inv(np.matrix(self.eig_vect)), columns=self.mat, index=self.idx)\n",
    "        self.eig_vect_inv_k = self.eig_vect_inv.iloc[:self.k, :]\n",
    "        self.yields = np.matrix(self.eig_scores_k) * np.matrix(self.eig_vect_inv_k)\n",
    "        self.yields = pd.DataFrame(data=self.yields, columns=self.mat, index=self.eig_scores_k.index)\n",
    "\n",
    "    def pca_oos(self, eig_vect_train, spot_test):\n",
    "        eig_scores_oos = np.matrix(spot_test) * np.matrix(eig_vect_train)\n",
    "        eig_scores_oos = pd.DataFrame(data=eig_scores_oos, columns=self.idx, index=pd.to_datetime(spot_test.index))\n",
    "        eig_scores_oos_k = eig_scores_oos.iloc[:, :self.k]\n",
    "        eig_vect_inv_oos = pd.DataFrame(data=np.linalg.inv(np.matrix(eig_vect_train)), columns=self.mat, index=self.idx)\n",
    "        eig_vect_inv_oos_k = eig_vect_inv_oos.iloc[:self.k, :]\n",
    "        yields_oos = np.matrix(eig_scores_oos_k) * np.matrix(eig_vect_inv_oos_k)\n",
    "        yields_oos = pd.DataFrame(data=yields_oos, columns=self.mat, index=eig_scores_oos_k.index)\n",
    "        return yields_oos\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Step 1: Resample to daily and sum up hourly returns\n",
    "    daily_returns = first_diff_df_cleaned.resample('D').sum()\n",
    "\n",
    "    # Step 2: Convert daily returns to weekly returns\n",
    "    weekly_returns = daily_returns.resample('W').sum()\n",
    "\n",
    "    # Convert to absolute returns\n",
    "    weekly_returns = weekly_returns.abs()\n",
    "\n",
    "    data = weekly_returns.values\n",
    "\n",
    "    # PPCA\n",
    "    ppca = PPCA(latent_dim=3)\n",
    "    ppca.fit(data)\n",
    "    print(ppca)\n",
    "    top_k_eigenvectors_ppca = ppca.get_top_eigenvectors(3)\n",
    "    print(\"Top 3 eigenvectors from PPCA:\\n\", top_k_eigenvectors_ppca)\n",
    "\n",
    "    # PCA using scikit-learn\n",
    "    pca_sklearn = SklearnPCA(n_components=3)\n",
    "    pca_sklearn.fit(data)\n",
    "    top_k_eigenvectors_pca_sklearn = pca_sklearn.components_.T\n",
    "    print(\"Top 3 eigenvectors from PCA (scikit-learn):\\n\", top_k_eigenvectors_pca_sklearn)\n",
    "\n",
    "    # Custom PCA\n",
    "    custom_pca = CustomPCA(weekly_returns, weekly_returns.columns.tolist(), 3)\n",
    "    top_k_eigenvectors_custom_pca = custom_pca.eig_vect_k\n",
    "    print(\"Top 3 eigenvectors from Custom PCA:\\n\", top_k_eigenvectors_custom_pca)\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    features = weekly_returns.columns\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.plot(features, top_k_eigenvectors_ppca[:, i], label=f'PPCA PC_{i+1}', linestyle='--', linewidth=2)\n",
    "        # Multiply the third principal component by -1 for the scikit-learn PCA\n",
    "        if i == 1:\n",
    "            plt.plot(features, top_k_eigenvectors_pca_sklearn[:, i], label=f'PCA (scikit-learn) PC_{i+1}', linewidth=2)\n",
    "        else:\n",
    "            plt.plot(features, top_k_eigenvectors_pca_sklearn[:, i], label=f'PCA (scikit-learn) PC_{i+1}', linewidth=2)\n",
    "        plt.plot(features, top_k_eigenvectors_custom_pca.iloc[:, i], label=f'Custom PCA PC_{i+1}', linestyle=':', linewidth=2)\n",
    "\n",
    "    plt.title('Comparison of Principal Components from PPCA, PCA (scikit-learn), and Custom PCA')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Component Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Components\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PPCA:\n",
    "    \"\"\"\n",
    "    Probabilistic PCA Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        self.latent_dim = latent_dim\n",
    "        self.x = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "        self.mu_mle_ = None\n",
    "        self.sample_variance_ = None\n",
    "\n",
    "    def fit(self, x: np.ndarray):\n",
    "        self.x = x\n",
    "        self.n_samples_ = x.shape[0]\n",
    "        self.n_features_ = x.shape[1]\n",
    "        self.mu_mle_ = self.x.mean(axis=0)\n",
    "        self.sample_variance_ = self.sample_variance()\n",
    "        return self\n",
    "\n",
    "    def sample_variance(self) -> np.ndarray:\n",
    "        S = (1 / self.n_samples_) * (self.x - self.mu_mle_).T @ (self.x - self.mu_mle_)\n",
    "        assert S.shape == (self.n_features_, self.n_features_)\n",
    "        return S\n",
    "\n",
    "class CustomPCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spot, maturities):\n",
    "        self.spot = spot\n",
    "        self.mat = maturities\n",
    "        self.pca_cov()\n",
    "\n",
    "    def pca_cov(self):\n",
    "        self.cov_matr = np.array(self.spot).T\n",
    "        self.cov_matr = np.cov(self.cov_matr, bias=True)\n",
    "        self.cov_matr = pd.DataFrame(data=self.cov_matr, columns=self.mat, index=self.mat)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    data = weekly_returns\n",
    "\n",
    "    # PPCA\n",
    "    ppca = PPCA(latent_dim=3)\n",
    "    ppca.fit(data)\n",
    "    ppca_cov_matrix = ppca.sample_variance()\n",
    "    print(\"Covariance Matrix from PPCA:\\n\", ppca_cov_matrix)\n",
    "\n",
    "    # Custom PCA\n",
    "    custom_pca = CustomPCA(weekly_returns, weekly_returns.columns.tolist())\n",
    "    pca_cov_matrix = custom_pca.cov_matr\n",
    "    print(\"Covariance Matrix from Custom PCA:\\n\", pca_cov_matrix)\n",
    "\n",
    "    # Check if the covariance matrices are the same\n",
    "    are_cov_matrices_equal = np.allclose(ppca_cov_matrix, pca_cov_matrix)\n",
    "    print(\"Are the covariance matrices equal? \", are_cov_matrices_equal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Tom-900/Probabilistic-Principal-Component-Analysis/blob/main/PPCA%20.ipynb\n",
    "https://github.com/davidstutz/probabilistic-pca\n",
    "https://github.com/cangermueller/ppca/tree/master/notebooks\n",
    "https://medium.com/practical-coding/the-simplest-generative-model-you-probably-missed-c840d68b704\n",
    "https://github.com/smrfeld/python_prob_pca_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Tom-900/Probabilistic-Principal-Component-Analysis/blob/main/code_python/PPCA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def PPCA(t, q, epsilon=0.001, method='ML'):\n",
    "    \"\"\"\n",
    "    Perform Probabilistic Principal Component Analysis (PPCA).\n",
    "\n",
    "    Parameters:\n",
    "    t (numpy.ndarray): Input data matrix with shape (d, N).\n",
    "    q (int): Number of principal components.\n",
    "    epsilon (float): Convergence threshold for EM method.\n",
    "    method (str): 'ML' for Maximum Likelihood, 'EM' for Expectation-Maximization.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The latent variables X.\n",
    "    \"\"\"\n",
    "    d, N = t.shape\n",
    "\n",
    "    # Step 1: Data centering\n",
    "    Mu = np.mean(t, axis=1, keepdims=True)\n",
    "    S = np.cov(t)\n",
    "\n",
    "    if method == 'ML':\n",
    "        # Step 2: Eigen decomposition\n",
    "        value, U = np.linalg.eigh(S)\n",
    "        idx = np.argsort(value)[::-1]\n",
    "        value = value[idx]\n",
    "        U = U[:, idx]\n",
    "\n",
    "        # Step 3: Select top q components\n",
    "        U = U[:, :q]\n",
    "        sigma_square = np.sum(value[q:]) / (d - q)\n",
    "        Lambda = np.diag(value[:q])\n",
    "        W = U @ np.sqrt(Lambda - sigma_square * np.eye(q))\n",
    "\n",
    "        # Step 4: Calculate latent variables\n",
    "        M = W.T @ W + sigma_square * np.eye(q)\n",
    "        X = np.linalg.inv(M) @ W.T @ (t - Mu)\n",
    "\n",
    "    elif method == 'EM':\n",
    "        value, U = np.linalg.eigh(S)\n",
    "        idx = np.argsort(value)[::-1]\n",
    "        value = value[idx]\n",
    "        U = U[:, idx]\n",
    "\n",
    "        U = U[:, :q]\n",
    "        Lambda = np.diag(value[:q])\n",
    "\n",
    "        W_new = U @ np.sqrt(Lambda)\n",
    "        sigma_square_new = 1\n",
    "        sigma_square_old = 5\n",
    "\n",
    "        while (np.linalg.norm(W_new - W_new, 'fro') > epsilon) or (abs(sigma_square_new - sigma_square_old) > epsilon):\n",
    "            M = W_new.T @ W_new + sigma_square_new * np.eye(q)\n",
    "            W_old = W_new.copy()\n",
    "            sigma_square_old = sigma_square_new\n",
    "\n",
    "            W_new = S @ W_old @ np.linalg.inv(sigma_square_old * np.eye(q) + np.linalg.inv(M) @ W_old.T @ S @ W_old)\n",
    "            sigma_square_new = np.trace(S - S @ W_old @ np.linalg.inv(M) @ W_new.T) / d\n",
    "\n",
    "            # Debug print for log-likelihood\n",
    "            C = W_new @ W_new.T + sigma_square_new * np.eye(d)\n",
    "            L = -N * (d * np.log(np.pi) + np.log(np.linalg.det(C)) + np.trace(np.linalg.inv(C) @ S)) / 2\n",
    "            print('updated log-likelihood: ', L)\n",
    "\n",
    "        M = W_new.T @ W_new + sigma_square_new * np.eye(q)\n",
    "        X = np.linalg.inv(M) @ W_new.T @ (t - Mu)\n",
    "\n",
    "    return X\n",
    "\n",
    "# Example usage:\n",
    "# t = np.random.randn(10, 100)  # Example data\n",
    "# q = 2  # Number of principal components\n",
    "# X = PPCA(t, q, method='EM')\n",
    "# print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = first_diff_df_cleaned\n",
    "\n",
    "# Convert DataFrame to NumPy array and transpose\n",
    "t = df.to_numpy().T\n",
    "\n",
    "# Number of principal components\n",
    "q = 5  # Example, you can choose any number of components less than 17\n",
    "\n",
    "# Call the PPCA function\n",
    "X = PPCA(t, q, method='EM')\n",
    "\n",
    "# Print the resulting latent variables\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def robust(t, q, epsilon=0.001, method='ML'):\n",
    "    \"\"\"\n",
    "    Perform Probabilistic Principal Component Analysis (PPCA).\n",
    "\n",
    "    Parameters:\n",
    "    t (numpy.ndarray): Input data matrix with shape (d, N).\n",
    "    q (int): Number of principal components.\n",
    "    epsilon (float): Convergence threshold for EM method.\n",
    "    method (str): 'ML' for Maximum Likelihood, 'EM' for Expectation-Maximization.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The latent variables X.\n",
    "    numpy.ndarray: The top q eigenvectors.\n",
    "    \"\"\"\n",
    "    d, N = t.shape\n",
    "\n",
    "    # Step 1: Data centering\n",
    "    Mu = np.mean(t, axis=1, keepdims=True)\n",
    "    S = np.cov(t)\n",
    "\n",
    "    if method == 'ML':\n",
    "        # Step 2: Eigen decomposition\n",
    "        value, U = np.linalg.eigh(S)\n",
    "        idx = np.argsort(value)[::-1]\n",
    "        value = value[idx]\n",
    "        U = U[:, idx]\n",
    "\n",
    "        # Step 3: Select top q components\n",
    "        U = U[:, :q]\n",
    "        sigma_square = np.sum(value[q:]) / (d - q)\n",
    "        Lambda = np.diag(value[:q])\n",
    "        W = U @ np.sqrt(Lambda - sigma_square * np.eye(q))\n",
    "\n",
    "        # Step 4: Calculate latent variables\n",
    "        M = W.T @ W + sigma_square * np.eye(q)\n",
    "        X = np.linalg.inv(M) @ W.T @ (t - Mu)\n",
    "\n",
    "    elif method == 'EM':\n",
    "        value, U = np.linalg.eigh(S)\n",
    "        idx = np.argsort(value)[::-1]\n",
    "        value = value[idx]\n",
    "        U = U[:, idx]\n",
    "\n",
    "        U = U[:, :q]\n",
    "        Lambda = np.diag(value[:q])\n",
    "\n",
    "        W_new = U @ np.sqrt(Lambda)\n",
    "        sigma_square_new = 1\n",
    "        sigma_square_old = 5\n",
    "\n",
    "        while (np.linalg.norm(W_new - W_new, 'fro') > epsilon) or (abs(sigma_square_new - sigma_square_old) > epsilon):\n",
    "            M = W_new.T @ W_new + sigma_square_new * np.eye(q)\n",
    "            W_old = W_new.copy()\n",
    "            sigma_square_old = sigma_square_new\n",
    "\n",
    "            W_new = S @ W_old @ np.linalg.inv(sigma_square_old * np.eye(q) + np.linalg.inv(M) @ W_old.T @ S @ W_old)\n",
    "            sigma_square_new = np.trace(S - S @ W_old @ np.linalg.inv(M) @ W_new.T) / d\n",
    "\n",
    "            # Debug print for log-likelihood\n",
    "            C = W_new @ W_new.T + sigma_square_new * np.eye(d)\n",
    "            L = -N * (d * np.log(np.pi) + np.log(np.linalg.det(C)) + np.trace(np.linalg.inv(C) @ S)) / 2\n",
    "            print('updated log-likelihood: ', L)\n",
    "\n",
    "        M = W_new.T @ W_new + sigma_square_new * np.eye(q)\n",
    "        X = np.linalg.inv(M) @ W_new.T @ (t - Mu)\n",
    "\n",
    "    # Return latent variables and top q eigenvectors\n",
    "    return X, U\n",
    "\n",
    "# Example usage:\n",
    "# Load your DataFrame\n",
    "# df = pd.read_csv('your_data.csv')  # Replace with actual loading method\n",
    "\n",
    "# For demonstration, let's create a random DataFrame\n",
    "df = first_diff_df_cleaned\n",
    "\n",
    "# Convert DataFrame to NumPy array and transpose\n",
    "t = df.to_numpy().T\n",
    "\n",
    "# Number of principal components\n",
    "q = 3  # We want the top 3 eigenvectors\n",
    "\n",
    "# Call the PPCA function\n",
    "X, top_eigenvectors = robust(t, q, method='ML')\n",
    "\n",
    "# Print the resulting latent variables and top 3 eigenvectors\n",
    "print(\"Latent variables:\\n\", X)\n",
    "print(\"Top 3 eigenvectors:\\n\", top_eigenvectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned = pd.read_csv(r'H:\\Excel\\first_diff_df_cleaned.csv', index_col=0, parse_dates=True)\n",
    "first_diff_df_cleaned.shape\n",
    "first_diff_df_cleaned = first_diff_df_cleaned.iloc[:, :-2]\n",
    "first_diff_df_cleaned\n",
    "\n",
    "\n",
    "# Define the path to the Parquet file using forward slashes\n",
    "file_path = 'C:/Users/srajan/Downloads/ES_future.parquet'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Ensure the data is sorted by date if it's not already\n",
    "df = df.sort_index()\n",
    "\n",
    "# Calculate returns\n",
    "returns_df = df.pct_change().dropna()\n",
    "\n",
    "# Display the first few rows of the returns DataFrame\n",
    "print(returns_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the Parquet file using forward slashes\n",
    "file_path = 'C:/Users/srajan/Downloads/ES_future.parquet'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Ensure the data is sorted by date if it's not already\n",
    "df = df.sort_index()\n",
    "\n",
    "# Calculate returns\n",
    "returns_df = df.pct_change().dropna()\n",
    "\n",
    "# Display the first few rows of the returns DataFrame\n",
    "print(returns_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = returns_df.join(first_diff_df_cleaned, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the indices present in returns_df but not in combined_df\n",
    "indices_in_returns_not_in_combined = returns_df.index.difference(combined_df.index)\n",
    "\n",
    "# Identify the indices present in first_diff_df_cleaned but not in combined_df\n",
    "indices_in_first_diff_not_in_combined = first_diff_df_cleaned.index.difference(combined_df.index)\n",
    "\n",
    "# Print the indices that are in one dataframe but not in the other\n",
    "print(\"Indices in returns_df but not in combined_df:\")\n",
    "print(indices_in_returns_not_in_combined)\n",
    "\n",
    "print(\"\\nIndices in first_diff_df_cleaned but not in combined_df:\")\n",
    "print(indices_in_first_diff_not_in_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IOHMM import UnSupervisedIOHMM, OLS, CrossEntropyMNL\n",
    "\n",
    "# Assuming combined_df is already provided\n",
    "covariate = 'Mid'\n",
    "target_columns = ['yield_1Y', 'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y',\n",
    "                  'yield_6Y', 'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y',\n",
    "                  'yield_12Y', 'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y',\n",
    "                  'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "combined_df[target_columns] = scaler.fit_transform(combined_df[target_columns])\n",
    "combined_df[covariate] = scaler.fit_transform(combined_df[[covariate]])\n",
    "\n",
    "# Placeholder for IOHMM import (simulated as we cannot run it)\n",
    "# from IOHMM import UnSupervisedIOHMM, OLS, CrossEntropyMNL\n",
    "\n",
    "# Initialize the model with two hidden states\n",
    "SHMM = UnSupervisedIOHMM(num_states=2, max_EM_iter=200, EM_tol=1e-6)\n",
    "\n",
    "# Set models for emissions, transitions, and initial probabilities\n",
    "SHMM.set_models(\n",
    "    model_emissions=[OLS() for _ in target_columns],\n",
    "    model_transition=CrossEntropyMNL(solver='lbfgs'),\n",
    "    model_initial=CrossEntropyMNL(solver='lbfgs')\n",
    ")\n",
    "\n",
    "# Specify inputs\n",
    "SHMM.set_inputs(\n",
    "    covariates_initial=[covariate],\n",
    "    covariates_transition=[covariate],\n",
    "    covariates_emissions=[[covariate] for _ in target_columns]\n",
    ")\n",
    "\n",
    "# Define the output target columns\n",
    "SHMM.set_outputs([target_columns])\n",
    "\n",
    "# Set the data for the model\n",
    "SHMM.set_data([combined_df])\n",
    "\n",
    "# Simulating training (will not execute without actual library and method definitions)\n",
    "try:\n",
    "    SHMM.train()\n",
    "    print(\"Model training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed during training:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Print the coefficients for the OLS model for each hidden state\n",
    "    print(\"Coefficients for State 0:\", SHMM.model_emissions[0][0].coef)\n",
    "    print(\"Coefficients for State 1:\", SHMM.model_emissions[1][0].coef)\n",
    "    \n",
    "    # Print the scale/dispersion for the OLS model for each hidden state\n",
    "    print(\"Scale/Dispersion for State 0:\", np.sqrt(SHMM.model_emissions[0][0].dispersion))\n",
    "    print(\"Scale/Dispersion for State 1:\", np.sqrt(SHMM.model_emissions[1][0].dispersion))\n",
    "    \n",
    "    # Try to get transition probabilities between hidden states\n",
    "    empty_input = np.array([[]])\n",
    "    if hasattr(SHMM.model_transition, 'predict_proba'):\n",
    "        print(\"Transition Probabilities from State 0:\", SHMM.model_transition.predict_proba(empty_input))\n",
    "    else:\n",
    "        print(\"No predict_proba method available. Check the correct method for transition probabilities.\")\n",
    "except Exception as e:\n",
    "    print(\"Error in extracting or printing model parameters:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHMM.model_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHMM.log_gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Verify that log_gammas is populated and contains the expected number of elements\n",
    "if hasattr(SHMM, 'log_gammas') and isinstance(SHMM.log_gammas, list) and len(SHMM.log_gammas) > 0:\n",
    "    state_sequences = []\n",
    "    num_sequences = len(SHMM.log_gammas)  # Use the actual number of sequences\n",
    "    for i in range(num_sequences):\n",
    "        sequence_length = len(SHMM.log_gammas[i])\n",
    "        for j in range(sequence_length):\n",
    "            state_sequences.append(np.argmax(np.exp(SHMM.log_gammas[i][j])))\n",
    "\n",
    "    # Assuming df has a 'unit' field that correlates to these sequences\n",
    "    pred_state_seq = []\n",
    "    if 'unit' in df.columns:\n",
    "        for i in range(1, df['unit'].max() + 1):\n",
    "            if df[df['unit'] == i].empty:\n",
    "                continue\n",
    "            first_idx = df[df['unit'] == i].index[0]\n",
    "            last_idx = df[df['unit'] == i].index[-1] + 1\n",
    "            pred_state_seq.append(state_sequences[first_idx:last_idx])\n",
    "    else:\n",
    "        print(\"DataFrame 'df' does not contain 'unit' field.\")\n",
    "    \n",
    "    # Output the predicted state sequences for inspection\n",
    "    print(pred_state_seq)\n",
    "else:\n",
    "    print(\"SHMM.log_gammas is not properly initialized or empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/56641267/input-output-hidden-markov-model-implementation-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sequences = []\n",
    "for gamma in SHMM.log_gammas:\n",
    "    # Calculate the most likely state for each timestep in each sequence\n",
    "    states = np.argmax(gamma, axis=1)\n",
    "    state_sequences.append(states)\n",
    "\n",
    "# Print the state sequences\n",
    "for seq in state_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = sum(len(seq) for seq in state_sequences)\n",
    "print(f\"Total number of timesteps across all sequences: {total_timesteps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming state_sequences is already filled with the decoded state sequences\n",
    "# Flatten the list of sequences\n",
    "flat_state_sequences = [state for sequence in state_sequences for state in sequence]\n",
    "\n",
    "# Check if the length of the flat list matches the number of rows in combined_df\n",
    "if len(flat_state_sequences) == len(combined_df):\n",
    "    combined_df['decoded_states'] = flat_state_sequences\n",
    "    print(\"State sequences appended successfully to DataFrame.\")\n",
    "else:\n",
    "    print(\"Mismatch in the number of data points and the number of decoded states.\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'decoded_states' column\n",
    "counts = combined_df['decoded_states'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume 'combined_df' is already defined and includes the 'decoded_states' column\n",
    "# combined_df = pd.read_csv('path_to_combined_df.csv')  # Placeholder for actual data loading\n",
    "\n",
    "# Drop any rows with NaN values\n",
    "first_diff_df_cleaned = combined_df.dropna()\n",
    "\n",
    "# Define start_date and end_date\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame based on the specified start and end dates\n",
    "filtered_df = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Separate the DataFrame into two regimes based on the 'decoded_states' column\n",
    "regime_0_df = filtered_df[filtered_df['decoded_states'] == 0].copy()\n",
    "regime_1_df = filtered_df[filtered_df['decoded_states'] == 1].copy()\n",
    "\n",
    "# Function to perform PCA and plot results\n",
    "def perform_and_plot_pca(df, regime_number):\n",
    "    if df.empty:\n",
    "        print(f\"No data available for regime {regime_number}\")\n",
    "        return\n",
    "\n",
    "    # Remove the first and last columns\n",
    "    numeric_df = df.iloc[:, 1:-1].select_dtypes(include=[np.number])\n",
    "    column_names = numeric_df.columns  # Save column names for x-axis labels\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # We use 3 components for illustrative purposes\n",
    "    principalComponents = pca.fit_transform(numeric_df)\n",
    "    eigenvectors = pca.components_\n",
    "\n",
    "    # Retrieve the explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    # Plotting the principal components\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(3):\n",
    "        plt.plot(column_names, eigenvectors[i], label=f'PC {i+1} ({explained_variance[i]:.2%} variance)', linestyle='--', linewidth=2)\n",
    "    \n",
    "    plt.title(f'PCA Eigenvector Loadings for Regime {regime_number}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Component Values')\n",
    "    plt.xticks(rotation=45)  # Rotate column names for better visibility\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Components\")\n",
    "    plt.tight_layout()  # Adjust layout to make room for label rotation\n",
    "    plt.show()\n",
    "\n",
    "    # Print top 3 eigenvectors\n",
    "    print(f\"Top 3 Eigenvectors (Regime {regime_number}):\")\n",
    "    for i in range(3):\n",
    "        print(f\"PC{i+1}: {eigenvectors[i]}\")\n",
    "    print(f\"PCA Explained Variance (Regime {regime_number}): {explained_variance}\")\n",
    "\n",
    "# Apply PCA to each regime\n",
    "perform_and_plot_pca(regime_0_df, 0)\n",
    "perform_and_plot_pca(regime_1_df, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian HMM (From old code )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_df_cleaned = pd.read_csv(r'H:\\Excel\\first_diff_df_cleaned.csv', index_col=0, parse_dates=True)\n",
    "first_diff_df_cleaned.shape\n",
    "first_diff_df_cleaned = first_diff_df_cleaned.iloc[:, :-2]\n",
    "first_diff_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Assume 'first_diff_df_cleaned' is already defined and loaded\n",
    "# first_diff_df_cleaned = pd.read_csv('path_to_first_diff_df_cleaned.csv')  # Placeholder for actual data loading\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df_HMM = first_diff_df_cleaned.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y'\n",
    "]\n",
    "\n",
    "# Number of hidden states for the HMM\n",
    "n_hidden_states = 2  # Adjustable depending on desired complexity of the model\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df_HMM[tenors].dropna().values\n",
    "\n",
    "# Initialize and fit the HMM with a specified random state for reproducibility\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "\n",
    "# Set uniform initial probabilities and transition probabilities as discussed\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Predict regimes based on the fitted model\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Add regime information back to the DataFrame\n",
    "filtered_df_HMM = filtered_df_HMM.loc[filtered_df_HMM[tenors].dropna().index]\n",
    "filtered_df_HMM['regime'] = regimes\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df_HMM[filtered_df_HMM['regime'] == i].copy()\n",
    "\n",
    "# Function to generate colors\n",
    "def rainbow(categories):\n",
    "    c_scale = cm.rainbow(np.linspace(0, 1, len(categories)))\n",
    "    c_dict = {}\n",
    "    for i, c in zip(categories, c_scale):\n",
    "        c_dict[i] = c\n",
    "    return c_dict\n",
    "\n",
    "# Prepare subplots for all regimes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(25, 11))  # Adjust the grid size based on the number of regimes\n",
    "axes = axes.flatten()  # Flatten to iterate easily if you have more than two axes\n",
    "\n",
    "# Perform PCA for each regime\n",
    "for idx, (regime, df) in enumerate(regime_dfs.items()):\n",
    "    # Ensure that the DataFrame only has the tenors columns\n",
    "    df = df[tenors].dropna()\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # Adjust components as necessary\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    \n",
    "    # Get eigenvectors for the top components\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plotting each component's loading per maturity\n",
    "    color = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "    for i in range(pca.n_components_):\n",
    "        axes[idx].plot(tenors, eigenvectors[i], color=color[i], label=f'PC_{i+1}', linewidth=2)\n",
    "    \n",
    "    # Formatting each subplot\n",
    "    axes[idx].set_title(f'Eigenvector Loadings for {regime}')\n",
    "    axes[idx].set_xlabel('Maturities')\n",
    "    axes[idx].legend(title=\"Components\")\n",
    "    \n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare a single plot for comparison between Regime 0 and Regime 1\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Regimes to compare\n",
    "regimes_to_plot = ['regime_0', 'regime_1']\n",
    "\n",
    "# Set up a color map and style cycle\n",
    "color_map = plt.get_cmap('tab10')  # Get a color map from matplotlib\n",
    "styles = ['-', '--']  # Line styles for differentiation\n",
    "\n",
    "# Perform PCA and plot for each selected regime\n",
    "for idx, regime in enumerate(regimes_to_plot):\n",
    "    df = regime_dfs[regime][tenors].dropna()  # Ensure DataFrame has the right columns and no NaNs\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)  # We use 3 components\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plot each component's loading per maturity\n",
    "    for i in range(pca.n_components_):\n",
    "        ax.plot(tenors, eigenvectors[i], color=color_map(idx * 3 + i), linestyle=styles[idx % len(styles)],\n",
    "                label=f'{regime} PC_{i+1}', linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('Comparison of PCA Eigenvector Loadings between Regime 0 and Regime 1')\n",
    "ax.set_xlabel('Maturities')\n",
    "ax.legend(title=\"Regimes and Components\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the 'decoded_states' column from filtered_df to filtered_df_HMM\n",
    "filtered_df_HMM = pd.concat([filtered_df_HMM, filtered_df['decoded_states']], axis=1)\n",
    "\n",
    "# Verify the concatenation\n",
    "print(filtered_df_HMM.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y'\n",
    "]\n",
    "\n",
    "# Assuming 'filtered_df' has 'decoded_states' and the HMM regimes are in 'regime'\n",
    "filtered_df_HMM = filtered_df_HMM.dropna(subset=tenors)\n",
    "filtered_df_HMM['regime'] = filtered_df_HMM['regime']  # Ensure regime column is correct\n",
    "filtered_df_HMM['decoded_states'] = filtered_df['decoded_states']\n",
    "\n",
    "# Function to perform PCA and plot results\n",
    "def perform_pca_and_plot(df, title, ax):\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(df[tenors])\n",
    "    eigenvectors = pca.components_\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "    for i, color in enumerate(colors):\n",
    "        ax.plot(tenors, eigenvectors[i], color=color, label=f'PC_{i+1}', linewidth=2)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Maturities')\n",
    "    ax.legend(title=\"Components\")\n",
    "\n",
    "# Prepare subplots for comparison\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n",
    "\n",
    "# Perform PCA for each regime\n",
    "for idx, regime in enumerate(filtered_df_HMM['regime'].unique()):\n",
    "    df_regime = filtered_df_HMM[filtered_df_HMM['regime'] == regime]\n",
    "    perform_pca_and_plot(df_regime, f'Eigenvector Loadings for Gaussian HMM Regime {regime}', axes[0, idx])\n",
    "\n",
    "# Perform PCA for each decoded state\n",
    "for idx, state in enumerate(filtered_df_HMM['decoded_states'].unique()):\n",
    "    df_state = filtered_df_HMM[filtered_df_HMM['decoded_states'] == state]\n",
    "    perform_pca_and_plot(df_state, f'Eigenvector Loadings for IOHMM Decoded State {state}', axes[1, idx])\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y'\n",
    "]\n",
    "\n",
    "# Assuming 'filtered_df' has 'decoded_states' and the HMM regimes are in 'regime'\n",
    "filtered_df_HMM = filtered_df_HMM.dropna(subset=tenors)\n",
    "filtered_df_HMM['regime'] = filtered_df_HMM['regime']  # Ensure regime column is correct\n",
    "filtered_df_HMM['decoded_states'] = filtered_df['decoded_states']\n",
    "\n",
    "# Function to perform PCA and plot results\n",
    "def perform_pca_and_plot(df, title, ax):\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(df[tenors])\n",
    "    eigenvectors = pca.components_\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "    for i, color in enumerate(colors):\n",
    "        ax.plot(tenors, eigenvectors[i], color=color, label=f'PC_{i+1}', linewidth=2)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Maturities')\n",
    "    ax.legend(title=\"Components\")\n",
    "\n",
    "# Prepare subplots for comparison\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 15))\n",
    "\n",
    "# Perform PCA for each regime\n",
    "for idx, regime in enumerate(filtered_df_HMM['regime'].unique()):\n",
    "    df_regime = filtered_df_HMM[filtered_df_HMM['regime'] == regime]\n",
    "    perform_pca_and_plot(df_regime, f'Eigenvector Loadings for Gaussian HMM Regime {regime}', axes[0, idx])\n",
    "\n",
    "# Perform PCA for each decoded state\n",
    "for idx, state in enumerate(filtered_df_HMM['decoded_states'].unique()):\n",
    "    df_state = filtered_df_HMM[filtered_df_HMM['decoded_states'] == state]\n",
    "    perform_pca_and_plot(df_state, f'Eigenvector Loadings for IOHMM Decoded State {state}', axes[1, idx])\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y'\n",
    "]\n",
    "\n",
    "# Assuming 'filtered_df' has 'decoded_states' and the HMM regimes are in 'regime'\n",
    "filtered_df_HMM = filtered_df_HMM.dropna(subset=tenors)\n",
    "filtered_df_HMM['regime'] = filtered_df_HMM['regime']  # Ensure regime column is correct\n",
    "filtered_df_HMM['decoded_states'] = filtered_df['decoded_states']\n",
    "\n",
    "# Function to perform PCA and plot results\n",
    "def perform_pca_and_print(df, title):\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(df[tenors])\n",
    "    eigenvectors = pca.components_\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    print(f\"\\n{title}\")\n",
    "    pca_df = pd.DataFrame(eigenvectors.T, columns=[f'PC_{i+1}' for i in range(eigenvectors.shape[0])], index=tenors)\n",
    "    print(pca_df)\n",
    "    print(f\"Explained Variance Ratio: {explained_variance}\")\n",
    "\n",
    "# Perform PCA for each regime and print results\n",
    "for idx, regime in enumerate(filtered_df_HMM['regime'].unique()):\n",
    "    df_regime = filtered_df_HMM[filtered_df_HMM['regime'] == regime]\n",
    "    perform_pca_and_print(df_regime, f'PCA Eigenvector Loadings for Gaussian HMM Regime {regime}')\n",
    "\n",
    "# Perform PCA for each decoded state and print results\n",
    "for idx, state in enumerate(filtered_df_HMM['decoded_states'].unique()):\n",
    "    df_state = filtered_df_HMM[filtered_df_HMM['decoded_states'] == state]\n",
    "    perform_pca_and_print(df_state, f'PCA Eigenvector Loadings for IOHMM Decoded State {state}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = r'H:\\Excelcombined_df_1.csv'\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "combined_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "# Assuming 'first_diff_df_cleaned' is already defined and loaded\n",
    "# first_diff_df_cleaned = pd.read_csv('path_to_file.csv')\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('2011-09-05')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "filtered_df_HMM = first_diff_df_cleaned.loc[(first_diff_df_cleaned.index >= start_date) & (first_diff_df_cleaned.index <= end_date)]\n",
    "\n",
    "# Define tenors\n",
    "tenors = ['yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "          'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "          'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Clean data\n",
    "X = filtered_df_HMM[tenors].dropna()\n",
    "\n",
    "# Fit HMM\n",
    "n_hidden_states = 2\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "model.startprob_ = np.full(n_hidden_states, 1/n_hidden_states)\n",
    "model.transmat_ = np.full((n_hidden_states, n_hidden_states), 1/n_hidden_states)\n",
    "np.fill_diagonal(model.transmat_, 0.7)\n",
    "model.transmat_ = model.transmat_ / model.transmat_.sum(axis=1, keepdims=True)\n",
    "model.fit(X)\n",
    "\n",
    "# Add regime to DataFrame\n",
    "filtered_df_HMM['regime'] = model.predict(X)\n",
    "\n",
    "# Plot and analyze residuals for each tenor in each regime\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 16))  # Adjust subplot dimensions based on number of tenors\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    for regime in range(n_hidden_states):\n",
    "        regime_data = filtered_df_HMM[filtered_df_HMM['regime'] == regime][tenor].dropna()\n",
    "        pca = PCA(n_components=1)  # Using one component for simplicity\n",
    "        if len(regime_data) > 1:  # Ensure there's enough data to perform PCA\n",
    "            components = pca.fit_transform(regime_data.values.reshape(-1, 1))\n",
    "            reconstruction = pca.inverse_transform(components)\n",
    "            residuals = regime_data.values.reshape(-1, 1) - reconstruction\n",
    "\n",
    "            # Plot residuals\n",
    "            axes[idx].scatter(regime_data.index, residuals, label=f'Regime {regime}', alpha=0.5)\n",
    "            axes[idx].xaxis.set_major_locator(mdates.YearLocator())  # Set major ticks to one tick per year\n",
    "            axes[idx].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Format the date to show only the year\n",
    "            axes[idx].xaxis.set_minor_locator(mdates.MonthLocator())  # Optional: Add minor ticks per month\n",
    "            axes[idx].tick_params(axis='x', rotation=45)  # Rotate the labels to prevent overlap\n",
    "\n",
    "    axes[idx].set_title(f'Residuals for {tenor}')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Residual')\n",
    "    if idx == 0 or idx % 4 == 0:  # Add legend to the first subplot of each row for clarity\n",
    "        axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "# Assuming 'first_diff_df_cleaned' is already defined and loaded\n",
    "# first_diff_df_cleaned = pd.read_csv('path_to_file.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('2011-09-05')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "filtered_df_HMM = combined_df.loc[(combined_df.index >= start_date) & (combined_df.index <= end_date)]\n",
    "\n",
    "# Define tenors\n",
    "tenors = ['yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "          'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "          'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Clean data\n",
    "X = filtered_df_HMM[tenors].dropna()\n",
    "\n",
    "# Fit HMM\n",
    "n_hidden_states = 2\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, random_state=42)\n",
    "model.startprob_ = np.full(n_hidden_states, 1/n_hidden_states)\n",
    "model.transmat_ = np.full((n_hidden_states, n_hidden_states), 1/n_hidden_states)\n",
    "np.fill_diagonal(model.transmat_, 0.7)\n",
    "model.transmat_ = model.transmat_ / model.transmat_.sum(axis=1, keepdims=True)\n",
    "model.fit(X)\n",
    "\n",
    "# Add regime to DataFrame\n",
    "filtered_df_HMM['regime'] = model.predict(X)\n",
    "\n",
    "# Plot and analyze residuals for each tenor in each regime\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 16))  # Adjust subplot dimensions based on number of tenors\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, tenor in enumerate(tenors):\n",
    "    for regime in range(n_hidden_states):\n",
    "        regime_data = filtered_df_HMM[filtered_df_HMM['regime'] == regime][tenor].dropna()\n",
    "        pca = PCA(n_components=1)  # Using one component for simplicity\n",
    "        if len(regime_data) > 1:  # Ensure there's enough data to perform PCA\n",
    "            components = pca.fit_transform(regime_data.values.reshape(-1, 1))\n",
    "            reconstruction = pca.inverse_transform(components)\n",
    "            residuals = regime_data.values.reshape(-1, 1) - reconstruction\n",
    "\n",
    "            # Plot residuals\n",
    "            axes[idx].scatter(regime_data.index, residuals, label=f'Regime {regime}', alpha=0.5)\n",
    "            axes[idx].xaxis.set_major_locator(mdates.YearLocator())  # Set major ticks to one tick per year\n",
    "            axes[idx].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))  # Format the date to show only the year\n",
    "            axes[idx].xaxis.set_minor_locator(mdates.MonthLocator())  # Optional: Add minor ticks per month\n",
    "            axes[idx].tick_params(axis='x', rotation=45)  # Rotate the labels to prevent overlap\n",
    "\n",
    "    axes[idx].set_title(f'Residuals for {tenor}')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Residual')\n",
    "    if idx == 0 or idx % 4 == 0:  # Add legend to the first subplot of each row for clarity\n",
    "        axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals for individual tenors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "# Assuming 'first_diff_df_cleaned' is already defined and loaded\n",
    "# Make sure 'first_diff_df_cleaned' DataFrame has a DateTime index.\n",
    "# first_diff_df_cleaned = pd.read_csv('path_to_file.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('2011-09-05')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "filtered_df_HMM = combined_df.loc[(combined_df.index >= start_date) & (combined_df.index <= end_date)]\n",
    "\n",
    "# Define tenors\n",
    "tenors = ['yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "          'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "          'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "# Clean data\n",
    "X = filtered_df_HMM[tenors].dropna()\n",
    "\n",
    "# Principal Component Analysis to reduce dimensionality and reconstruct data\n",
    "pca = PCA(n_components=3)  # Adjust components as necessary\n",
    "components = pca.fit_transform(X)\n",
    "reconstruction = pca.inverse_transform(components)\n",
    "residuals = X - reconstruction\n",
    "\n",
    "# Prepare to plot all residuals in one plot\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# Plot residuals for each tenor\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(tenors)))  # Generate a color for each tenor\n",
    "for i, tenor in enumerate(tenors):\n",
    "    tenor_residuals = residuals.iloc[:, i]\n",
    "    ax.plot(X.index, tenor_residuals, label=f'Residuals for {tenor}', color=colors[i])\n",
    "\n",
    "# Formatting the plot\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "ax.set_title('Individual Residuals of Yield Tenors')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1,1))  # Move legend outside the plot\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the absolute residuals for each tenor\n",
    "average_absolute_residuals = residuals.abs().mean()\n",
    "\n",
    "# Rank the tenors based on the absolute average of their residuals\n",
    "ranked_tenors_by_abs = average_absolute_residuals.sort_values()\n",
    "\n",
    "# Display the ranked tenors with their average absolute residuals\n",
    "print(ranked_tenors_by_abs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix of the residuals\n",
    "residuals_correlation = residuals.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(residuals_correlation)\n",
    "\n",
    "# Optionally, you can plot the correlation matrix for better visualization\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(residuals_correlation, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Yield Tenors Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OU Process wit average sigma - Not correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "# Assuming 'residuals' is your DataFrame containing residuals for all tenors\n",
    "# residuals shape is expected to be 16346 rows × 16 columns\n",
    "\n",
    "def fit_ar1(residuals):\n",
    "    \"\"\"\n",
    "    Fits an AR(1) model to the provided residuals.\n",
    "    Returns the lag-1 coefficient and the intercept.\n",
    "    \"\"\"\n",
    "    model = AutoReg(residuals.dropna(), lags=1)\n",
    "    model_fitted = model.fit()\n",
    "    return model_fitted.params['const'], model_fitted.params[residuals.name + '.L1']\n",
    "\n",
    "def ar1_to_ou(const, lag1_coef, dt=1/9):\n",
    "    \"\"\"\n",
    "    Transforms AR(1) parameters to Ornstein-Uhlenbeck (OU) parameters.\n",
    "    dt is the time increment based on hourly tick data, set as 1/9.\n",
    "    Returns mean reversion speed (a), mean level (b), and sigma.\n",
    "    \"\"\"\n",
    "    a = -np.log(lag1_coef) / dt\n",
    "    b = const / (1 - lag1_coef)\n",
    "    sigma = np.sqrt(np.var(residuals) * 2 * a / (1 - lag1_coef**2))\n",
    "    return a, b, sigma\n",
    "\n",
    "# Create a dictionary to store the OU parameters for each tenor\n",
    "ou_parameters = {}\n",
    "sigma_values = {}  # Dictionary to store sigma values for each tenor\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ValueWarning)\n",
    "    for column in residuals.columns:\n",
    "        const, lag1_coef = fit_ar1(residuals[column])\n",
    "        a, b, sigma = ar1_to_ou(const, lag1_coef)\n",
    "        ou_parameters[column] = {'a': a, 'b': b, 'sigma': sigma}\n",
    "        # Append sigma to the list for each tenor\n",
    "        if column not in sigma_values:\n",
    "            sigma_values[column] = []\n",
    "        sigma_values[column].append(sigma)\n",
    "\n",
    "# Calculate average sigma for each tenor\n",
    "average_sigma_per_tenor = {tenor: np.mean(sigs) for tenor, sigs in sigma_values.items()}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "ou_parameters_df = pd.DataFrame(ou_parameters).T\n",
    "ou_parameters_df['average_sigma'] = ou_parameters_df.index.map(average_sigma_per_tenor)\n",
    "\n",
    "# Print the DataFrame in a tabular format\n",
    "print(ou_parameters_df[['a', 'b', 'average_sigma']].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_parameters_df.iloc[:, 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simulate_ou_process(a, b, sigma, dt=1/9, X0=0, N=16346):\n",
    "    \"\"\"\n",
    "    Simulates an Ornstein-Uhlenbeck process.\n",
    "\n",
    "    Parameters:\n",
    "    - a: rate of mean reversion\n",
    "    - b: long-term mean\n",
    "    - sigma: volatility parameter\n",
    "    - dt: time step (assumed to be 1/9 based on your dataset)\n",
    "    - X0: initial value of the process\n",
    "    - N: number of steps (default set to match the length of residuals)\n",
    "\n",
    "    Returns:\n",
    "    - X: simulated OU process values\n",
    "    \"\"\"\n",
    "    X = np.zeros(N)\n",
    "    X[0] = X0\n",
    "    \n",
    "    for i in range(1, N):\n",
    "        X[i] = X[i-1] + a * (b - X[i-1]) * dt + sigma * np.sqrt(dt) * np.random.normal()\n",
    "        \n",
    "    return X\n",
    "\n",
    "# Assuming 'ou_parameters_df' contains the previously calculated parameters a, b, and average sigma\n",
    "# Assuming 'residuals' is your DataFrame containing residuals for all tenors (16346 rows × 16 columns)\n",
    "\n",
    "# Initialize a DataFrame to store the simulated OU processes\n",
    "ou_simulated_df = pd.DataFrame(index=residuals.index)\n",
    "\n",
    "for tenor in residuals.columns:\n",
    "    a = ou_parameters_df.loc[tenor, 'a']\n",
    "    b = ou_parameters_df.loc[tenor, 'b']\n",
    "    sigma = ou_parameters_df.loc[tenor, 'average_sigma']\n",
    "    X0 = residuals[tenor].iloc[0]  # Start with the initial value of the residuals\n",
    "    \n",
    "    # Simulate the OU process\n",
    "    ou_simulated_df[tenor] = simulate_ou_process(a, b, sigma, dt=1/9, X0=X0, N=len(residuals))\n",
    "\n",
    "print(ou_simulated_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_simulated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Plotting the simulated OU processes for each tenor\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "for tenor in ou_simulated_df.columns:\n",
    "    ax.plot(ou_simulated_df.index, ou_simulated_df[tenor], label=f'Simulated OU Process: {tenor}')\n",
    "\n",
    "# Formatting the plot\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "ax.set_title('Simulated Ornstein-Uhlenbeck Processes for Different Tenors')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('OU Process Value')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))  # Move legend outside the plot\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Determine the number of rows and columns for subplots\n",
    "num_plots = len(ou_simulated_df.columns)\n",
    "nrows = int(np.ceil(num_plots / 2))  # Set 2 columns per row\n",
    "ncols = 2\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 3*nrows), sharex=True)\n",
    "\n",
    "# Flatten the axes array for easy iteration if it is multidimensional\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plotting the simulated OU processes for each tenor in its own subplot\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, num_plots))  # Generate a color for each plot\n",
    "\n",
    "for i, (tenor, color) in enumerate(zip(ou_simulated_df.columns, colors)):\n",
    "    axes[i].plot(ou_simulated_df.index, ou_simulated_df[tenor], label=f'{tenor}', color=color)\n",
    "    axes[i].set_title(f'Simulated OU Process: {tenor}')\n",
    "    axes[i].set_ylabel('OU Process Value')\n",
    "    axes[i].legend(loc='upper left')\n",
    "\n",
    "    # Formatting the date on x-axis\n",
    "    axes[i].xaxis.set_major_locator(mdates.YearLocator())\n",
    "    axes[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    axes[i].xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    plt.setp(axes[i].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Common x-axis label\n",
    "axes[-1].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit everything nicely\n",
    "plt.suptitle('Simulated Ornstein-Uhlenbeck Processes for Different Tenors', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_simulated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "# Assuming 'combined_df' and 'ou_simulated_df' are already defined and loaded\n",
    "# Ensure 'combined_df' DataFrame has a DateTime index and contains the relevant columns.\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('2011-09-05')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "filtered_df_HMM = combined_df.loc[(combined_df.index >= start_date) & (combined_df.index <= end_date)]\n",
    "\n",
    "# Define tenors\n",
    "tenors = ['yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "          'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "          'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Clean data\n",
    "X = filtered_df_HMM[tenors].dropna()\n",
    "\n",
    "# Principal Component Analysis to reduce dimensionality and reconstruct data\n",
    "pca = PCA(n_components=3)  # Adjust components as necessary\n",
    "components = pca.fit_transform(X)\n",
    "reconstruction = pca.inverse_transform(components)\n",
    "residuals = X - reconstruction\n",
    "\n",
    "# Number of subplots\n",
    "num_tenors = len(tenors)\n",
    "num_cols = 3  # Number of columns in subplot grid\n",
    "num_rows = (num_tenors + num_cols - 1) // num_cols  # Calculate number of rows needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(26, num_rows * 5), sharex=True, sharey=True)\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "# Colors for plotting\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(ou_simulated_df.columns)))  # Generate colors for each tenor in ou_simulated_df\n",
    "\n",
    "# Plot residuals and all simulated OU processes for each tenor in separate subplots\n",
    "for i, tenor in enumerate(tenors):\n",
    "    tenor_residuals = residuals[tenor]\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.plot(X.index, tenor_residuals, label=f'Residuals for {tenor}', color='blue', linewidth=2)  # Plot residuals\n",
    "    \n",
    "    # Overlay all simulated OU processes as shadows\n",
    "    for j, simulated_tenor in enumerate(ou_simulated_df.columns):\n",
    "        tenor_simulated = ou_simulated_df[simulated_tenor]\n",
    "        ax.plot(X.index, tenor_simulated, label=f'Simulated OU for {simulated_tenor}', color=colors[j], linestyle='--', alpha=0.1)\n",
    "    \n",
    "    ax.set_title(tenor)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1,1))  # Move legend outside the plot\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(num_tenors, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout to make room for the subplots and labels\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])  # Adjust rect to fit all subplots\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_parameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_parameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single residual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "# Assuming 'residuals' is your DataFrame containing residuals for all tenors\n",
    "# residuals shape is expected to be 16346 rows × 16 columns\n",
    "\n",
    "def fit_ar1(residuals):\n",
    "    \"\"\"\n",
    "    Fits an AR(1) model to the provided residuals.\n",
    "    Returns the lag-1 coefficient and the intercept.\n",
    "    \"\"\"\n",
    "    model = AutoReg(residuals.dropna(), lags=1)\n",
    "    model_fitted = model.fit()\n",
    "    return model_fitted.params['const'], model_fitted.params[residuals.name + '.L1'], model_fitted.resid\n",
    "\n",
    "def ar1_to_ou(const, lag1_coef, residuals, dt=1):\n",
    "    \"\"\"\n",
    "    Transforms AR(1) parameters to Ornstein-Uhlenbeck (OU) parameters.\n",
    "    dt is the time increment and is assumed to be 1 for hourly data.\n",
    "    Returns mean reversion speed (a), mean level (b), and sigma.\n",
    "    \"\"\"\n",
    "    a = -np.log(lag1_coef) / dt\n",
    "    b = const / (1 - lag1_coef)\n",
    "    sigma = np.sqrt(np.var(residuals) * 2 * a / (1 - lag1_coef**2))\n",
    "    return a, b, sigma\n",
    "\n",
    "# Create a dictionary to store the OU parameters for each tenor\n",
    "ou_parameters = {}\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ValueWarning)\n",
    "    for column in residuals.columns:\n",
    "        const, lag1_coef, ar1_residuals = fit_ar1(residuals[column])\n",
    "        a, b, sigma = ar1_to_ou(const, lag1_coef, ar1_residuals)\n",
    "        ou_parameters[column] = {'a': a, 'b': b, 'sigma': sigma}\n",
    "\n",
    "# Convert dictionary to DataFrame for better readability and analysis\n",
    "ou_parameters_df = pd.DataFrame(ou_parameters).T\n",
    "print(ou_parameters_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OU Simulated for every residual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simulate_ou_process(a, b, sigma, dt=1/9, X0=0, N=16346):\n",
    "    \"\"\"\n",
    "    Simulates an Ornstein-Uhlenbeck process.\n",
    "\n",
    "    Parameters:\n",
    "    - a: rate of mean reversion\n",
    "    - b: long-term mean\n",
    "    - sigma: volatility parameter\n",
    "    - dt: time step (assumed to be 1/9 based on your dataset)\n",
    "    - X0: initial value of the process\n",
    "    - N: number of steps (default set to match the length of residuals)\n",
    "\n",
    "    Returns:\n",
    "    - X: simulated OU process values\n",
    "    \"\"\"\n",
    "    X = np.zeros(N)\n",
    "    X[0] = X0\n",
    "    \n",
    "    for i in range(1, N):\n",
    "        X[i] = X[i-1] + a * (b - X[i-1]) * dt + sigma * np.sqrt(dt) * np.random.normal()\n",
    "        \n",
    "    return X\n",
    "\n",
    "# Assuming 'ou_parameters_df' contains the previously calculated parameters a, b, and average sigma\n",
    "# Assuming 'residuals' is your DataFrame containing residuals for all tenors (16346 rows × 16 columns)\n",
    "\n",
    "# Initialize a DataFrame to store the simulated OU processes\n",
    "ou_simulated_df = pd.DataFrame(index=residuals.index)\n",
    "\n",
    "for tenor in residuals.columns:\n",
    "    a = ou_parameters_df.loc[tenor, 'a']\n",
    "    b = ou_parameters_df.loc[tenor, 'b']\n",
    "    sigma = ou_parameters_df.loc[tenor, 'sigma']\n",
    "    X0 = residuals[tenor].iloc[0]  # Start with the initial value of the residuals\n",
    "    \n",
    "    # Simulate the OU process\n",
    "    ou_simulated_df[tenor] = simulate_ou_process(a, b, sigma, dt=1/9, X0=X0, N=len(residuals))\n",
    "\n",
    "print(ou_simulated_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "# Assuming 'combined_df' and 'ou_simulated_df' are already defined and loaded\n",
    "# Ensure 'combined_df' DataFrame has a DateTime index and contains the relevant columns.\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('2011-09-05')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "#start_date = pd.to_datetime('2022-02-02')\n",
    "#end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "filtered_df_HMM = combined_df.loc[(combined_df.index >= start_date) & (combined_df.index <= end_date)]\n",
    "\n",
    "# Define tenors\n",
    "tenors = ['yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "          'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "          'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Clean data\n",
    "X = filtered_df_HMM[tenors].dropna()\n",
    "\n",
    "# Principal Component Analysis to reduce dimensionality and reconstruct data\n",
    "pca = PCA(n_components=3)  # Adjust components as necessary\n",
    "components = pca.fit_transform(X)\n",
    "reconstruction = pca.inverse_transform(components)\n",
    "residuals = X - reconstruction\n",
    "\n",
    "# Number of subplots\n",
    "num_tenors = len(tenors)\n",
    "num_cols = 3  # Number of columns in subplot grid\n",
    "num_rows = (num_tenors + num_cols - 1) // num_cols  # Calculate number of rows needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(26, num_rows * 5), sharex=True, sharey=True)\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "# Colors for plotting\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(ou_simulated_df.columns)))  # Generate colors for each tenor in ou_simulated_df\n",
    "\n",
    "# Plot residuals and all simulated OU processes for each tenor in separate subplots\n",
    "for i, tenor in enumerate(tenors):\n",
    "    tenor_residuals = residuals[tenor]\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.plot(X.index, tenor_residuals, label=f'Residuals for {tenor}', color='blue', linewidth=2)  # Plot residuals\n",
    "    \n",
    "    # Overlay all simulated OU processes as shadows\n",
    "    for j, simulated_tenor in enumerate(ou_simulated_df.columns):\n",
    "        tenor_simulated = ou_simulated_df[simulated_tenor]\n",
    "        ax.plot(X.index, tenor_simulated, label=f'Simulated OU for {simulated_tenor}', color=colors[j], linestyle='--', alpha=0.1)\n",
    "    \n",
    "    ax.set_title(tenor)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1,1))  # Move legend outside the plot\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(num_tenors, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout to make room for the subplots and labels\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])  # Adjust rect to fit all subplots\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals for regime switch PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Year 2022-02-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Assume 'combined_df' is already defined and loaded\n",
    "# combined_df = pd.read_csv('path_to_combined_df.csv')  # Placeholder for actual data loading\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df_HMM = combined_df.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y'\n",
    "]\n",
    "\n",
    "# Number of hidden states for the HMM\n",
    "n_hidden_states = 2  # Adjustable depending on desired complexity of the model\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df_HMM[tenors].dropna().values\n",
    "\n",
    "# Initialize and fit the HMM with a specified random state for reproducibility\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "\n",
    "# Set uniform initial probabilities and transition probabilities as discussed\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Predict regimes based on the fitted model\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Add regime information back to the DataFrame\n",
    "filtered_df_HMM = filtered_df_HMM.loc[filtered_df_HMM[tenors].dropna().index]\n",
    "filtered_df_HMM['regime'] = regimes\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df_HMM[filtered_df_HMM['regime'] == i].copy()\n",
    "\n",
    "# Function to generate colors\n",
    "def rainbow(categories):\n",
    "    c_scale = cm.rainbow(np.linspace(0, 1, len(categories)))\n",
    "    c_dict = {}\n",
    "    for i, c in zip(categories, c_scale):\n",
    "        c_dict[i] = c\n",
    "    return c_dict\n",
    "\n",
    "# Prepare subplots for all regimes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_hidden_states, figsize=(25, 11))  # Adjust the grid size based on the number of regimes\n",
    "axes = axes.flatten()  # Flatten to iterate easily if you have more than two axes\n",
    "\n",
    "# Perform PCA for each regime\n",
    "for idx, (regime, df) in enumerate(regime_dfs.items()):\n",
    "    # Ensure that the DataFrame only has the tenors columns\n",
    "    df = df[tenors].dropna()\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    \n",
    "    # Get eigenvectors for the top components\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plotting each component's loading per maturity\n",
    "    color = cm.rainbow(np.linspace(0, 1, pca.n_components_))\n",
    "    for i in range(pca.n_components_):\n",
    "        axes[idx].plot(tenors, eigenvectors[i], color=color[i], label=f'PC_{i+1}', linewidth=2)\n",
    "    \n",
    "    # Formatting each subplot\n",
    "    axes[idx].set_title(f'Eigenvector Loadings for {regime}')\n",
    "    axes[idx].set_xlabel('Maturities')\n",
    "    axes[idx].legend(title=\"Components\")\n",
    "    \n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare a single plot for comparison between Regime 0 and Regime 1\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Regimes to compare\n",
    "regimes_to_plot = ['regime_0', 'regime_1']\n",
    "\n",
    "# Set up a color map and style cycle\n",
    "color_map = plt.get_cmap('tab10')  # Get a color map from matplotlib\n",
    "styles = ['-', '--']  # Line styles for differentiation\n",
    "\n",
    "# Perform PCA and plot for each selected regime\n",
    "for idx, regime in enumerate(regimes_to_plot):\n",
    "    df = regime_dfs[regime]\n",
    "    df = df[tenors].dropna()\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    # Plot each component's loading per maturity\n",
    "    for i in range(pca.n_components_):\n",
    "        ax.plot(tenors, eigenvectors[i], color=color_map(idx * 3 + i), linestyle=styles[idx % len(styles)],\n",
    "                label=f'{regime} PC_{i+1}', linewidth=2)\n",
    "\n",
    "# Formatting\n",
    "ax.set_title('Comparison of PCA Eigenvector Loadings between Regime 0 and Regime 1')\n",
    "ax.set_xlabel('Maturities')\n",
    "ax.legend(title=\"Regimes and Components\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute residuals inside every regime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Assume 'first_diff_df_cleaned' is already defined and loaded\n",
    "# first_diff_df_cleaned = pd.read_csv('path_to_first_diff_df_cleaned.csv')  # Placeholder for actual data loading\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df_HMM = combined_df.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y'\n",
    "]\n",
    "\n",
    "# Number of hidden states for the HMM\n",
    "n_hidden_states = 2  # Adjustable depending on desired complexity of the model\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df_HMM[tenors].dropna().values\n",
    "\n",
    "# Initialize and fit the HMM with a specified random state for reproducibility\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "\n",
    "# Set uniform initial probabilities and transition probabilities\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Predict regimes based on the fitted model\n",
    "regimes = model.predict(X)\n",
    "\n",
    "# Add regime information back to the DataFrame\n",
    "filtered_df_HMM = filtered_df_HMM.loc[filtered_df_HMM[tenors].dropna().index]\n",
    "filtered_df_HMM['regime'] = regimes\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df_HMM[filtered_df_HMM['regime'] == i].copy()\n",
    "\n",
    "# Dictionary to store residuals for each regime\n",
    "residuals_dict = {}\n",
    "\n",
    "# Perform PCA for each regime and calculate residuals\n",
    "for regime, df in regime_dfs.items():\n",
    "    df_tenors = df[tenors].dropna()\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(df_tenors)\n",
    "    reconstruction = pca.inverse_transform(principalComponents)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = df_tenors - reconstruction\n",
    "    residuals_dict[regime] = residuals\n",
    "    \n",
    "    # Plot residuals for each tenor within the regime\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, tenor in enumerate(tenors):\n",
    "        plt.plot(residuals.index, residuals.iloc[:, i], label=f'{tenor}')\n",
    "    \n",
    "    plt.title(f'Residuals for {regime}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()\n",
    "\n",
    "# Save the residuals to an Excel file\n",
    "#with pd.ExcelWriter('H:\\\\Excel\\\\regime_residuals.xlsx') as writer:\n",
    "    #for regime, residuals in residuals_dict.items():\n",
    "        #residuals.to_excel(writer, sheet_name=regime)\n",
    "\n",
    "#print(\"Residuals saved to H:\\\\Excel\\\\regime_residuals.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute OU process under each regimes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "# Assuming 'residuals_dict' is a dictionary containing DataFrames with residuals for each regime\n",
    "# Each DataFrame in residuals_dict has residuals for all tenors, with regimes as keys\n",
    "# Example structure: residuals_dict = {'regime_0': df_regime_0, 'regime_1': df_regime_1, ...}\n",
    "\n",
    "def fit_ar1(residuals):\n",
    "    \"\"\"\n",
    "    Fits an AR(1) model to the provided residuals.\n",
    "    Returns the lag-1 coefficient and the intercept.\n",
    "    \"\"\"\n",
    "    model = AutoReg(residuals.dropna(), lags=1)\n",
    "    model_fitted = model.fit()\n",
    "    return model_fitted.params['const'], model_fitted.params[residuals.name + '.L1'], model_fitted.resid\n",
    "\n",
    "def ar1_to_ou(const, lag1_coef, residuals, dt=1):\n",
    "    \"\"\"\n",
    "    Transforms AR(1) parameters to Ornstein-Uhlenbeck (OU) parameters.\n",
    "    dt is the time increment and is assumed to be 1 for hourly data.\n",
    "    Returns mean reversion speed (a), mean level (b), and sigma.\n",
    "    \"\"\"\n",
    "    a = -np.log(lag1_coef) / dt\n",
    "    b = const / (1 - lag1_coef)\n",
    "    sigma = np.sqrt(np.var(residuals) * 2 * a / (1 - lag1_coef**2))\n",
    "    return a, b, sigma\n",
    "\n",
    "# Create a dictionary to store the OU parameters for each tenor and regime\n",
    "ou_parameters = {}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ValueWarning)\n",
    "    for regime, residuals in residuals_dict.items():\n",
    "        ou_parameters[regime] = {}\n",
    "        for column in residuals.columns:\n",
    "            const, lag1_coef, ar1_residuals = fit_ar1(residuals[column])\n",
    "            a, b, sigma = ar1_to_ou(const, lag1_coef, ar1_residuals)\n",
    "            ou_parameters[regime][column] = {'a': a, 'b': b, 'sigma': sigma}\n",
    "\n",
    "# Convert dictionary to DataFrame for better readability and analysis\n",
    "ou_parameters_df = pd.concat({k: pd.DataFrame(v).T for k, v in ou_parameters.items()}, axis=0)\n",
    "print(ou_parameters_df.to_string())\n",
    "\n",
    "# Save the OU parameters to an Excel file\n",
    "ou_parameters_df.to_excel('H:\\\\Excel\\\\ou_parameters.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization 2, OU Process for each regimes for every tenor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'combined_df' and 'ou_parameters_df' are already defined and loaded\n",
    "# Ensure 'combined_df' DataFrame has a DateTime index and contains the relevant columns.\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "filtered_df_HMM = combined_df.loc[(combined_df.index >= start_date) & (combined_df.index <= end_date)]\n",
    "\n",
    "# Define tenors\n",
    "tenors = ['yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "          'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "          'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Clean data\n",
    "X = filtered_df_HMM[tenors].dropna()\n",
    "\n",
    "# Perform PCA to reduce dimensionality and reconstruct data\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X)\n",
    "reconstruction = pca.inverse_transform(components)\n",
    "residuals = X - reconstruction\n",
    "\n",
    "# Function to simulate the OU process\n",
    "def simulate_ou_process(a, b, sigma, T, dt=1):\n",
    "    N = int(T/dt)\n",
    "    ou_process = np.zeros(N)\n",
    "    ou_process[0] = b  # Initial value\n",
    "    for t in range(1, N):\n",
    "        ou_process[t] = ou_process[t-1] + a * (b - ou_process[t-1]) * dt + sigma * np.sqrt(dt) * np.random.normal()\n",
    "    return ou_process\n",
    "\n",
    "# Number of subplots\n",
    "num_tenors = len(tenors)\n",
    "num_cols = 3  # Number of columns in subplot grid\n",
    "num_rows = (num_tenors + num_cols - 1) // num_cols  # Calculate number of rows needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(26, num_rows * 5), sharex=True, sharey=True)\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "# Simulate OU process for each tenor and each regime, then plot with residuals\n",
    "ou_simulated_df = pd.DataFrame(index=X.index)\n",
    "\n",
    "for i, tenor in enumerate(tenors):\n",
    "    ax = axes[i]\n",
    "    tenor_residuals = residuals[tenor]\n",
    "    \n",
    "    # Plot the residuals\n",
    "    ax.plot(X.index, tenor_residuals, label=f'Residuals for {tenor}', color='blue', linewidth=2)\n",
    "    \n",
    "    # Simulate OU process for each regime and overlay\n",
    "    for regime in ou_parameters_df.index.levels[0]:\n",
    "        params = ou_parameters_df.loc[regime].loc[tenor]\n",
    "        a, b, sigma = params['a'], params['b'], params['sigma']\n",
    "        T = len(X)  # Time period to simulate over\n",
    "        ou_simulation = simulate_ou_process(a, b, sigma, T)\n",
    "        ou_simulated_df[f'{tenor}_regime_{regime}'] = ou_simulation  # Store simulated data\n",
    "        \n",
    "        # Overlay the simulated OU process\n",
    "        ax.plot(X.index, ou_simulation, label=f'Simulated OU for {tenor} - Regime {regime}', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax.set_title(tenor)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1,1))\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(num_tenors, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout to make room for the subplots and labels\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regime 0 and Regime 1 seperate plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming 'combined_df' and 'ou_parameters_df' are already defined and loaded\n",
    "# Ensure 'combined_df' DataFrame has a DateTime index and contains the relevant columns.\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('2022-02-02')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "filtered_df_HMM = combined_df.loc[(combined_df.index >= start_date) & (combined_df.index <= end_date)]\n",
    "\n",
    "# Define tenors\n",
    "tenors = ['yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "          'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "          'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y']\n",
    "\n",
    "# Clean data\n",
    "X = filtered_df_HMM[tenors].dropna()\n",
    "\n",
    "# Perform PCA to reduce dimensionality and reconstruct data\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X)\n",
    "reconstruction = pca.inverse_transform(components)\n",
    "residuals = X - reconstruction\n",
    "\n",
    "# Function to simulate the OU process\n",
    "def simulate_ou_process(a, b, sigma, T, dt=1):\n",
    "    N = int(T/dt)\n",
    "    ou_process = np.zeros(N)\n",
    "    ou_process[0] = b  # Initial value\n",
    "    for t in range(1, N):\n",
    "        ou_process[t] = ou_process[t-1] + a * (b - ou_process[t-1]) * dt + sigma * np.sqrt(dt) * np.random.normal()\n",
    "    return ou_process\n",
    "\n",
    "# Number of subplots\n",
    "num_tenors = len(tenors)\n",
    "num_cols = 3  # Number of columns in subplot grid\n",
    "num_rows = (num_tenors + num_cols - 1) // num_cols  # Calculate number of rows needed\n",
    "\n",
    "# Plotting for Regime 0\n",
    "fig0, axes0 = plt.subplots(num_rows, num_cols, figsize=(26, num_rows * 5), sharex=True, sharey=True)\n",
    "axes0 = axes0.flatten()\n",
    "\n",
    "# Plotting for Regime 1\n",
    "fig1, axes1 = plt.subplots(num_rows, num_cols, figsize=(26, num_rows * 5), sharex=True, sharey=True)\n",
    "axes1 = axes1.flatten()\n",
    "\n",
    "# Simulate OU process for each tenor and each regime, then plot with residuals\n",
    "ou_simulated_df = pd.DataFrame(index=X.index)\n",
    "\n",
    "for i, tenor in enumerate(tenors):\n",
    "    tenor_residuals = residuals[tenor]\n",
    "    \n",
    "    # Plot the residuals for Regime 0\n",
    "    ax0 = axes0[i]\n",
    "    ax0.plot(X.index, tenor_residuals, label=f'Residuals for {tenor}', color='blue', linewidth=2)\n",
    "    \n",
    "    # Simulate OU process for Regime 0 and overlay\n",
    "    params0 = ou_parameters_df.loc['regime_0'].loc[tenor]\n",
    "    a0, b0, sigma0 = params0['a'], params0['b'], params0['sigma']\n",
    "    T = len(X)  # Time period to simulate over\n",
    "    ou_simulation0 = simulate_ou_process(a0, b0, sigma0, T)\n",
    "    ou_simulated_df[f'{tenor}_regime_0'] = ou_simulation0  # Store simulated data\n",
    "    ax0.plot(X.index, ou_simulation0, label=f'Simulated OU for {tenor} - Regime 0', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax0.set_title(f'{tenor} - Regime 0')\n",
    "    ax0.set_xlabel('Date')\n",
    "    ax0.set_ylabel('Value')\n",
    "    ax0.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax0.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax0.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax0.xaxis.get_majorticklabels(), rotation=45)\n",
    "    ax0.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    # Plot the residuals for Regime 1\n",
    "    ax1 = axes1[i]\n",
    "    ax1.plot(X.index, tenor_residuals, label=f'Residuals for {tenor}', color='blue', linewidth=2)\n",
    "    \n",
    "    # Simulate OU process for Regime 1 and overlay\n",
    "    params1 = ou_parameters_df.loc['regime_1'].loc[tenor]\n",
    "    a1, b1, sigma1 = params1['a'], params1['b'], params1['sigma']\n",
    "    ou_simulation1 = simulate_ou_process(a1, b1, sigma1, T)\n",
    "    ou_simulated_df[f'{tenor}_regime_1'] = ou_simulation1  # Store simulated data\n",
    "    ax1.plot(X.index, ou_simulation1, label=f'Simulated OU for {tenor} - Regime 1', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax1.set_title(f'{tenor} - Regime 1')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax1.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Hide any unused subplots for Regime 0\n",
    "for j in range(num_tenors, len(axes0)):\n",
    "    axes0[j].axis('off')\n",
    "\n",
    "# Hide any unused subplots for Regime 1\n",
    "for j in range(num_tenors, len(axes1)):\n",
    "    axes1[j].axis('off')\n",
    "\n",
    "# Adjust layout to make room for the subplots and labels for Regime 0\n",
    "plt.figure(fig0.number)\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "# Adjust layout to make room for the subplots and labels for Regime 1\n",
    "plt.figure(fig1.number)\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate the OU process\n",
    "def simulate_ou_process(a, b, sigma, T, dt=1):\n",
    "    N = int(T/dt)\n",
    "    ou_process = np.zeros(N)\n",
    "    ou_process[0] = b  # Initial value\n",
    "    for t in range(1, N):\n",
    "        ou_process[t] = ou_process[t-1] + a * (b - ou_process[t-1]) * dt + sigma * np.sqrt(dt) * np.random.normal()\n",
    "    return ou_process\n",
    "\n",
    "# Define function to calculate 99% confidence intervals\n",
    "def calculate_confidence_intervals(data):\n",
    "    return sem(data) * 2.58  # 99% confidence interval\n",
    "\n",
    "# Collect parameter estimates and confidence intervals\n",
    "parameter_estimates = {'a': [], 'b': [], 'sigma': []}\n",
    "confidence_intervals = {'a': [], 'b': [], 'sigma': []}\n",
    "\n",
    "for regime in ['regime_0', 'regime_1']:\n",
    "    for tenor in tenors:\n",
    "        params = ou_parameters_df.loc[regime].loc[tenor]\n",
    "        a, b, sigma = params['a'], params['b'], params['sigma']\n",
    "        \n",
    "        # Store parameters\n",
    "        parameter_estimates['a'].append(a)\n",
    "        parameter_estimates['b'].append(b)\n",
    "        parameter_estimates['sigma'].append(sigma)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        ci_a = calculate_confidence_intervals(residuals[tenor])\n",
    "        ci_b = calculate_confidence_intervals(residuals[tenor])\n",
    "        ci_sigma = calculate_confidence_intervals(residuals[tenor])\n",
    "        \n",
    "        confidence_intervals['a'].append(ci_a)\n",
    "        confidence_intervals['b'].append(ci_b)\n",
    "        confidence_intervals['sigma'].append(ci_sigma)\n",
    "\n",
    "# Number of parameters to plot (a, b, sigma)\n",
    "params = ['a', 'b', 'sigma']\n",
    "num_params = len(params)\n",
    "\n",
    "# Plotting the parameters with confidence intervals for both regimes\n",
    "fig, axes = plt.subplots(2, num_params, figsize=(20, 10), sharex=True)\n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    # Regime 0\n",
    "    axes[0, i].errorbar(tenors, parameter_estimates[param][:len(tenors)], \n",
    "                        yerr=confidence_intervals[param][:len(tenors)], \n",
    "                        fmt='o', capsize=5, label='Regime 0')\n",
    "    axes[0, i].set_title(param)\n",
    "    axes[0, i].set_ylabel('Regime 0')\n",
    "\n",
    "    # Regime 1\n",
    "    axes[1, i].errorbar(tenors, parameter_estimates[param][len(tenors):], \n",
    "                        yerr=confidence_intervals[param][len(tenors):], \n",
    "                        fmt='o', capsize=5, label='Regime 1')\n",
    "    axes[1, i].set_ylabel('Regime 1')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticklabels(tenors, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training  Test and Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from hmmlearn import hmm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assume 'combined_df' is already defined and loaded\n",
    "# combined_df = pd.read_csv('path_to_combined_df.csv')  # Placeholder for actual data loading\n",
    "\n",
    "# Define start_date and end_date for data filtering\n",
    "start_date = pd.to_datetime('2011-09-05')\n",
    "end_date = pd.to_datetime('2023-07-05')\n",
    "\n",
    "# Filter the DataFrame to the specified start and end dates\n",
    "filtered_df_HMM = combined_df.loc[start_date:end_date].copy()\n",
    "\n",
    "# Define the tenors to use for the analysis\n",
    "tenors = [\n",
    "    'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y', 'yield_6Y',\n",
    "    'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y', 'yield_12Y',\n",
    "    'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y', 'yield_40Y', 'yield_50Y'\n",
    "]\n",
    "\n",
    "# Prepare the feature matrix by dropping rows with any NaN values in the tenors columns\n",
    "X = filtered_df_HMM[tenors].dropna().values\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp = train_test_split(X, test_size=0.4, random_state=42)\n",
    "X_valid, X_test = train_test_split(X_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Number of hidden states for the HMM (adjustable)\n",
    "n_hidden_states = 2\n",
    "\n",
    "# Initialize and fit the HMM using the training set\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "model.fit(X_train)\n",
    "\n",
    "# Use the validation set to predict regimes and refine the model if necessary\n",
    "regimes_valid = model.predict(X_valid)\n",
    "\n",
    "# Use the test set for final evaluation\n",
    "regimes_test = model.predict(X_test)\n",
    "\n",
    "# Add regime information back to the original filtered DataFrame\n",
    "filtered_df_HMM = filtered_df_HMM.loc[filtered_df_HMM[tenors].dropna().index]\n",
    "filtered_df_HMM['regime'] = np.concatenate([model.predict(X_train), regimes_valid, regimes_test])\n",
    "\n",
    "# Store data for each regime in a dictionary for easy access\n",
    "regime_dfs = {}\n",
    "for i in range(n_hidden_states):\n",
    "    regime_name = f'regime_{i}'\n",
    "    regime_dfs[regime_name] = filtered_df_HMM[filtered_df_HMM['regime'] == i].copy()\n",
    "\n",
    "# Dictionary to store residuals for each regime\n",
    "residuals_dict = {}\n",
    "\n",
    "# Perform PCA for each regime and calculate residuals\n",
    "for regime, df in regime_dfs.items():\n",
    "    df_tenors = df[tenors].dropna()\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(df_tenors)\n",
    "    reconstruction = pca.inverse_transform(principalComponents)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = df_tenors - reconstruction\n",
    "    residuals_dict[regime] = residuals\n",
    "    \n",
    "    # Plot residuals for each tenor within the regime\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, tenor in enumerate(tenors):\n",
    "        plt.plot(residuals.index, residuals.iloc[:, i], label=f'{tenor}')\n",
    "    \n",
    "    plt.title(f'Residuals for {regime}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()\n",
    "\n",
    "# Function to fit an AR(1) model to the residuals and extract parameters\n",
    "def fit_ar1(residuals):\n",
    "    \"\"\"\n",
    "    Fits an AR(1) model to the provided residuals.\n",
    "    Returns the lag-1 coefficient and the intercept.\n",
    "    \"\"\"\n",
    "    model = AutoReg(residuals.dropna(), lags=1)\n",
    "    model_fitted = model.fit()\n",
    "    return model_fitted.params['const'], model_fitted.params[residuals.name + '.L1'], model_fitted.resid\n",
    "\n",
    "# Function to transform AR(1) parameters to Ornstein-Uhlenbeck (OU) parameters\n",
    "def ar1_to_ou(const, lag1_coef, residuals, dt=1):\n",
    "    \"\"\"\n",
    "    Transforms AR(1) parameters to Ornstein-Uhlenbeck (OU) parameters.\n",
    "    dt is the time increment and is assumed to be 1 for hourly data.\n",
    "    Returns mean reversion speed (a), mean level (b), and sigma.\n",
    "    \"\"\"\n",
    "    a = -np.log(lag1_coef) / dt\n",
    "    b = const / (1 - lag1_coef)\n",
    "    sigma = np.sqrt(np.var(residuals) * 2 * a / (1 - lag1_coef**2))\n",
    "    return a, b, sigma\n",
    "\n",
    "# Create a dictionary to store the OU parameters for each tenor and regime\n",
    "ou_parameters = {}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ValueWarning)\n",
    "    for regime, residuals in residuals_dict.items():\n",
    "        ou_parameters[regime] = {}\n",
    "        for column in residuals.columns:\n",
    "            const, lag1_coef, ar1_residuals = fit_ar1(residuals[column])\n",
    "            a, b, sigma = ar1_to_ou(const, lag1_coef, ar1_residuals)\n",
    "            ou_parameters[regime][column] = {'a': a, 'b': b, 'sigma': sigma}\n",
    "\n",
    "# Convert dictionary to DataFrame for better readability and analysis\n",
    "ou_parameters_df = pd.concat({k: pd.DataFrame(v).T for k, v in ou_parameters.items()}, axis=0)\n",
    "print(ou_parameters_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train, X_valid, X_test, and filtered_df_HMM have already been defined\n",
    "\n",
    "# Fit the HMM on the training data\n",
    "model = hmm.GaussianHMM(n_components=n_hidden_states, covariance_type=\"diag\", n_iter=100, init_params='mc', random_state=42)\n",
    "model.startprob_ = np.ones(n_hidden_states) / n_hidden_states\n",
    "transmat = np.full((n_hidden_states, n_hidden_states), 1 / n_hidden_states)\n",
    "np.fill_diagonal(transmat, 0.7)\n",
    "transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "model.transmat_ = transmat\n",
    "\n",
    "model.fit(X_train)\n",
    "\n",
    "# Predict regimes for training, validation, and test datasets\n",
    "regimes_train = model.predict(X_train)\n",
    "regimes_valid = model.predict(X_valid)\n",
    "regimes_test = model.predict(X_test)\n",
    "\n",
    "# Assign regimes back to the corresponding datasets\n",
    "filtered_df_HMM_train = pd.DataFrame(X_train, columns=tenors)\n",
    "filtered_df_HMM_train['regime'] = regimes_train\n",
    "\n",
    "filtered_df_HMM_valid = pd.DataFrame(X_valid, columns=tenors)\n",
    "filtered_df_HMM_valid['regime'] = regimes_valid\n",
    "\n",
    "filtered_df_HMM_test = pd.DataFrame(X_test, columns=tenors)\n",
    "filtered_df_HMM_test['regime'] = regimes_test\n",
    "\n",
    "# Define a function to compute OU parameters for a given dataset\n",
    "def compute_ou_parameters(filtered_df_HMM):\n",
    "    regime_dfs = {}\n",
    "    residuals_dict = {}\n",
    "    ou_parameters = {}\n",
    "\n",
    "    for i in range(n_hidden_states):\n",
    "        regime_name = f'regime_{i}'\n",
    "        regime_dfs[regime_name] = filtered_df_HMM[filtered_df_HMM['regime'] == i].copy()\n",
    "\n",
    "        df_tenors = regime_dfs[regime_name][tenors].dropna()\n",
    "        pca = PCA(n_components=3)\n",
    "        principalComponents = pca.fit_transform(df_tenors)\n",
    "        reconstruction = pca.inverse_transform(principalComponents)\n",
    "\n",
    "        residuals = df_tenors - reconstruction\n",
    "        residuals_dict[regime_name] = residuals\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=ValueWarning)\n",
    "            ou_parameters[regime_name] = {}\n",
    "            for column in residuals.columns:\n",
    "                const, lag1_coef, ar1_residuals = fit_ar1(residuals[column])\n",
    "                a, b, sigma = ar1_to_ou(const, lag1_coef, ar1_residuals)\n",
    "                ou_parameters[regime_name][column] = {'a': a, 'b': b, 'sigma': sigma}\n",
    "\n",
    "    return pd.concat({k: pd.DataFrame(v).T for k, v in ou_parameters.items()}, axis=0)\n",
    "\n",
    "# Compute OU parameters for training, validation, and test datasets\n",
    "ou_parameters_train = compute_ou_parameters(filtered_df_HMM_train)\n",
    "ou_parameters_valid = compute_ou_parameters(filtered_df_HMM_valid)\n",
    "ou_parameters_test = compute_ou_parameters(filtered_df_HMM_test)\n",
    "\n",
    "# Print the results\n",
    "print(\"OU Parameters for Training Data:\")\n",
    "print(ou_parameters_train.to_string())\n",
    "\n",
    "print(\"\\nOU Parameters for Validation Data:\")\n",
    "print(ou_parameters_valid.to_string())\n",
    "\n",
    "print(\"\\nOU Parameters for Test Data:\")\n",
    "print(ou_parameters_test.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_parameters_train.shape,ou_parameters_test.shape, ou_parameters_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_confidence_intervals(residuals):\n",
    "    # Assuming residuals is a series of residual values\n",
    "    # Calculate the standard error as an example (Standard Deviation / sqrt(N))\n",
    "    standard_error = residuals.std() / np.sqrt(len(residuals))\n",
    "    return standard_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize storage for parameter estimates and their confidence intervals\n",
    "parameter_estimates = {'a': [], 'b': [], 'sigma': []}\n",
    "confidence_intervals = {'a': [], 'b': [], 'sigma': []}\n",
    "\n",
    "# Iterate through each regime and tenor to gather data\n",
    "for regime in ['regime_0', 'regime_1']:\n",
    "    for tenor in tenors:\n",
    "        params = ou_parameters_df.loc[regime, tenor]\n",
    "        a, b, sigma = params['a'], params['b'], params['sigma']\n",
    "        \n",
    "        # Store parameters\n",
    "        parameter_estimates['a'].append(a)\n",
    "        parameter_estimates['b'].append(b)\n",
    "        parameter_estimates['sigma'].append(sigma)\n",
    "        \n",
    "        # Get residuals from the dictionary\n",
    "        residuals = residuals_dict[regime][tenor]\n",
    "        \n",
    "        # Calculate confidence intervals for each parameter\n",
    "        ci_a = calculate_confidence_intervals(residuals)\n",
    "        ci_b = calculate_confidence_intervals(residuals)\n",
    "        ci_sigma = calculate_confidence_intervals(residuals)\n",
    "        \n",
    "        confidence_intervals['a'].append(ci_a)\n",
    "        confidence_intervals['b'].append(ci_b)\n",
    "        confidence_intervals['sigma'].append(ci_sigma)\n",
    "\n",
    "# Define the parameters to plot\n",
    "params = ['a', 'b', 'sigma']\n",
    "\n",
    "# Setup the plotting framework\n",
    "fig, axes = plt.subplots(2, len(params), figsize=(20, 10), sharex=True)\n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    # Plotting for Regime 0\n",
    "    axes[0, i].errorbar(tenors, parameter_estimates[param][:len(tenors)], \n",
    "                        yerr=confidence_intervals[param][:len(tenors)], \n",
    "                        fmt='o', capsize=5, label='Regime 0')\n",
    "    axes[0, i].set_title(param)\n",
    "    axes[0, i].set_ylabel('Regime 0')\n",
    "\n",
    "    # Plotting for Regime 1\n",
    "    axes[1, i].errorbar(tenors, parameter_estimates[param][len(tenors):], \n",
    "                        yerr=confidence_intervals[param][len(tenors):], \n",
    "                        fmt='o', capsize=5, label='Regime 1')\n",
    "    axes[1, i].set_ylabel('Regime 1')\n",
    "\n",
    "# Set x-ticks and labels for all subplots\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks(np.arange(len(tenors)))\n",
    "    ax.set_xticklabels(tenors, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IOHMM and find the OU  parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IOHMM import UnSupervisedIOHMM, OLS, CrossEntropyMNL\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the data\n",
    "first_diff_df_cleaned = pd.read_csv(r'H:\\Excel\\first_diff_df_cleaned.csv', index_col=0, parse_dates=True)\n",
    "first_diff_df_cleaned = first_diff_df_cleaned.iloc[:, :-2]\n",
    "\n",
    "# Define the path to the Parquet file using forward slashes\n",
    "file_path = 'C:/Users/srajan/Downloads/ES_future.parquet'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Ensure the data is sorted by date if it's not already\n",
    "df = df.sort_index()\n",
    "\n",
    "# Calculate returns\n",
    "returns_df = df.pct_change().dropna()\n",
    "\n",
    "# Combine data\n",
    "combined_df = returns_df.join(first_diff_df_cleaned, how='inner')\n",
    "\n",
    "# Normalize the data\n",
    "covariate = 'Mid'\n",
    "target_columns = ['yield_1Y', 'yield_2Y', 'yield_3Y', 'yield_4Y', 'yield_5Y',\n",
    "                  'yield_6Y', 'yield_7Y', 'yield_8Y', 'yield_9Y', 'yield_10Y',\n",
    "                  'yield_12Y', 'yield_15Y', 'yield_20Y', 'yield_25Y', 'yield_30Y',\n",
    "                  'yield_40Y', 'yield_50Y']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "combined_df[target_columns] = scaler.fit_transform(combined_df[target_columns])\n",
    "combined_df[covariate] = scaler.fit_transform(combined_df[[covariate]])\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data, temp_data = train_test_split(combined_df, test_size=0.4, shuffle=False)\n",
    "valid_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Initialize the model with two hidden states\n",
    "SHMM = UnSupervisedIOHMM(num_states=2, max_EM_iter=200, EM_tol=1e-6)\n",
    "\n",
    "# Set models for emissions, transitions, and initial probabilities\n",
    "SHMM.set_models(\n",
    "    model_emissions=[OLS() for _ in target_columns],\n",
    "    model_transition=CrossEntropyMNL(solver='lbfgs'),\n",
    "    model_initial=CrossEntropyMNL(solver='lbfgs')\n",
    ")\n",
    "\n",
    "# Specify inputs\n",
    "SHMM.set_inputs(\n",
    "    covariates_initial=[covariate],\n",
    "    covariates_transition=[covariate],\n",
    "    covariates_emissions=[[covariate] for _ in target_columns]\n",
    ")\n",
    "\n",
    "# Define the output target columns\n",
    "SHMM.set_outputs([target_columns])\n",
    "\n",
    "# Set the data for the model and train it\n",
    "SHMM.set_data([train_data])\n",
    "\n",
    "try:\n",
    "    SHMM.train()\n",
    "    print(\"Model training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed during training:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = sum(len(seq) for seq in state_sequences)\n",
    "print(f\"Total number of timesteps across all sequences: {total_timesteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
